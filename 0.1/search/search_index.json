{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Approximate Bayesian Computation Python Package","text":""},{"location":"#package-description","title":"Package Description","text":"<p>Approximate Bayesian Computation (ABC) is a statistical method to fit a Bayesian model to data when the likelihood function is hard to compute. The <code>approxbayescomp</code> package implements an efficient form of ABC \u2014 the sequential Monte Carlo (SMC) algorithm. While it can handle any general statistical problem, we built in some models so that fitting insurance loss distributions is particularly easy. </p>"},{"location":"#installation","title":"Installation","text":"<p>To install simply run</p> <p><code>pip install approxbayescomp</code></p> <p>The source code for the package is available on Github.</p>"},{"location":"#example","title":"Example","text":""},{"location":"#using-a-built-in-data-generating-process-simulation-method","title":"Using a built-in data generating process simulation method","text":"<p>Consider a basic insurance example where each month our insurance company receives a random number of claims, each of which is of a random size. Specifically, say that in month \\(i\\) we have \\(N_i \\sim \\mathsf{Poisson}(\\lambda)\\) i.i.d. number of claims, and each claim is \\(U_{i,j} \\sim \\mathsf{Lognormal}(\\mu, \\sigma^2)\\) sized and i.i.d.</p> <p>At each month we can observe the aggregate claims, that is, \\( X_i = \\sum_{j=1}^{N_i} U_{i,j} \\) for \\(i=1,\\dots,T\\), that is, we observe \\(T\\) months of data. Lastly, we have the prior beliefs that \\(\\lambda \\sim \\mathsf{Unif}(0, 100),\\) \\(\\mu \\sim \\mathsf{Unif}(-5, 5),\\) and \\(\\sigma \\sim \\mathsf{Unif}(0, 3).\\)</p> <p>The <code>approxbayescomp</code> code to fit this data would be:</p> <pre><code>import approxbayescomp as abc\n\n# Load data to fit (modify this line to load real observations!)\nobsData = [1.0, 2.0, 3.0]\n\n# Specify our prior beliefs over (lambda, mu, sigma).\nprior = abc.IndependentUniformPrior([(0, 100), (-5, 5), (0, 3)])\n\n# Fit the model to the data using ABC\nmodel = abc.Model(\"poisson\", \"lognormal\", abc.Psi(\"sum\"), prior)\nnumIters = 6  # The number of SMC iterations to perform\npopSize = 250  # The population size of the SMC method\n\nfit = abc.smc(numIters, popSize, obsData, model)\n</code></pre> <p>Then <code>fit</code> will contain a collection of weighted samples from the approximate posterior distribution of \\((\\lambda, \\mu, \\sigma)\\). The posterior mean for these parameters would be easily calculated:</p> <pre><code>print(\"Posterior mean of lambda: \", np.sum(fit.samples[:, 0] * fit.weights))\nprint(\"Posterior mean of mu: \", np.sum(fit.samples[:, 1] * fit.weights))\nprint(\"Posterior mean of sigma: \", np.sum(fit.samples[:, 2] * fit.weights))\n</code></pre>"},{"location":"#using-a-user-suppled-simulation-method","title":"Using a user-suppled simulation method","text":"<p>We have built many standard insurance loss models into the package, so in the previous example</p> <pre><code>model = abc.Model(\"poisson\", \"lognormal\", abc.Psi(\"sum\"), prior)\n</code></pre> <p>is all that is required to specify this data-generating process. However, for non-insurance processes, we have to supply a function to simulate from the data-generating process. The equivalent version for this example would be:</p> <pre><code>import numpy as np\n\ndef simulate_aggregate_claims(rg, theta, T):\n    \"\"\"\n    Generate T observations from the model specified by theta\n    using the random number generator rg.\n    \"\"\"\n    lam, mu, sigma = theta\n    freqs = rg.poisson(lam, size=T)\n    aggClaims = np.empty(T, np.float64)\n    for t in range(T):\n        aggClaims[t] = np.sum(rg.lognormal(mu, sigma, size=freqs[t]))\n    return aggClaims\n\nsimulator = lambda rg, theta: simulate_aggregate_claims(rg, theta, len(obsData))\nmodel = abc.SimulationModel(simulator, prior)\n</code></pre> <p>Modifying just these lines will generate the identical output as the example above.</p>"},{"location":"#other-examples-and-resources","title":"Other Examples and Resources","text":"<p>See the What is ABC page for an illustrative example of the core ABC concept. For examples of this package in use, start with the Geometric-Exponential example page and the following ones.</p> <p>This package is the result of our paper \"Approximate Bayesian Computation to fit and compare insurance loss models\". For a detailed description of the aims and methodology of ABC check out this paper. It was written with ABC newcomers in mind.</p> <p>If you prefer audio/video, see Patrick's 7 min lightning talk at the Insurance Data Science conference:</p> <p> </p> ABC Talk at Insurance Data Science conference"},{"location":"#details","title":"Details","text":"<p>The main design goal for this package was computational speed. ABC is notoriously computationally demanding, so we spent a long time optimising the code as much as possible. The key functions are JIT-compiled to C with <code>numba</code> (we experimented with JIT-compiling the entire SMC algorithm, but <code>numba</code>'s random variable generation is surprisingly slower than <code>numpy</code>'s implementation). Everything that can be <code>numpy</code>-vectorised has been. And we scale to use as many CPU cores available on a machine using <code>joblib</code>. We also aimed to have total reproducibility, so for any given seed value the resulting ABC posterior samples will always be identical.</p> <p>Our main dependencies are joblib, numba, numpy, and scipy. Also, the package sometimes calls functions from matplotlib, tqdm, and hilbertcurve.</p> <p>Note</p> <p>Patrick has a rough start at a C++ version of this package at the cppabc repository. It only handles the specific Geometric-Exponential random sums case, though if you are interested in collaborating to expand this, let him know!</p>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Patrick Laub (author, maintainer),</li> <li>Pierre-Olivier Goffard (author).</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>Pierre-Olivier Goffard, Patrick J. Laub (2021), Approximate Bayesian Computations to fit and compare insurance loss models, Insurance: Mathematics and Economics, 100, pp. 350-371</p> <pre><code>@article{approxbayescomp,\n  title={Approximate Bayesian Computations to fit and compare insurance loss models},\n  author={Goffard, Pierre-Olivier and Laub, Patrick J},\n  journal={Insurance: Mathematics and Economics},\n  volume={100},\n  pages={350--371},\n  year={2021}\n}\n</code></pre>"},{"location":"bivariate-observations/","title":"Bivariate Observations","text":"<p>Insurers are typically exposed to more than one type of risk, and it can be beneficial for them to consider the joint risk profile for related products.</p> <p>This example considers a joint model for the frequency of claims reported for two nonlife insurance portfolios. The claim counts are Poisson distributed  with respective intensity $\\Lambda w_1$ and $\\Lambda w_2$ where $\\Lambda$ is some non-negative random variable.</p> <p>The frequency data $(n_1,m_1),\\ldots, (n_t,m_t)$ is i.i.d. according to a bivariate counting distribution with joint p.m.f. given by</p> <p>$$ \tp_{N,M}(n,m) = \\int\\frac{\\mathrm{e}^{-\\lambda w_1}(\\lambda w_1)^n}{n!}\\frac{\\mathrm{e}^{-\\lambda w_2}(\\lambda w_2)^m}{m!} \\mathrm{d} \\mathbb{P}_\\Lambda(\\lambda), \\quad n,m = 0, 1, \\dots. $$</p> <p>The severities associated to a given time period $s=1,\\ldots, t$ form two mutually independent, \\iid sequences of exponentially distributed random variables,</p> <p>$$ \tu_{s,1},\\ldots, u_{s,n_s} \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathsf{Exp}(m_1 = 10) \\quad \\text{and} \\quad \tv_{s,1},\\ldots, v_{s,m_s} \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathsf{Exp}(m_2 = 40). $$</p> <p>The model encapsulate the link between the frequencies of two insurance portfolios while accommodating for the well known overdispersed nature of the claim count data.</p> <p>We start by importing some necessary packages.</p> In\u00a0[1]: Copied! <pre>%config InlineBackend.figure_format = 'retina'\nimport approxbayescomp as abc\nimport numpy as np\nimport numpy.random as rnd\n</pre> %config InlineBackend.figure_format = 'retina' import approxbayescomp as abc import numpy as np import numpy.random as rnd <p>We will fit simulated data, so that we know the true value of the parameters for the data-generating process. We let $\\Lambda$ be a lognormal random variable $\\mathsf{Lognormal}(\\sigma=0.2)$ (the mean log parameter is set to $0$) as it is consistent with the use of a generalized linear model equipped with a log link function to estimate the Poisson intensity given a set of covariates. The marginal components of the claim frequency distribution are set to $w_1 = 15$ and $w_2 = 5$. Finally, we observe $T = 250$ i.i.d. bivariate observations.</p> In\u00a0[2]: Copied! <pre># Create a pseudorandom number generator\nrg = rnd.default_rng(1234)\n\n# Parameters of the true model\nfreq = \"bivariate poisson\"\nsev = \"exponential\"\n\u03c3 = 0.2\nw1 = 15\nw2 = 5\nm1 = 10\nm2 = 70\ntrueTheta = (\u03c3, w1, w2, m1, m2)\n\n# Setting the time horizon\nT = 250\n\n# Simulating the claim data\nclaimsData = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)\n\n# Simulating the observed data\npsi = abc.Psi(\"sum\")\n\nxData1 = abc.compute_psi(claimsData[0][0], claimsData[0][1], psi)\nxData2 = abc.compute_psi(claimsData[1][0], claimsData[1][1], psi)\n\nxData = np.vstack([xData1, xData2]).T\n</pre> # Create a pseudorandom number generator rg = rnd.default_rng(1234)  # Parameters of the true model freq = \"bivariate poisson\" sev = \"exponential\" \u03c3 = 0.2 w1 = 15 w2 = 5 m1 = 10 m2 = 70 trueTheta = (\u03c3, w1, w2, m1, m2)  # Setting the time horizon T = 250  # Simulating the claim data claimsData = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)  # Simulating the observed data psi = abc.Psi(\"sum\")  xData1 = abc.compute_psi(claimsData[0][0], claimsData[0][1], psi) xData2 = abc.compute_psi(claimsData[1][0], claimsData[1][1], psi)  xData = np.vstack([xData1, xData2]).T <p>We can see if any of this observed data contains pesky zeros:</p> In\u00a0[3]: Copied! <pre>np.sum(xData == 0)\n</pre> np.sum(xData == 0) Out[3]: <pre>4</pre> <p>Also, as we have bivariate data, we will use a special distance function which expects to receive a matrix with two columns:</p> In\u00a0[4]: Copied! <pre>xData.shape\n</pre> xData.shape Out[4]: <pre>(250, 2)</pre> <p>With this data, we create objects to represent the prior and the entire model. The priors are set as</p> <p>$$\\sigma \\sim \\mathsf{Unif}(0, 2), \\quad w_1 \\sim \\mathsf{Unif}(0, 50), \\quad w_2 \\sim \\mathsf{Unif}(0, 50)$$ $$m_1 \\sim \\mathsf{Unif}(0, 100), \\quad \\text{ and } \\quad m_2 \\sim \\mathsf{Unif}(0, 100).$$</p> In\u00a0[5]: Copied! <pre>prior = abc.IndependentUniformPrior([(0, 2), (0, 50), (0, 50), (0, 100), (0, 100)])\nmodel = abc.Model(freq, sev, psi, prior)\n</pre> prior = abc.IndependentUniformPrior([(0, 2), (0, 50), (0, 50), (0, 100), (0, 100)]) model = abc.Model(freq, sev, psi, prior) <p>Finally, we call the main <code>smc</code> method to fit the observed <code>xData</code>.</p> In\u00a0[7]: Copied! <pre>numIters = 10\npopSize = 250\n%time fit = abc.smc(numIters, popSize, xData, model, sumstats=abc.wass_2Ddist_ss, distance=abc.wass_2Ddist, numProcs=40, seed=1)\n</pre> numIters = 10 popSize = 250 %time fit = abc.smc(numIters, popSize, xData, model, sumstats=abc.wass_2Ddist_ss, distance=abc.wass_2Ddist, numProcs=40, seed=1) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>CPU times: user 1.05 s, sys: 156 ms, total: 1.2 s\nWall time: 23.9 s\n</pre> In\u00a0[8]: Copied! <pre>abc.plot_posteriors(\n    fit,\n    prior,\n    subtitles=[\"$\\\\delta$\", \"$w_1$\", \"$w_2$\", \"$m_1$\", \"$m_2$\"],\n    refLines=trueTheta,\n    removeYAxis=True,\n)\n</pre> abc.plot_posteriors(     fit,     prior,     subtitles=[\"$\\\\delta$\", \"$w_1$\", \"$w_2$\", \"$m_1$\", \"$m_2$\"],     refLines=trueTheta,     removeYAxis=True, ) <p>Trying a second time with the <code>matchZeros</code> flag on:</p> In\u00a0[9]: Copied! <pre>numIters = 10\npopSize = 250\n%time fitMatchZeros = abc.smc(numIters, popSize, xData, model, matchZeros=True, sumstats=abc.wass_2Ddist_ss, distance=abc.wass_2Ddist, numProcs=40, seed=1)\n</pre> numIters = 10 popSize = 250 %time fitMatchZeros = abc.smc(numIters, popSize, xData, model, matchZeros=True, sumstats=abc.wass_2Ddist_ss, distance=abc.wass_2Ddist, numProcs=40, seed=1) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>CPU times: user 1.27 s, sys: 151 ms, total: 1.42 s\nWall time: 12.7 s\n</pre> In\u00a0[10]: Copied! <pre>abc.plot_posteriors(\n    fitMatchZeros,\n    prior,\n    subtitles=[\"$\\\\delta$\", \"$w_1$\", \"$w_2$\", \"$m_1$\", \"$m_2$\"],\n    refLines=trueTheta,\n    removeYAxis=True,\n)\n</pre> abc.plot_posteriors(     fitMatchZeros,     prior,     subtitles=[\"$\\\\delta$\", \"$w_1$\", \"$w_2$\", \"$m_1$\", \"$m_2$\"],     refLines=trueTheta,     removeYAxis=True, ) <p>In this case, the <code>matchZeros</code> is more accurate (which is normal) but also faster (which is a little surprising). It seems being strict on matching the zeros helps push the SMC search in the right direction early in the iterations.</p>"},{"location":"bivariate-observations/#bivariate-observations","title":"Bivariate Observations\u00b6","text":""},{"location":"bivariate-observations/#generating-some-synthetic-data-to-fit","title":"Generating some synthetic data to fit\u00b6","text":""},{"location":"bivariate-observations/#use-abc-to-fit-both-the-frequency-and-claim-size-distributions","title":"Use ABC to fit both the frequency and claim size distributions\u00b6","text":""},{"location":"frequency-dependent-claim-sizes/","title":"Frequency-Dependent Claim Sizes","text":"<p>In this example, we look at a compound sum where the claim frequency and the claim sizes are dependent.</p> <p>Specifically, let's say that the claim frequency variables are Negative Binomial distributed</p> <p>$$ N_i \\overset{\\mathrm{i.i.d.}}{\\sim} \\textsf{Poisson}(\\lambda), \\quad i = 1, \\dots, T $$</p> <p>and the individual claim sizes are freqency dependent exponential, which means that</p> <p>$$ U_{i,1} \\ldots, U_{i, N_i} \\,|\\, N_i \\overset{\\mathrm{i.i.d.}}{\\sim} \\textsf{Exp}(\\beta\\times \\mathrm{e}^{\\delta N_i}), \\quad i = 1, \\dots, T. $$</p> <p>The available data is the total claim sizes</p> <p>$$ X_i = \\sum_{j = 1}^{N_i} U_{i,j}, \\quad i = 1, \\ldots, T. $$</p> <p>We start by importing some necessary packages.</p> In\u00a0[1]: Copied! <pre>%config InlineBackend.figure_format = 'retina'\nimport approxbayescomp as abc\nimport numpy as np\nimport numpy.random as rnd\n</pre> %config InlineBackend.figure_format = 'retina' import approxbayescomp as abc import numpy as np import numpy.random as rnd <p>We will fit simulated data, so that we know the true value of the parameters for the data-generating process. Here, we start with $\\lambda = 4$, $\\beta = 2$, and $\\delta = 0.2$, and say that we observe $T = 100$ i.i.d. observations of the compound sum.</p> In\u00a0[2]: Copied! <pre># Create a pseudorandom number generator\nrg = rnd.default_rng(1234)\n\n# Parameters of the true model\nfreq = \"poisson\"\nsev = \"frequency dependent exponential\"\n\u03bb = 4\n\u03b2 = 2\n\u03b4 = 0.2\ntrueTheta = (\u03bb, \u03b2, \u03b4)\n\n# Setting the time horizon\nT = 100\n\n# Simulating the claim data\nfreqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)\n\n# Simulating the observed data\npsi = abc.Psi(\"sum\")\nxData = abc.compute_psi(freqs, sevs, psi)\n</pre> # Create a pseudorandom number generator rg = rnd.default_rng(1234)  # Parameters of the true model freq = \"poisson\" sev = \"frequency dependent exponential\" \u03bb = 4 \u03b2 = 2 \u03b4 = 0.2 trueTheta = (\u03bb, \u03b2, \u03b4)  # Setting the time horizon T = 100  # Simulating the claim data freqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)  # Simulating the observed data psi = abc.Psi(\"sum\") xData = abc.compute_psi(freqs, sevs, psi) <p>We can see if any of this observed data contains pesky zeros:</p> In\u00a0[3]: Copied! <pre>np.sum(xData == 0)\n</pre> np.sum(xData == 0) Out[3]: <pre>3</pre> <p>With this data, we create objects to represent the prior and the entire model. The priors are set as $\\lambda \\sim \\mathsf{Unif}(0, 10)$, $\\beta \\sim \\mathsf{Unif}(0, 20)$, and $\\delta \\sim \\mathsf{Unif}(-1, 1).$</p> In\u00a0[4]: Copied! <pre>prior = abc.IndependentUniformPrior([(0, 10), (0, 20), (-1, 1)], (\"\u03bb\", \"\u03b2\", \"\u03b4\"))\nmodel = abc.Model(freq, sev, psi, prior)\n</pre> prior = abc.IndependentUniformPrior([(0, 10), (0, 20), (-1, 1)], (\"\u03bb\", \"\u03b2\", \"\u03b4\")) model = abc.Model(freq, sev, psi, prior) <p>After, we call the main <code>smc</code> method which is provided by <code>approxbayescomp</code> to fit the observed <code>xData</code>.</p> In\u00a0[5]: Copied! <pre>numIters = 10\npopSize = 100\n%time fit = abc.smc(numIters, popSize, xData, model, seed=1)\n</pre> numIters = 10 popSize = 100 %time fit = abc.smc(numIters, popSize, xData, model, seed=1) <pre>CPU times: user 14.1 s, sys: 135 ms, total: 14.3 s\nWall time: 14.1 s\n</pre> In\u00a0[6]: Copied! <pre>abc.plot_posteriors(\n    fit, prior, subtitles=[\"$\\\\lambda$\", \"$\\\\beta$\", \"$\\\\delta$\"], refLines=trueTheta\n)\n</pre> abc.plot_posteriors(     fit, prior, subtitles=[\"$\\\\lambda$\", \"$\\\\beta$\", \"$\\\\delta$\"], refLines=trueTheta ) <p>This quick fit managed to capture $\\delta$ quite well, $\\beta$ with moderate success, and $\\lambda$ with slightly less success.</p> <p>The observed data does have zeros, so enabling the <code>matchZeros</code> flag should increase the accuracy (and computation time) of the fits.</p> In\u00a0[7]: Copied! <pre>%time fitMatchZeros = abc.smc(numIters, popSize, xData, model, matchZeros=True, seed=1)\n</pre> %time fitMatchZeros = abc.smc(numIters, popSize, xData, model, matchZeros=True, seed=1) <pre>CPU times: user 37.5 s, sys: 342 ms, total: 37.9 s\nWall time: 37.5 s\n</pre> In\u00a0[8]: Copied! <pre>abc.plot_posteriors(\n    fitMatchZeros,\n    prior,\n    subtitles=[\"$\\\\lambda$\", \"$\\\\beta$\", \"$\\\\delta$\"],\n    refLines=trueTheta,\n)\n</pre> abc.plot_posteriors(     fitMatchZeros,     prior,     subtitles=[\"$\\\\lambda$\", \"$\\\\beta$\", \"$\\\\delta$\"],     refLines=trueTheta, )"},{"location":"frequency-dependent-claim-sizes/#frequency-dependent-claim-sizes","title":"Frequency-Dependent Claim Sizes\u00b6","text":""},{"location":"frequency-dependent-claim-sizes/#generating-some-synthetic-data-to-fit","title":"Generating some synthetic data to fit\u00b6","text":""},{"location":"frequency-dependent-claim-sizes/#use-abc-to-fit-both-the-frequency-and-claim-size-distributions","title":"Use ABC to fit both the frequency and claim size distributions\u00b6","text":""},{"location":"geometric-exponential/","title":"Geometric-Exponential Compound Sums","text":"<p>Consider a basic insurance example where each month our insurance company receives a random number of claims, each of which is of a random size. Specifically, say that in month $i$ we have $N_i \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathsf{Geometric}(p)$ number of claims, and each claim is $U_{i,j} \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathsf{Exponential}(\\lambda = 1/\\delta)$ sized.</p> <p>At each month we can observe the aggregate claims, that is, $ X_i = \\sum_{j=1}^{N_i} U_{i,j} $ for $i=1,\\dots,T$, that is, we observe $T$ months of data.</p> <p>In this scenario, the likelihood of the $X_i$'s is tractable, so it is a useful test case to compare ABC against more traditional fitting methods, like Markov Chain Monte Carlo (MCMC) and Maximum Likelihood Estimation (MLE).</p> <p>We start by importing some necessary packages.</p> In\u00a0[1]: Copied! <pre>%config InlineBackend.figure_format = 'retina'\nimport approxbayescomp as abc\nimport numpy as np\nimport numpy.random as rnd\n</pre> %config InlineBackend.figure_format = 'retina' import approxbayescomp as abc import numpy as np import numpy.random as rnd <p>We will fit simulated data, so that we know the true value of the parameters for the data-generating process. Here, we start with $p = 0.8$, and $\\delta = 5$, and say that we observe $T = 100$ i.i.d. observations of the compound sum.</p> In\u00a0[2]: Copied! <pre># Create a pseudorandom number generator\nrg = rnd.default_rng(1234)\n\n# Parameters of the true model\nfreq = \"geometric\"\nsev = \"exponential\"\ntrueTheta = (0.8, 5)\n\n# Setting the time horizon\nT = 100\n</pre> # Create a pseudorandom number generator rg = rnd.default_rng(1234)  # Parameters of the true model freq = \"geometric\" sev = \"exponential\" trueTheta = (0.8, 5)  # Setting the time horizon T = 100 <p>Inside <code>approxbayescomp</code>, generating this compound sum is done in two steps. Firstly, we generate the entire claims process:</p> In\u00a0[3]: Copied! <pre># Simulating the claim data\nfreqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)\n</pre> # Simulating the claim data freqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, trueTheta) <p>So <code>freqs</code> contains the $\\{ N_i \\}_{i=1,\\dots,T}$, and <code>sevs</code> contains the $\\{ U_{i,j} \\}_{i=1,\\dots,T, j=1,\\dots,N_i }$. For example:</p> In\u00a0[4]: Copied! <pre>freqs\n</pre> freqs Out[4]: <pre>array([ 6,  3,  8,  3,  4,  1,  0,  1,  6,  4,  7, 23, 11,  1,  2,  3, 14,\n        0,  0, 12,  0, 16,  2,  4,  0,  3,  5,  0,  0,  3,  5,  4,  6,  4,\n        3,  7,  2,  9,  0, 10,  1,  2,  9,  6, 13,  6,  2,  6,  1,  0,  2,\n        3,  4,  2,  9,  2,  2,  2,  4, 13,  3,  9,  3,  5,  0,  3,  2,  1,\n        3,  1, 14,  0,  3,  5,  0,  0,  4,  1,  5,  0,  0,  0,  6,  1,  1,\n        0,  9,  2,  1,  1,  7,  0,  0,  0,  1, 10,  2, 20,  4,  0])</pre> In\u00a0[5]: Copied! <pre>sevs[0:6]\n</pre> sevs[0:6] Out[5]: <pre>array([ 0.53075068,  8.74170314,  1.93800992,  1.23094078,  3.07323683,\n       12.37846317])</pre> <p>Note, the <code>sevs</code> vector contains all the claims sizes (a.k.a. severities) for all time periods in one long vector. That is, the length of <code>sevs</code> will be <code>np.sum(freqs)</code> long. This is to save computational time in the ABC algorithm, as the <code>simulate_claim_data</code> function is called many times.</p> <p>Then the $\\Psi$ operation is applied to produce a single value for each time period. Here, this means that we observe the sum of all the claims in each period, however <code>psi</code> can be set to handle censored values like stop-loss observations.</p> In\u00a0[6]: Copied! <pre># Simulating the observed data\npsi = abc.Psi(\"sum\")\nxData = abc.compute_psi(freqs, sevs, psi)\n</pre> # Simulating the observed data psi = abc.Psi(\"sum\") xData = abc.compute_psi(freqs, sevs, psi) In\u00a0[7]: Copied! <pre>xData\n</pre> xData Out[7]: <pre>array([ 27.89310452,  17.68445011,  43.97815086,  10.40718159,\n         8.81323352,   2.91015669,   0.        ,   0.73331647,\n        54.66211006,  15.40214731,  97.62341685,  68.75165795,\n        41.72228953,  16.97354716,  15.02697784,   9.2411639 ,\n        71.63352158,   0.        ,   0.        ,  47.31622624,\n         0.        ,  43.97581615,   0.9500043 ,  28.01171543,\n         0.        ,   6.39736742,  34.87762318,   0.        ,\n         0.        ,  13.20533558,  19.67856556,  20.78462773,\n        31.03229256,  16.08706747,   7.63900497,  40.2456368 ,\n        17.32230075,  38.54744184,   0.        ,  51.08710316,\n         2.37910798,   4.61828744,  65.38123231,  35.85594666,\n        98.43324628,  19.38163374,   6.45563628,  12.0755808 ,\n         0.90372386,   0.        ,   8.29971217,  29.44775501,\n        23.33491296,  25.80718148,  12.5482116 ,  10.15175395,\n        26.38410576,  38.06253589,  52.00790943,  82.63320756,\n        10.04313675,  39.01542845,  11.83534301,  24.36216167,\n         0.        ,  36.35342184,   1.51834634,   1.61287464,\n        11.03044295,   3.44329813,  76.99523447,   0.        ,\n        16.25783756,  28.20808019,   0.        ,   0.        ,\n        19.46729005,   5.23993577,  16.14610925,   0.        ,\n         0.        ,   0.        ,  19.96795078,   1.97252796,\n        10.84627396,   0.        ,  43.36145952,   5.53146636,\n         1.31794268,   2.62059041,  40.49646268,   0.        ,\n         0.        ,   0.        ,   0.1206393 ,  59.5075802 ,\n         8.46419934, 100.21132471,  14.67992158,   0.        ])</pre> <p>The observed values can include many zero values, as the compound distribution has an atom as 0.</p> In\u00a0[8]: Copied! <pre>print(\"Number of zeros:\", np.sum(xData == 0))\n</pre> print(\"Number of zeros:\", np.sum(xData == 0)) <pre>Number of zeros: 21\n</pre> <p>With this data, we create objects to represent the prior and the entire model. The priors are simply that $p \\sim \\mathsf{Unif}(0, 1)$ and $\\delta \\sim \\mathsf{Unif}(0, 100).$</p> In\u00a0[9]: Copied! <pre>prior = abc.IndependentUniformPrior([(0, 1), (0, 100)], (\"p\", \"\u03b4\"))\nmodel = abc.Model(freq, sev, psi, prior)\n</pre> prior = abc.IndependentUniformPrior([(0, 1), (0, 100)], (\"p\", \"\u03b4\")) model = abc.Model(freq, sev, psi, prior) <p>After, we call the main <code>smc</code> method which is provided by <code>approxbayescomp</code> to fit the observed <code>xData</code>.</p> In\u00a0[10]: Copied! <pre>numIters = 10\npopSize = 100\n%time fit = abc.smc(numIters, popSize, xData, model, seed=1)\n</pre> numIters = 10 popSize = 100 %time fit = abc.smc(numIters, popSize, xData, model, seed=1) <pre>CPU times: user 5.8 s, sys: 59.3 ms, total: 5.86 s\nWall time: 5.82 s\n</pre> In\u00a0[11]: Copied! <pre>abc.plot_posteriors(fit, prior, subtitles=[\"$p$\", \"$\\\\delta$\"], refLines=trueTheta)\n</pre> abc.plot_posteriors(fit, prior, subtitles=[\"$p$\", \"$\\\\delta$\"], refLines=trueTheta) <p>Technically, the theory behind ABC doesn't like the fact that the compound distribution we're fitting is a combination of a continuous distribution and a discrete atom (at 0). The theory says that we should reject fake data which contains a different number of zero observations as the observed data. This behaviour can be enabled by the <code>matchZeros</code> flag. When the observed data contains zeros, enabling <code>matchZeros</code> should increase the accuracy of the fits slightly, though it likely to drastically increase the computational time.</p> In\u00a0[12]: Copied! <pre>%time fitMatchZeros = abc.smc(numIters, popSize, xData, model, matchZeros=True, seed=1)\n</pre> %time fitMatchZeros = abc.smc(numIters, popSize, xData, model, matchZeros=True, seed=1) <pre>CPU times: user 1min 39s, sys: 316 ms, total: 1min 39s\nWall time: 1min 39s\n</pre> In\u00a0[13]: Copied! <pre>abc.plot_posteriors(\n    fitMatchZeros, prior, subtitles=[\"$p$\", \"$\\delta$\"], refLines=trueTheta\n)\n</pre> abc.plot_posteriors(     fitMatchZeros, prior, subtitles=[\"$p$\", \"$\\delta$\"], refLines=trueTheta ) <p>As the likelihood for this simple/special case is tractable, we can fit the data using the standard Markov Chain Monte Carlo method via <code>PyMC3</code>:</p> In\u00a0[14]: Copied! <pre>import pymc3 as pm\n\nbasic_model = pm.Model()\n\nwith basic_model:\n    # Priors for unknown model parameters\n    p = pm.Uniform(\"p\", 0, 1)\n    \u03b4 = pm.Uniform(\"\u03b4\", 0, 100)\n\n    # Log probability of the compound-sum variable\n    def logp(t0, sumData):\n        return T * np.log(1 - p) + (T - t0) * np.log(p / \u03b4) - (1 - p) / \u03b4 * sumData\n\n    exp_surv = pm.DensityDist(\n        \"X\", logp, observed={\"t0\": np.sum(xData == 0), \"sumData\": np.sum(xData)}\n    )\n\n    trace = pm.sample(\n        1000, tune=500, chains=1, random_seed=1, return_inferencedata=True\n    )\n</pre> import pymc3 as pm  basic_model = pm.Model()  with basic_model:     # Priors for unknown model parameters     p = pm.Uniform(\"p\", 0, 1)     \u03b4 = pm.Uniform(\"\u03b4\", 0, 100)      # Log probability of the compound-sum variable     def logp(t0, sumData):         return T * np.log(1 - p) + (T - t0) * np.log(p / \u03b4) - (1 - p) / \u03b4 * sumData      exp_surv = pm.DensityDist(         \"X\", logp, observed={\"t0\": np.sum(xData == 0), \"sumData\": np.sum(xData)}     )      trace = pm.sample(         1000, tune=500, chains=1, random_seed=1, return_inferencedata=True     ) <pre>WARNING (theano.link.c.cmodule): install mkl with `conda install mkl-service`: No module named 'mkl'\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (1 chains in 1 job)\nNUTS: [\u03b4, p]\n</pre>        100.00% [1500/1500 00:01&lt;00:00 Sampling chain 0, 0 divergences]      <pre>Sampling 1 chain for 500 tune and 1_000 draw iterations (500 + 1_000 draws total) took 1 seconds.\nOnly one chain was sampled, this makes it impossible to run some convergence checks\n</pre> <p>Comparing this 'true' MCMC posterior to the 'approximate' ABC posterior we can see there is quite a strong agreement between the two:</p> In\u00a0[15]: Copied! <pre>import matplotlib.pyplot as plt\n\nabc.plot_posteriors(fitMatchZeros, prior)\n\nfig = plt.gcf()\naxs = fig.axes\n\nfor l, param in enumerate([\"p\", \"\u03b4\"]):\n    nMCMC = len(trace.posterior[param][0])\n    abc.weighted_distplot(\n        trace.posterior[param][0], np.ones(nMCMC) / nMCMC, ax=axs[l], hist=False\n    )\n\nplt.legend([\"ABC\", \"Prior\", \"MCMC\"])\n</pre> import matplotlib.pyplot as plt  abc.plot_posteriors(fitMatchZeros, prior)  fig = plt.gcf() axs = fig.axes  for l, param in enumerate([\"p\", \"\u03b4\"]):     nMCMC = len(trace.posterior[param][0])     abc.weighted_distplot(         trace.posterior[param][0], np.ones(nMCMC) / nMCMC, ax=axs[l], hist=False     )  plt.legend([\"ABC\", \"Prior\", \"MCMC\"])"},{"location":"geometric-exponential/#geometric-exponential-compound-sums","title":"Geometric-Exponential Compound Sums\u00b6","text":""},{"location":"geometric-exponential/#generating-some-synthetic-data-to-fit","title":"Generating some synthetic data to fit\u00b6","text":""},{"location":"geometric-exponential/#use-abc-to-fit-both-the-frequency-and-claim-size-distributions","title":"Use ABC to fit both the frequency and claim size distributions\u00b6","text":""},{"location":"geometric-exponential/#comparing-against-mcmc-with-pymc3","title":"Comparing against MCMC with PyMC3\u00b6","text":""},{"location":"insurance-model-example/","title":"Insurance model example","text":"In\u00a0[\u00a0]: Copied! <pre>import approxbayescomp as abc\n</pre> import approxbayescomp as abc In\u00a0[\u00a0]: Copied! <pre># Load data to fit (modify this line to load real observations!)\nobsData = [1.0, 2.0, 3.0]\n</pre> # Load data to fit (modify this line to load real observations!) obsData = [1.0, 2.0, 3.0] In\u00a0[\u00a0]: Copied! <pre># Specify our prior beliefs over (lambda, mu, sigma).\nprior = abc.IndependentUniformPrior([(0, 100), (-5, 5), (0, 3)])\n</pre> # Specify our prior beliefs over (lambda, mu, sigma). prior = abc.IndependentUniformPrior([(0, 100), (-5, 5), (0, 3)]) In\u00a0[\u00a0]: Copied! <pre># Fit the model to the data using ABC\nmodel = abc.Model(\"poisson\", \"lognormal\", abc.Psi(\"sum\"), prior)\nnumIters = 6  # The number of SMC iterations to perform\npopSize = 250  # The population size of the SMC method\n</pre> # Fit the model to the data using ABC model = abc.Model(\"poisson\", \"lognormal\", abc.Psi(\"sum\"), prior) numIters = 6  # The number of SMC iterations to perform popSize = 250  # The population size of the SMC method In\u00a0[\u00a0]: Copied! <pre>fit = abc.smc(numIters, popSize, obsData, model)\n</pre> fit = abc.smc(numIters, popSize, obsData, model)"},{"location":"model-selection/","title":"Model Selection (WIP)","text":"<p>In this notebook we are are conducting a simulation experiment where the claim frequency are Negative Binomial distributed</p> <p>$$ n_s\\underset{\\textbf{i.i.d.}}{\\sim}\\text{Neg-Bin}(\\alpha = 4, p = 2/3),\\text{ }s = 1,\\ldots, 30 $$</p> <p>and the individual claim sizes are weibull distributed</p> <p>$$ u_1,\\ldots, u_{n_s}\\underset{\\textbf{i.i.d.}}{\\sim}\\text{Weib}(k = 1/2, \\beta = 1),\\text{ }s = 1,\\ldots 30. $$</p> <p>The available data is aggregated claim sizes in excess of the priority $c=1$ asociated to aa global stop-loss treaty, we have</p> <p>$$ x_s = \\left(\\sum_{k = 1}^{n_s}u_k-c\\right)_{+},\\text{ }s = 1,\\ldots, t. $$</p> <p>Our aim is to look into the finite sample performance of our ABC implementation when deciding which model is the most suited.</p> In\u00a0[1]: Copied! <pre>from preamble import *\nfrom infer_loss_distribution import *\nfrom infer_count_distribution import *\n\n%config InlineBackend.figure_format = 'retina'\n%load_ext lab_black\n</pre> from preamble import * from infer_loss_distribution import * from infer_count_distribution import *  %config InlineBackend.figure_format = 'retina' %load_ext lab_black In\u00a0[3]: Copied! <pre>import sys\n\nprint(\"ABC version:\", abc.__version__)\nprint(\"Python version:\", sys.version)\nprint(\"Numpy version:\", np.__version__)\nprint(\"PyMC3 version:\", pm.__version__)\nprint(\"Arviz version:\", arviz.__version__)\n\ntic()\n</pre> import sys  print(\"ABC version:\", abc.__version__) print(\"Python version:\", sys.version) print(\"Numpy version:\", np.__version__) print(\"PyMC3 version:\", pm.__version__) print(\"Arviz version:\", arviz.__version__)  tic() <pre>ABC version: 0.1.1\nPython version: 3.8.10 (default, May 19 2021, 18:05:58) \n[GCC 7.3.0]\nNumpy version: 1.20.3\nPyMC3 version: 3.11.2\nArviz version: 0.11.2\n</pre> In\u00a0[4]: Copied! <pre>FAST = False\n\n# Processor information and SMC calibration parameters\nif not FAST:\n    numIters = 7\n    numItersData = 10\n    popSize = 1000\n    popSizeModels = 1000\n    epsMin = 0\n    timeout = 1000\nelse:\n    numIters = 4\n    numItersData = 8\n    popSize = 500\n    popSizeModels = 1000\n    epsMin = 1\n    timeout = 30\n\nsmcArgs = {\"timeout\": timeout, \"epsMin\": epsMin, \"verbose\": True}\nsmcArgs[\"numProcs\"] = 40\n</pre> FAST = False  # Processor information and SMC calibration parameters if not FAST:     numIters = 7     numItersData = 10     popSize = 1000     popSizeModels = 1000     epsMin = 0     timeout = 1000 else:     numIters = 4     numItersData = 8     popSize = 500     popSizeModels = 1000     epsMin = 1     timeout = 30  smcArgs = {\"timeout\": timeout, \"epsMin\": epsMin, \"verbose\": True} smcArgs[\"numProcs\"] = 40 In\u00a0[5]: Copied! <pre>rg = default_rng(123)\n\nsample_sizes = [50, 250]\nT = sample_sizes[-1]\nt = np.arange(1, T + 1, 1)\n\n# Frequency-Loss Model\n\u03b1, p, k, \u03b2 = 4, 2 / 3, 1 / 3, 1\n\u03b8_True = \u03b1, p, k, \u03b2\n\u03b8_sev = k, \u03b2\n\u03b8_freq = \u03b1, p\nfreq = \"negative binomial\"\nsev = \"weibull\"\n\n# Aggregation process\nc = 1\npsi = abc.Psi(\"GSL\", c)\n\nfreqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, \u03b8_True)\ndf_full = pd.DataFrame(\n    {\n        \"time_period\": np.concatenate([np.repeat(s, freqs[s - 1]) for s in t]),\n        \"claim_size\": sevs,\n    }\n)\n\nxData = abc.compute_psi(freqs, sevs, psi)\n\ndf_agg = pd.DataFrame({\"time_period\": t, \"N\": freqs, \"X\": xData})\n</pre> rg = default_rng(123)  sample_sizes = [50, 250] T = sample_sizes[-1] t = np.arange(1, T + 1, 1)  # Frequency-Loss Model \u03b1, p, k, \u03b2 = 4, 2 / 3, 1 / 3, 1 \u03b8_True = \u03b1, p, k, \u03b2 \u03b8_sev = k, \u03b2 \u03b8_freq = \u03b1, p freq = \"negative binomial\" sev = \"weibull\"  # Aggregation process c = 1 psi = abc.Psi(\"GSL\", c)  freqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, \u03b8_True) df_full = pd.DataFrame(     {         \"time_period\": np.concatenate([np.repeat(s, freqs[s - 1]) for s in t]),         \"claim_size\": sevs,     } )  xData = abc.compute_psi(freqs, sevs, psi)  df_agg = pd.DataFrame({\"time_period\": t, \"N\": freqs, \"X\": xData}) In\u00a0[6]: Copied! <pre>[np.sum(xData[:ss] &gt; 0) for ss in sample_sizes]\n</pre> [np.sum(xData[:ss] &gt; 0) for ss in sample_sizes] Out[6]: <pre>[23, 130]</pre> In\u00a0[7]: Copied! <pre>Bayesian_Summary = pd.DataFrame({\"model\": [], \"ss\": [], \"k\": [], \"\u03b2\": []})\nmodels = [\"True weibull\", \"True gamma\"]\nfor m in models:\n    for ss in sample_sizes:\n\n        uData = np.array(df_full.claim_size[df_full.time_period &lt;= ss])\n        print(\"The number of individual claim sizes is \", len(uData))\n        if m == \"True weibull\":\n            # We fit a Weibull model using SMC\n            with pm.Model() as model_sev:\n                k = pm.Uniform(\"k\", lower=1e-1, upper=10)\n                \u03b2 = pm.Uniform(\"\u03b2\", lower=0, upper=20)\n                U = pm.Weibull(\"U\", alpha=k, beta=\u03b2, observed=uData)\n                %time trace = pm.sample_smc(popSize, random_seed=1, chains=1)\n\n        elif m == \"True gamma\":\n            # We fit a gamma model using SMC\n            with pm.Model() as model_sev:\n                param1 = pm.Uniform(\"k\", lower=0, upper=10)\n                param2 = pm.Uniform(\"\u03b2\", lower=0, upper=50)\n                U = pm.Gamma(\"U\", alpha=param1, beta=1 / param2, observed=uData)\n                %time trace = pm.sample_smc(popSize, random_seed=1, chains=1)\n\n        arviz.plot_posterior(trace)\n\n        log_lik = trace.report.log_marginal_likelihood[0]\n\n        res = pd.DataFrame(\n            {\n                \"model\": [m],\n                \"ss\": [ss],\n                \"k\": [trace[\"k\"].mean()],\n                \"\u03b2\": [trace[\"\u03b2\"].mean()],\n                \"marginal_log_likelihood\": [log_lik],\n            }\n        )\n        Bayesian_Summary = pd.concat([Bayesian_Summary, res])\n\nmax_marginal_log_likelihood = (\n    Bayesian_Summary[[\"ss\", \"marginal_log_likelihood\"]]\n    .groupby(\"ss\")\n    .max()\n    .marginal_log_likelihood.values\n)\nmax_marginal_log_likelihood = np.concatenate(\n    [max_marginal_log_likelihood, max_marginal_log_likelihood]\n)\nBayesian_Summary[\"BF\"] = np.exp(\n    Bayesian_Summary.marginal_log_likelihood - max_marginal_log_likelihood\n)\nsum_BF = Bayesian_Summary[[\"ss\", \"BF\"]].groupby(\"ss\").sum().BF.values\nsum_BF = np.concatenate([sum_BF, sum_BF])\nBayesian_Summary[\"model_probability\"] = Bayesian_Summary.BF / sum_BF\nBayesian_Summary\n</pre> Bayesian_Summary = pd.DataFrame({\"model\": [], \"ss\": [], \"k\": [], \"\u03b2\": []}) models = [\"True weibull\", \"True gamma\"] for m in models:     for ss in sample_sizes:          uData = np.array(df_full.claim_size[df_full.time_period &lt;= ss])         print(\"The number of individual claim sizes is \", len(uData))         if m == \"True weibull\":             # We fit a Weibull model using SMC             with pm.Model() as model_sev:                 k = pm.Uniform(\"k\", lower=1e-1, upper=10)                 \u03b2 = pm.Uniform(\"\u03b2\", lower=0, upper=20)                 U = pm.Weibull(\"U\", alpha=k, beta=\u03b2, observed=uData)                 %time trace = pm.sample_smc(popSize, random_seed=1, chains=1)          elif m == \"True gamma\":             # We fit a gamma model using SMC             with pm.Model() as model_sev:                 param1 = pm.Uniform(\"k\", lower=0, upper=10)                 param2 = pm.Uniform(\"\u03b2\", lower=0, upper=50)                 U = pm.Gamma(\"U\", alpha=param1, beta=1 / param2, observed=uData)                 %time trace = pm.sample_smc(popSize, random_seed=1, chains=1)          arviz.plot_posterior(trace)          log_lik = trace.report.log_marginal_likelihood[0]          res = pd.DataFrame(             {                 \"model\": [m],                 \"ss\": [ss],                 \"k\": [trace[\"k\"].mean()],                 \"\u03b2\": [trace[\"\u03b2\"].mean()],                 \"marginal_log_likelihood\": [log_lik],             }         )         Bayesian_Summary = pd.concat([Bayesian_Summary, res])  max_marginal_log_likelihood = (     Bayesian_Summary[[\"ss\", \"marginal_log_likelihood\"]]     .groupby(\"ss\")     .max()     .marginal_log_likelihood.values ) max_marginal_log_likelihood = np.concatenate(     [max_marginal_log_likelihood, max_marginal_log_likelihood] ) Bayesian_Summary[\"BF\"] = np.exp(     Bayesian_Summary.marginal_log_likelihood - max_marginal_log_likelihood ) sum_BF = Bayesian_Summary[[\"ss\", \"BF\"]].groupby(\"ss\").sum().BF.values sum_BF = np.concatenate([sum_BF, sum_BF]) Bayesian_Summary[\"model_probability\"] = Bayesian_Summary.BF / sum_BF Bayesian_Summary <pre>The number of individual claim sizes is  98\n</pre> <pre>Initializing SMC sampler...\nSampling 1 chain in 1 job\nStage:   0 Beta: 0.000\nStage:   1 Beta: 0.001\nStage:   2 Beta: 0.015\nStage:   3 Beta: 0.069\nStage:   4 Beta: 0.163\nStage:   5 Beta: 0.427\nStage:   6 Beta: 1.000\n/home/plaub/miniconda3/lib/python3.8/site-packages/arviz/data/io_pymc3.py:96: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  warnings.warn(\n</pre> <pre>CPU times: user 1.37 s, sys: 88.3 ms, total: 1.46 s\nWall time: 2.4 s\nThe number of individual claim sizes is  525\n</pre> <pre>Initializing SMC sampler...\nSampling 1 chain in 1 job\nStage:   0 Beta: 0.000\nStage:   1 Beta: 0.000\nStage:   2 Beta: 0.003\nStage:   3 Beta: 0.014\nStage:   4 Beta: 0.034\nStage:   5 Beta: 0.090\nStage:   6 Beta: 0.276\nStage:   7 Beta: 0.946\nStage:   8 Beta: 1.000\n/home/plaub/miniconda3/lib/python3.8/site-packages/arviz/data/io_pymc3.py:96: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  warnings.warn(\n</pre> <pre>CPU times: user 1.96 s, sys: 4.12 ms, total: 1.97 s\nWall time: 1.96 s\nThe number of individual claim sizes is  98\n</pre> <pre>Initializing SMC sampler...\nSampling 1 chain in 1 job\nStage:   0 Beta: 0.001\nStage:   1 Beta: 0.003\nStage:   2 Beta: 0.016\nStage:   3 Beta: 0.076\nStage:   4 Beta: 0.339\nStage:   5 Beta: 1.000\n/home/plaub/miniconda3/lib/python3.8/site-packages/arviz/data/io_pymc3.py:96: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  warnings.warn(\n</pre> <pre>CPU times: user 1.27 s, sys: 12.2 ms, total: 1.28 s\nWall time: 1.28 s\nThe number of individual claim sizes is  525\n</pre> <pre>Initializing SMC sampler...\nSampling 1 chain in 1 job\nStage:   0 Beta: 0.000\nStage:   1 Beta: 0.001\nStage:   2 Beta: 0.003\nStage:   3 Beta: 0.015\nStage:   4 Beta: 0.070\nStage:   5 Beta: 0.353\nStage:   6 Beta: 1.000\n/home/plaub/miniconda3/lib/python3.8/site-packages/arviz/data/io_pymc3.py:96: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  warnings.warn(\n</pre> <pre>CPU times: user 1.15 s, sys: 7.64 ms, total: 1.16 s\nWall time: 1.15 s\n</pre> Out[7]: model ss k \u03b2 marginal_log_likelihood BF model_probability 0 True weibull 50.0 0.336828 1.012060 -90.848702 1.000000e+00 9.999998e-01 0 True weibull 250.0 0.315289 0.886874 -410.439780 1.000000e+00 1.000000e+00 0 True gamma 50.0 0.194597 40.320096 -106.467064 1.648283e-07 1.648283e-07 0 True gamma 250.0 0.188851 38.701698 -463.865195 6.275335e-24 6.275335e-24 In\u00a0[8]: Copied! <pre># Frequency-Loss Model\n\u03b1, p, k, \u03b2 = 4, 2 / 3, 1 / 3, 1\nrg = default_rng(123)\nuData_10000 = abc.simulate_claim_sizes(rg, 10000, sev, \u03b8_sev)\nr_mle, m_mle, BIC = infer_gamma(uData_10000, [1, 1])\n\n\u03b8_plot = [[\u03b1, p, k, \u03b2], [\u03b1, p, np.NaN, np.NaN]]\n\u03b8_mle = [[np.NaN, np.NaN, np.NaN, np.NaN], [np.NaN, np.NaN, r_mle, m_mle]]\n</pre> # Frequency-Loss Model \u03b1, p, k, \u03b2 = 4, 2 / 3, 1 / 3, 1 rg = default_rng(123) uData_10000 = abc.simulate_claim_sizes(rg, 10000, sev, \u03b8_sev) r_mle, m_mle, BIC = infer_gamma(uData_10000, [1, 1])  \u03b8_plot = [[\u03b1, p, k, \u03b2], [\u03b1, p, np.NaN, np.NaN]] \u03b8_mle = [[np.NaN, np.NaN, np.NaN, np.NaN], [np.NaN, np.NaN, r_mle, m_mle]] In\u00a0[9]: Copied! <pre>params = ((\"\u03b1\", \"p\", \"k\", \"\u03b2\"), (\"\u03b1\", \"p\", \"r\", \"m\"))\n\nprior1 = abc.IndependentUniformPrior(\n    [(0, 20), (1e-3, 1), (1e-1, 10), (0, 20)], params[0]\n)\nmodel1 = abc.Model(\"negative binomial\", \"weibull\", psi, prior1)\n\nprior2 = abc.IndependentUniformPrior(\n    [(0, 20), (1e-3, 1), (1e-1, 10), (0, 50)], params[1]\n)\nmodel2 = abc.Model(\"negative binomial\", \"gamma\", psi, prior2)\n\nmodels = [model1, model2]\nmodel_names = [\"ABC negative binomial - weibull\", \"ABC negative binomial - gamma\"]\n</pre> params = ((\"\u03b1\", \"p\", \"k\", \"\u03b2\"), (\"\u03b1\", \"p\", \"r\", \"m\"))  prior1 = abc.IndependentUniformPrior(     [(0, 20), (1e-3, 1), (1e-1, 10), (0, 20)], params[0] ) model1 = abc.Model(\"negative binomial\", \"weibull\", psi, prior1)  prior2 = abc.IndependentUniformPrior(     [(0, 20), (1e-3, 1), (1e-1, 10), (0, 50)], params[1] ) model2 = abc.Model(\"negative binomial\", \"gamma\", psi, prior2)  models = [model1, model2] model_names = [\"ABC negative binomial - weibull\", \"ABC negative binomial - gamma\"] In\u00a0[10]: Copied! <pre>model_proba_abc = pd.DataFrame({\"model\": [], \"ss\": [], \"model_probability\": []})\ndfabc = pd.DataFrame(\n    {\"model\": [], \"ss\": [], \"weights\": [], \"\u03b1\": [], \"p\": [], \"param1\": [], \"param2\": []}\n)\n\nfor ss in sample_sizes:\n    xDataSS = df_agg.X[df_agg.time_period &lt;= ss].values\n\n    %time fit = abc.smc(numIters, popSizeModels, xDataSS, models, **smcArgs)\n\n    for k in range(len(models)):\n        weights = fit.weights[fit.models == k]\n        res_mp = pd.DataFrame(\n            {\n                \"model\": pd.Series([model_names[k]]),\n                \"ss\": np.array([ss]),\n                \"model_probability\": pd.Series(np.sum(fit.weights[fit.models == k])),\n            }\n        )\n\n        model_proba_abc = pd.concat([model_proba_abc, res_mp])\n\n        res_post_samples = pd.DataFrame(\n            {\n                \"model\": np.repeat(model_names[k], len(weights)),\n                \"ss\": np.repeat(ss, len(weights)),\n                \"weights\": weights / np.sum(weights),\n                \"\u03b1\": np.array(fit.samples)[fit.models == k, 0],\n                \"p\": np.array(fit.samples)[fit.models == k, 1],\n                \"param1\": np.array(fit.samples)[fit.models == k, 2],\n                \"param2\": np.array(fit.samples)[fit.models == k, 3],\n            }\n        )\n        dfabc = pd.concat([dfabc, res_post_samples])\n</pre> model_proba_abc = pd.DataFrame({\"model\": [], \"ss\": [], \"model_probability\": []}) dfabc = pd.DataFrame(     {\"model\": [], \"ss\": [], \"weights\": [], \"\u03b1\": [], \"p\": [], \"param1\": [], \"param2\": []} )  for ss in sample_sizes:     xDataSS = df_agg.X[df_agg.time_period &lt;= ss].values      %time fit = abc.smc(numIters, popSizeModels, xDataSS, models, **smcArgs)      for k in range(len(models)):         weights = fit.weights[fit.models == k]         res_mp = pd.DataFrame(             {                 \"model\": pd.Series([model_names[k]]),                 \"ss\": np.array([ss]),                 \"model_probability\": pd.Series(np.sum(fit.weights[fit.models == k])),             }         )          model_proba_abc = pd.concat([model_proba_abc, res_mp])          res_post_samples = pd.DataFrame(             {                 \"model\": np.repeat(model_names[k], len(weights)),                 \"ss\": np.repeat(ss, len(weights)),                 \"weights\": weights / np.sum(weights),                 \"\u03b1\": np.array(fit.samples)[fit.models == k, 0],                 \"p\": np.array(fit.samples)[fit.models == k, 1],                 \"param1\": np.array(fit.samples)[fit.models == k, 2],                 \"param2\": np.array(fit.samples)[fit.models == k, 3],             }         )         dfabc = pd.concat([dfabc, res_post_samples])  Starting ABC-SMC with population size of 1000 and sample size of 50 (~&gt; 50) on 40 processes.<p>Finished iteration 0, eps = 161.91, time = 6.0s / 0.1m, ESS = [519 481] -&gt; [337 163], numSims = 1000 \tmodel populations = [337, 163], model weights = [0.67 0.33]<p>Finished iteration 1, eps = 15.15, time = 3.0s / 0.0m, ESS = [743 550] -&gt; [419  81], numSims = 612440 \tmodel populations = [462, 92], model weights = [0.83 0.17]<p>Finished iteration 2, eps = 13.46, time = 1.0s / 0.0m, ESS = [1065  600] -&gt; [370 130], numSims = 639590 \tmodel populations = [462, 145], model weights = [0.75 0.25]<p>Finished iteration 3, eps = 12.83, time = 1.0s / 0.0m, ESS = [1103  319] -&gt; [261 239], numSims = 877396 \tmodel populations = [362, 331], model weights = [0.62 0.38]<p>Finished iteration 4, eps = 12.03, time = 1.0s / 0.0m, ESS = [855 570] -&gt; [174 326], numSims = 1499735 \tmodel populations = [223, 444], model weights = [0.52 0.48]<p>Finished iteration 5, eps = 11.46, time = 2.0s / 0.0m, ESS = [449 916] -&gt; [145 355], numSims = 1724297 \tmodel populations = [183, 551], model weights = [0.5 0.5]<p>Finished iteration 6, eps = 10.90, time = 2.0s / 0.0m, ESS = [256 918] -&gt; [157 343], numSims = 2470486 \tmodel populations = [212, 473], model weights = [0.55 0.45]<p>Finished iteration 7, eps = 10.90, time = 3.0s / 0.1m, ESS = [284 962] -&gt; [261 533], numSims = 3294649 \tmodel populations = [326, 674], model weights = [0.55 0.45] </p></p></p></p></p></p></p></p> <pre>Final population dists &lt;= 10.62, ESS = [261 533]\n\tmodel populations = [326, 674], model weights = [0.55 0.45]\nCPU times: user 10.6 s, sys: 1.39 s, total: 11.9 s\nWall time: 21.6 s\n</pre>  Starting ABC-SMC with population size of 1000 and sample size of 250 (~&gt; 250) on 40 processes.<p>Finished iteration 0, eps = 158.81, time = 1.0s / 0.0m, ESS = [519 481] -&gt; [339 161], numSims = 1000 \tmodel populations = [339, 161], model weights = [0.68 0.32]<p>Finished iteration 1, eps = 12.51, time = 7.0s / 0.1m, ESS = [749 546] -&gt; [399 101], numSims = 3066721 \tmodel populations = [443, 115], model weights = [0.78 0.22]<p>Finished iteration 2, eps = 10.92, time = 3.0s / 0.1m, ESS = [1035  580] -&gt; [339 161], numSims = 2925521 \tmodel populations = [476, 184], model weights = [0.71 0.29]<p>Finished iteration 3, eps = 10.15, time = 6.0s / 0.1m, ESS = [1145  368] -&gt; [248 252], numSims = 4223080 \tmodel populations = [343, 327], model weights = [0.56 0.44]<p>Finished iteration 4, eps = 9.01, time = 8.0s / 0.1m, ESS = [920 648] -&gt; [167 333], numSims = 5836570 \tmodel populations = [203, 399], model weights = [0.44 0.56]<p>Finished iteration 5, eps = 8.05, time = 5.0s / 0.1m, ESS = [ 303 1035] -&gt; [ 59 441], numSims = 3371523 \tmodel populations = [102, 569], model weights = [0.46 0.54]<p>Finished iteration 6, eps = 7.30, time = 7.0s / 0.1m, ESS = [ 124 1367] -&gt; [ 45 455], numSims = 4398979 \tmodel populations = [59, 519], model weights = [0.55 0.45]<p>Finished iteration 7, eps = 7.30, time = 7.0s / 0.1m, ESS = [ 130 1481] -&gt; [104 726], numSims = 3788676 \tmodel populations = [138, 862], model weights = [0.59 0.41] </p></p></p></p></p></p></p></p> <pre>Final population dists &lt;= 7.07, ESS = [104 726]\n\tmodel populations = [138, 862], model weights = [0.59 0.41]\nCPU times: user 9.46 s, sys: 655 ms, total: 10.1 s\nWall time: 44.5 s\n</pre> In\u00a0[11]: Copied! <pre>for l in range(len(models)):\n    fig, axs = plt.subplots(1, len(params[l]), tight_layout=True)\n    prior = models[l].prior\n\n    for k in range(len(params[l])):\n        pLims = [prior.marginals[k].isf(1), prior.marginals[k].isf(0)]\n        axs[k].set_xlim(pLims)\n\n        for i, ss in enumerate(sample_sizes):\n            selector = (dfabc.ss == ss) &amp; (dfabc.model == model_names[l])\n            sample = np.array(dfabc[[\"\u03b1\", \"p\", \"param1\", \"param2\"]])[selector, k]\n            weights = dfabc.weights[selector].values\n            dataResampled, xs, ys = abc.resample_and_kde(\n                sample, weights / sum(weights), clip=pLims\n            )\n            axs[k].plot(xs, ys)\n            axs[k].axvline(\u03b8_plot[l][k], **trueStyle)\n            axs[k].axvline(\u03b8_mle[l][k], **mleStyle)\n\n            axs[k].set_title(\"$\" + params[l][k] + \"$\")\n            axs[k].set_yticks([])\n\n    sns.despine(left=True)\n    # plt.save_cropped(f\"../Figures/hist-negbin-weibull-model-selection-{l}.pdf\")\n</pre> for l in range(len(models)):     fig, axs = plt.subplots(1, len(params[l]), tight_layout=True)     prior = models[l].prior      for k in range(len(params[l])):         pLims = [prior.marginals[k].isf(1), prior.marginals[k].isf(0)]         axs[k].set_xlim(pLims)          for i, ss in enumerate(sample_sizes):             selector = (dfabc.ss == ss) &amp; (dfabc.model == model_names[l])             sample = np.array(dfabc[[\"\u03b1\", \"p\", \"param1\", \"param2\"]])[selector, k]             weights = dfabc.weights[selector].values             dataResampled, xs, ys = abc.resample_and_kde(                 sample, weights / sum(weights), clip=pLims             )             axs[k].plot(xs, ys)             axs[k].axvline(\u03b8_plot[l][k], **trueStyle)             axs[k].axvline(\u03b8_mle[l][k], **mleStyle)              axs[k].set_title(\"$\" + params[l][k] + \"$\")             axs[k].set_yticks([])      sns.despine(left=True)     # plt.save_cropped(f\"../Figures/hist-negbin-weibull-model-selection-{l}.pdf\") In\u00a0[12]: Copied! <pre>params = ((\"k\", \"\u03b2\"), (\"r\", \"m\"))\n\nprior1 = abc.IndependentUniformPrior([(1e-1, 10), (0, 20)], params[0])\nprior2 = abc.IndependentUniformPrior([(0, 10), (0, 50)], params[1])\n\nmodel_names = (\"ABC with freqs - weibull\", \"ABC with freqs - gamma\")\n</pre> params = ((\"k\", \"\u03b2\"), (\"r\", \"m\"))  prior1 = abc.IndependentUniformPrior([(1e-1, 10), (0, 20)], params[0]) prior2 = abc.IndependentUniformPrior([(0, 10), (0, 50)], params[1])  model_names = (\"ABC with freqs - weibull\", \"ABC with freqs - gamma\") In\u00a0[13]: Copied! <pre>model_proba_abc_freq = pd.DataFrame({\"model\": [], \"ss\": [], \"model_probability\": []})\ndfabc_freq = pd.DataFrame(\n    {\"model\": [], \"ss\": [], \"weights\": [], \"param1\": [], \"param2\": []}\n)\n\nfor ss in sample_sizes:\n    xDataSS = df_agg.X[df_agg.time_period &lt;= ss].values\n    nData = df_agg.N[df_agg.time_period &lt;= ss].values\n\n    model1 = abc.Model(nData, \"weibull\", psi, prior1)\n    model2 = abc.Model(nData, \"gamma\", psi, prior2)\n    models = [model1, model2]\n\n    %time fit = abc.smc(numItersData, popSizeModels, xDataSS, models, **smcArgs)\n\n    for k in range(len(models)):\n        weights = fit.weights[fit.models == k]\n        res_mp = pd.DataFrame(\n            {\n                \"model\": pd.Series([model_names[k]]),\n                \"ss\": np.array([ss]),\n                \"model_probability\": pd.Series(np.sum(fit.weights[fit.models == k])),\n            }\n        )\n\n        model_proba_abc_freq = pd.concat([model_proba_abc_freq, res_mp])\n\n        res_post_samples = pd.DataFrame(\n            {\n                \"model\": np.repeat(model_names[k], len(weights)),\n                \"ss\": np.repeat(ss, len(weights)),\n                \"weights\": weights / np.sum(weights),\n                \"param1\": np.array(fit.samples)[fit.models == k, 0],\n                \"param2\": np.array(fit.samples)[fit.models == k, 1],\n            }\n        )\n        dfabc_freq = pd.concat([dfabc_freq, res_post_samples])\n</pre> model_proba_abc_freq = pd.DataFrame({\"model\": [], \"ss\": [], \"model_probability\": []}) dfabc_freq = pd.DataFrame(     {\"model\": [], \"ss\": [], \"weights\": [], \"param1\": [], \"param2\": []} )  for ss in sample_sizes:     xDataSS = df_agg.X[df_agg.time_period &lt;= ss].values     nData = df_agg.N[df_agg.time_period &lt;= ss].values      model1 = abc.Model(nData, \"weibull\", psi, prior1)     model2 = abc.Model(nData, \"gamma\", psi, prior2)     models = [model1, model2]      %time fit = abc.smc(numItersData, popSizeModels, xDataSS, models, **smcArgs)      for k in range(len(models)):         weights = fit.weights[fit.models == k]         res_mp = pd.DataFrame(             {                 \"model\": pd.Series([model_names[k]]),                 \"ss\": np.array([ss]),                 \"model_probability\": pd.Series(np.sum(fit.weights[fit.models == k])),             }         )          model_proba_abc_freq = pd.concat([model_proba_abc_freq, res_mp])          res_post_samples = pd.DataFrame(             {                 \"model\": np.repeat(model_names[k], len(weights)),                 \"ss\": np.repeat(ss, len(weights)),                 \"weights\": weights / np.sum(weights),                 \"param1\": np.array(fit.samples)[fit.models == k, 0],                 \"param2\": np.array(fit.samples)[fit.models == k, 1],             }         )         dfabc_freq = pd.concat([dfabc_freq, res_post_samples])  Starting ABC-SMC with population size of 1000 and sample size of 50 (~&gt; 50) on 40 processes.<p>Finished iteration 0, eps = 31.52, time = 1.0s / 0.0m, ESS = [519 481] -&gt; [431  69], numSims = 1000 \tmodel populations = [431, 69], model weights = [0.86 0.14]<p>Finished iteration 1, eps = 14.61, time = 1.0s / 0.0m, ESS = [712 585] -&gt; [ 52 448], numSims = 1210651 \tmodel populations = [52, 573], model weights = [0.16 0.84]<p>Finished iteration 2, eps = 11.63, time = 1.0s / 0.0m, ESS = [ 490 1247] -&gt; [ 79 421], numSims = 177485 \tmodel populations = [106, 470], model weights = [0.14 0.86]<p>Finished iteration 3, eps = 10.77, time = 1.0s / 0.0m, ESS = [647 928] -&gt; [278 222], numSims = 39556 \tmodel populations = [321, 298], model weights = [0.2 0.8]<p>Finished iteration 4, eps = 9.68, time = 1.0s / 0.0m, ESS = [966 697] -&gt; [397 103], numSims = 72111 \tmodel populations = [455, 113], model weights = [0.39 0.61]<p>Finished iteration 5, eps = 8.11, time = 1.0s / 0.0m, ESS = [1240  510] -&gt; [471  29], numSims = 166508 \tmodel populations = [521, 30], model weights = [0.75 0.25]<p>Finished iteration 6, eps = 6.41, time = 1.0s / 0.0m, ESS = [1362  223] -&gt; [495   5], numSims = 376363 \tmodel populations = [626, 5], model weights = [0.97 0.03]<p>Finished iteration 7, eps = 4.57, time = 1.0s / 0.0m, ESS = [1626   68] -&gt; [498   0], numSims = 925220 \tmodel populations = [548, 0], model weights = [1. 0.]<p>Finished iteration 8, eps = 3.36, time = 2.0s / 0.0m, ESS = [1690   19] -&gt; [499   0], numSims = 2025936 \tmodel populations = [594, 0], model weights = [1. 0.]<p>Finished iteration 9, eps = 2.48, time = 3.0s / 0.1m, ESS = [1676    0] -&gt; [500   0], numSims = 4807213 \tmodel populations = [573, 0], model weights = [1. 0.]<p>Finished iteration 10, eps = 2.48, time = 7.0s / 0.1m, ESS = [1630    0] -&gt; [874   0], numSims = 10158051 \tmodel populations = [1000, 0], model weights = [1. 0.] </p></p></p></p></p></p></p></p></p></p></p> <pre>Final population dists &lt;= 2.24, ESS = [874   0]\n\tmodel populations = [1000, 0], model weights = [1. 0.]\nCPU times: user 12.3 s, sys: 983 ms, total: 13.3 s\nWall time: 20.9 s\n</pre>  Starting ABC-SMC with population size of 1000 and sample size of 250 (~&gt; 250) on 40 processes.<p>Finished iteration 0, eps = 26.17, time = 1.0s / 0.0m, ESS = [519 481] -&gt; [430  70], numSims = 1000 \tmodel populations = [430, 70], model weights = [0.86 0.14]<p>Finished iteration 1, eps = 12.65, time = 5.0s / 0.1m, ESS = [779 511] -&gt; [125 375], numSims = 4312120 \tmodel populations = [125, 504], model weights = [0.37 0.63]<p>Finished iteration 2, eps = 7.95, time = 4.0s / 0.1m, ESS = [ 501 1343] -&gt; [ 66 434], numSims = 2598910 \tmodel populations = [68, 486], model weights = [0.16 0.84]<p>Finished iteration 3, eps = 6.94, time = 1.0s / 0.0m, ESS = [ 383 1219] -&gt; [268 232], numSims = 55989 \tmodel populations = [426, 262], model weights = [0.31 0.69]<p>Finished iteration 4, eps = 5.34, time = 1.0s / 0.0m, ESS = [1029  698] -&gt; [488  12], numSims = 89275 \tmodel populations = [558, 12], model weights = [0.89 0.11]<p>Finished iteration 5, eps = 4.45, time = 1.0s / 0.0m, ESS = [1343  264] -&gt; [599   0], numSims = 165564 \tmodel populations = [701, 0], model weights = [1. 0.]<p>Finished iteration 6, eps = 4.45, time = 1.0s / 0.0m, ESS = [1721   21] -&gt; [1336    0], numSims = 267283 \tmodel populations = [1505, 0], model weights = [1. 0.]<p>Finished iteration 7, eps = 3.27, time = 1.0s / 0.0m, ESS = [1777    0] -&gt; [500   0], numSims = 272925 \tmodel populations = [525, 0], model weights = [1. 0.]<p>Finished iteration 8, eps = 2.84, time = 1.0s / 0.0m, ESS = [1682    0] -&gt; [500   0], numSims = 783279 \tmodel populations = [603, 0], model weights = [1. 0.]<p>Finished iteration 9, eps = 2.46, time = 2.0s / 0.0m, ESS = [1691    0] -&gt; [500   0], numSims = 1960341 \tmodel populations = [583, 0], model weights = [1. 0.]<p>Finished iteration 10, eps = 2.46, time = 4.0s / 0.1m, ESS = [1753    0] -&gt; [872   0], numSims = 3912166 \tmodel populations = [1000, 0], model weights = [1. 0.] </p></p></p></p></p></p></p></p></p></p></p> <pre>/home/plaub/miniconda3/lib/python3.8/site-packages/approxbayescomp/smc.py:371: RuntimeWarning: invalid value encountered in double_scalars\n  np.sum(weights[ms == m]) ** 2\n</pre> <pre>Final population dists &lt;= 2.35, ESS = [872   0]\n\tmodel populations = [1000, 0], model weights = [1. 0.]\nCPU times: user 11 s, sys: 904 ms, total: 11.9 s\nWall time: 22.4 s\n</pre> In\u00a0[14]: Copied! <pre>for l in range(len(models)):\n    modelName = model_names[l]\n    prior = models[l].prior\n    fig, axs = plt.subplots(1, len(params[l]), tight_layout=True)\n\n    for k in range(len(params[l])):\n        pLims = [prior.marginals[k].isf(1), prior.marginals[k].isf(0)]\n        # axs[k].set_xlim(pLims)\n\n        for ss in sample_sizes:\n            sampleData = dfabc_freq.query(\"ss == @ss &amp; model == @modelName\")\n            selector = (dfabc_freq.ss == ss) &amp; (dfabc_freq.model == model_names[l])\n            sample = np.array(dfabc_freq[[\"param1\", \"param2\"]])[selector, k]\n            weights = dfabc_freq.weights[selector].values\n            if sampleData.shape[0] &gt; 1:\n                dataResampled, xs, ys = abc.resample_and_kde(\n                    sample, weights / sum(weights), clip=pLims\n                )\n                axs[k].plot(xs, ys)\n\n            axs[k].axvline(\u03b8_plot[l][k + 2], **trueStyle)\n            axs[k].axvline(\u03b8_mle[l][k + 2], **mleStyle)\n\n            axs[k].set_title(\n                \"$\" + params[l][k] + f\"\\\\in ({pLims[0]:.0f}, {pLims[1]:.0f})$\"\n            )\n            axs[k].set_yticks([])\n\n    sns.despine(left=True)\n    # plt.save_cropped(f\"../Figures/hist-freqs-weibull-model-selection-{l}.pdf\")\nBayesian_Summary\n</pre> for l in range(len(models)):     modelName = model_names[l]     prior = models[l].prior     fig, axs = plt.subplots(1, len(params[l]), tight_layout=True)      for k in range(len(params[l])):         pLims = [prior.marginals[k].isf(1), prior.marginals[k].isf(0)]         # axs[k].set_xlim(pLims)          for ss in sample_sizes:             sampleData = dfabc_freq.query(\"ss == @ss &amp; model == @modelName\")             selector = (dfabc_freq.ss == ss) &amp; (dfabc_freq.model == model_names[l])             sample = np.array(dfabc_freq[[\"param1\", \"param2\"]])[selector, k]             weights = dfabc_freq.weights[selector].values             if sampleData.shape[0] &gt; 1:                 dataResampled, xs, ys = abc.resample_and_kde(                     sample, weights / sum(weights), clip=pLims                 )                 axs[k].plot(xs, ys)              axs[k].axvline(\u03b8_plot[l][k + 2], **trueStyle)             axs[k].axvline(\u03b8_mle[l][k + 2], **mleStyle)              axs[k].set_title(                 \"$\" + params[l][k] + f\"\\\\in ({pLims[0]:.0f}, {pLims[1]:.0f})$\"             )             axs[k].set_yticks([])      sns.despine(left=True)     # plt.save_cropped(f\"../Figures/hist-freqs-weibull-model-selection-{l}.pdf\") Bayesian_Summary In\u00a0[16]: Copied! <pre>model_proba_df = pd.concat(\n    [\n        Bayesian_Summary[[\"model\", \"ss\", \"model_probability\"]],\n        model_proba_abc,\n        model_proba_abc_freq,\n    ]\n)\nmodel_proba_df = model_proba_df[\n    np.char.find(model_proba_df.model.tolist(), \"weibull\") &gt; -1\n]\n\nmodel_proba_df.model = model_proba_df.model.replace(\n    {\n        \"True weibull\": \"True\\n(w/ $N$'s, $U$'s)\",\n        \"ABC negative binomial - weibull\": \"ABC\\n(w/ $X$'s)\",\n        \"ABC with freqs - weibull\": \"ABC\\n(w/ $X$'s, $N$'s)\",\n    }\n)\n\nmodel_proba_df = model_proba_df.sort_values(\"model\")\n\nfig, ax = plt.subplots(1, 1, tight_layout=True)\n\ng = sns.barplot(\n    x=\"model\",\n    y=\"model_probability\",\n    hue=\"ss\",\n    data=model_proba_df,\n    #     legend=False,\n    ax=ax,\n)\nplt.legend([], frameon=False)\nplt.ylabel(\"\")\nplt.xlabel(\"\")\nplt.title(\"\")\n\nsns.despine()\nsave_cropped(\"../Figures-Slides/barplot-negbin-weibull-model-selection.pdf\")\n</pre> model_proba_df = pd.concat(     [         Bayesian_Summary[[\"model\", \"ss\", \"model_probability\"]],         model_proba_abc,         model_proba_abc_freq,     ] ) model_proba_df = model_proba_df[     np.char.find(model_proba_df.model.tolist(), \"weibull\") &gt; -1 ]  model_proba_df.model = model_proba_df.model.replace(     {         \"True weibull\": \"True\\n(w/ $N$'s, $U$'s)\",         \"ABC negative binomial - weibull\": \"ABC\\n(w/ $X$'s)\",         \"ABC with freqs - weibull\": \"ABC\\n(w/ $X$'s, $N$'s)\",     } )  model_proba_df = model_proba_df.sort_values(\"model\")  fig, ax = plt.subplots(1, 1, tight_layout=True)  g = sns.barplot(     x=\"model\",     y=\"model_probability\",     hue=\"ss\",     data=model_proba_df,     #     legend=False,     ax=ax, ) plt.legend([], frameon=False) plt.ylabel(\"\") plt.xlabel(\"\") plt.title(\"\")  sns.despine() save_cropped(\"../Figures-Slides/barplot-negbin-weibull-model-selection.pdf\") In\u00a0[17]: Copied! <pre>model_proba_df = pd.concat(\n    [\n        Bayesian_Summary[[\"model\", \"ss\", \"model_probability\"]],\n        model_proba_abc,\n        model_proba_abc_freq,\n    ]\n)\nprint(\n    pd.pivot_table(\n        model_proba_df,\n        values=\"model_probability\",\n        index=[\"ss\"],\n        columns=[\"model\"],\n        aggfunc=np.sum,\n    ).to_latex()\n)\n</pre> model_proba_df = pd.concat(     [         Bayesian_Summary[[\"model\", \"ss\", \"model_probability\"]],         model_proba_abc,         model_proba_abc_freq,     ] ) print(     pd.pivot_table(         model_proba_df,         values=\"model_probability\",         index=[\"ss\"],         columns=[\"model\"],         aggfunc=np.sum,     ).to_latex() ) <pre>\\begin{tabular}{lrrrrrr}\n\\toprule\nmodel &amp;  ABC negative binomial - gamma &amp;  ABC negative binomial - weibull &amp;  ABC with freqs - gamma &amp;  ABC with freqs - weibull &amp;    True gamma &amp;  True weibull \\\\\nss    &amp;                                &amp;                                  &amp;                         &amp;                           &amp;               &amp;               \\\\\n\\midrule\n50.0  &amp;                       0.453952 &amp;                         0.546048 &amp;                     0.0 &amp;                       1.0 &amp;  1.648283e-07 &amp;           1.0 \\\\\n250.0 &amp;                       0.405963 &amp;                         0.594037 &amp;                     0.0 &amp;                       1.0 &amp;  6.275335e-24 &amp;           1.0 \\\\\n\\bottomrule\n\\end{tabular}\n\n</pre>"},{"location":"model-selection/#model-selection-wip","title":"Model Selection (WIP)\u00b6","text":""},{"location":"model-selection/#true-posterior-samples","title":"True posterior samples\u00b6","text":"<p>We run a Bayesian analysis on the individual claim data and compuet the model probabilities when a Weibull or a gamma distribution is assumed. The prior distribution on the parameters are taken as independent uniform distribution (as in the ABC approach).</p>"},{"location":"model-selection/#fitting-a-weibull-and-a-gamma-model-to-the-individual-loss-data","title":"Fitting a Weibull and a gamma model to the individual loss data\u00b6","text":""},{"location":"model-selection/#abc-posterior-for-choosing-between-weibull-and-gamma-to-model-the-claim-sizes","title":"ABC posterior for choosing between Weibull and gamma to model the claim sizes\u00b6","text":""},{"location":"seasonal-claim-arrivals/","title":"Seasonal Claim Arrivals (WIP)","text":"<p>Here, we set the claim arrival process to be governed by an inhomogenous Poisson process $N_t$ with intensity function</p> <p>$$ \\lambda(t) = b[\\sin(2\\pi c t) + 1] $$</p> <p>The claim frequency data $n_s,\\,s = 1,\\ldots, t$ correspond to the increments of this non homogeneous Poisson process. These are independent Poisson variables $\\text{Pois}[\\mu(s)]$ with</p> <p>$$ \\mu(s) = \\int_{s-1}^s\\lambda(u)du = b + \\frac{b}{2\\pi c}\\left[\\cos(2\\pi(s-1) c) - \\cos(2\\pi s c)\\right],\\quad s = 1,\\ldots, t $$</p> In\u00a0[1]: Copied! <pre>from preamble import *\n\n%config InlineBackend.figure_format = 'retina'\n%load_ext lab_black\n</pre> from preamble import *  %config InlineBackend.figure_format = 'retina' %load_ext lab_black In\u00a0[4]: Copied! <pre>numIters = 10\npopSize = 1000\nsmcArgs = {\"verbose\": True, \"numProcs\": 40}\n</pre> numIters = 10 popSize = 1000 smcArgs = {\"verbose\": True, \"numProcs\": 40} In\u00a0[5]: Copied! <pre>rg = default_rng(123)\nsample_sizes = [50, 250]\nT = sample_sizes[-1]\n\na, b, c, \u03bc, \u03c3 = 1, 5, 1 / 50, 0, 0.5\n\n\u03b8_True = a, b, c, \u03bc, \u03c3\n\nfreq, sev, theta = \"cyclical_poisson\", \"lognormal\", [a, b, c, \u03bc, \u03c3]\n\nfreqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, theta)\n\n# Aggregation process\npsi = abc.Psi(\"sum\")\n\nxData = abc.compute_psi(freqs, sevs, psi)\n</pre> rg = default_rng(123) sample_sizes = [50, 250] T = sample_sizes[-1]  a, b, c, \u03bc, \u03c3 = 1, 5, 1 / 50, 0, 0.5  \u03b8_True = a, b, c, \u03bc, \u03c3  freq, sev, theta = \"cyclical_poisson\", \"lognormal\", [a, b, c, \u03bc, \u03c3]  freqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, theta)  # Aggregation process psi = abc.Psi(\"sum\")  xData = abc.compute_psi(freqs, sevs, psi) In\u00a0[6]: Copied! <pre>t = np.arange(T)\nmus = (\n    a\n    + b\n    + b * (np.cos(2 * np.pi * t * c) - np.cos(2 * np.pi * (t + 1) * c)) / 2 / np.pi / c\n)\nexpXs = np.exp(\u03bc + \u03c3 ** 2 / 2) * mus\n# expXs = \u03bc * mus\n\nfig, ax = plt.subplots(1, 1, tight_layout=True)\nax.plot(t, expXs, label=\"Expected value for $x_s$\", c=\"k\")\nax.scatter(t, xData, label=\"Observed $x_s$\")\nax.set(xlabel=\"$t$\", title=\"\")\nax.xaxis.set_label_coords(1.05, -0.025)\n# ax.grid()\n# handles, labels = ax.get_legend_handles_labels()\n# fig.legend(\n#     handles, labels, ncol=2, borderaxespad=-0.5, loc=\"lower center\", frameon=False\n# )\nsns.despine()\nsave_cropped(\"../Figures/cyclical_poisson_lognormal_data.pdf\")\n</pre> t = np.arange(T) mus = (     a     + b     + b * (np.cos(2 * np.pi * t * c) - np.cos(2 * np.pi * (t + 1) * c)) / 2 / np.pi / c ) expXs = np.exp(\u03bc + \u03c3 ** 2 / 2) * mus # expXs = \u03bc * mus  fig, ax = plt.subplots(1, 1, tight_layout=True) ax.plot(t, expXs, label=\"Expected value for $x_s$\", c=\"k\") ax.scatter(t, xData, label=\"Observed $x_s$\") ax.set(xlabel=\"$t$\", title=\"\") ax.xaxis.set_label_coords(1.05, -0.025) # ax.grid() # handles, labels = ax.get_legend_handles_labels() # fig.legend( #     handles, labels, ncol=2, borderaxespad=-0.5, loc=\"lower center\", frameon=False # ) sns.despine() save_cropped(\"../Figures/cyclical_poisson_lognormal_data.pdf\") In\u00a0[7]: Copied! <pre>params = (\"a\", \"b\", \"c\", \"\\mu\", \"\\sigma\")\nprior = abc.IndependentUniformPrior(\n    [(0, 50), (0, 50), (1 / 1000, 1 / 10), (-10, 10), (0, 3)]\n)\nmodel = abc.Model(\"cyclical_poisson\", \"lognormal\", psi, prior)\n</pre> params = (\"a\", \"b\", \"c\", \"\\mu\", \"\\sigma\") prior = abc.IndependentUniformPrior(     [(0, 50), (0, 50), (1 / 1000, 1 / 10), (-10, 10), (0, 3)] ) model = abc.Model(\"cyclical_poisson\", \"lognormal\", psi, prior) In\u00a0[8]: Copied! <pre>%%time \nAR = 2\ndfABC = pd.DataFrame()\n\nfor ss in sample_sizes:\n    xDataSS = xData[:ss]\n\n    #scale_x = 100\n    #scale_t = scale_x * (np.max(xData) - np.min(xData)) / (ss - 1) * AR\n    gamma = (np.max(xDataSS) - np.min(xDataSS)) / (ss - 1) * AR\n\n\n    %time fit1 = abc.smc(numIters, popSize, xDataSS, model, sumstats=abc.identity, **smcArgs)\n    %time fit2 = abc.smc(numIters, popSize, xDataSS, model, sumstats=abc.wrap_ss_curve_matching(gamma),\\\n                          distance=abc.wass_2Ddist, **smcArgs)\n    %time fit3 = abc.smc(numIters, popSize, xDataSS, model, **smcArgs)\n    fits = (fit1,fit2, fit3)\n\n    \n    distances = [\"L1\", \"curve_matching\", \"sorted_data\"]\n    for k, fit in enumerate(fits):\n\n        columns = {\n            \"ss\": np.repeat(ss, popSize),\n            \"weights\": fit.weights,\n            \"distance\": np.repeat(distances[k], popSize),\n        }\n\n        for i, param in enumerate(params):\n            columns[param] = fit.samples[:, i]\n\n        res = pd.DataFrame(columns)\n\n        dfABC = pd.concat([dfABC, res])\n</pre> %%time  AR = 2 dfABC = pd.DataFrame()  for ss in sample_sizes:     xDataSS = xData[:ss]      #scale_x = 100     #scale_t = scale_x * (np.max(xData) - np.min(xData)) / (ss - 1) * AR     gamma = (np.max(xDataSS) - np.min(xDataSS)) / (ss - 1) * AR       %time fit1 = abc.smc(numIters, popSize, xDataSS, model, sumstats=abc.identity, **smcArgs)     %time fit2 = abc.smc(numIters, popSize, xDataSS, model, sumstats=abc.wrap_ss_curve_matching(gamma),\\                           distance=abc.wass_2Ddist, **smcArgs)     %time fit3 = abc.smc(numIters, popSize, xDataSS, model, **smcArgs)     fits = (fit1,fit2, fit3)           distances = [\"L1\", \"curve_matching\", \"sorted_data\"]     for k, fit in enumerate(fits):          columns = {             \"ss\": np.repeat(ss, popSize),             \"weights\": fit.weights,             \"distance\": np.repeat(distances[k], popSize),         }          for i, param in enumerate(params):             columns[param] = fit.samples[:, i]          res = pd.DataFrame(columns)          dfABC = pd.concat([dfABC, res])  Starting ABC-SMC with population size of 1000 and sample size of 50 (~&gt; 50) on 40 processes.<p>Finished iteration 0, eps = 241.76, time = 8.0s / 0.1m, ESS = 1000 -&gt; 500, numSims = 1000<p>Finished iteration 1, eps = 6.32, time = 9.0s / 0.1m, ESS = 1288 -&gt; 500, numSims = 3697354<p>Finished iteration 2, eps = 5.80, time = 7.0s / 0.1m, ESS = 1791 -&gt; 500, numSims = 2889878<p>Finished iteration 3, eps = 5.30, time = 1.0s / 0.0m, ESS = 1602 -&gt; 500, numSims = 93969<p>Finished iteration 4, eps = 4.97, time = 3.0s / 0.0m, ESS = 1497 -&gt; 500, numSims = 156253<p>Finished iteration 5, eps = 4.41, time = 3.0s / 0.1m, ESS = 1539 -&gt; 500, numSims = 283890<p>Finished iteration 6, eps = 3.67, time = 5.0s / 0.1m, ESS = 1522 -&gt; 500, numSims = 114461<p>Finished iteration 7, eps = 3.15, time = 1.0s / 0.0m, ESS = 1535 -&gt; 500, numSims = 66093<p>Finished iteration 8, eps = 2.81, time = 1.0s / 0.0m, ESS = 1526 -&gt; 500, numSims = 76549<p>Finished iteration 9, eps = 2.57, time = 3.0s / 0.1m, ESS = 1531 -&gt; 500, numSims = 97035<p>Finished iteration 10, eps = 2.57, time = 1.0s / 0.0m, ESS = 1547 -&gt; 793, numSims = 127006 </p></p></p></p></p></p></p></p></p></p></p> <pre>Final population dists &lt;= 2.51, ESS = 793\nCPU times: user 13.8 s, sys: 2.01 s, total: 15.8 s\nWall time: 43.1 s\n</pre>  Starting ABC-SMC with population size of 1000 and sample size of 50 (~&gt; 100) on 40 processes.<p>Finished iteration 0, eps = 241.76, time = 3.0s / 0.1m, ESS = 1000 -&gt; 500, numSims = 1000<p>Finished iteration 1, eps = 6.27, time = 8.0s / 0.1m, ESS = 1288 -&gt; 500, numSims = 3697354<p>Finished iteration 2, eps = 5.39, time = 8.0s / 0.1m, ESS = 1805 -&gt; 500, numSims = 2828178<p>Finished iteration 3, eps = 4.52, time = 1.0s / 0.0m, ESS = 1703 -&gt; 500, numSims = 100196<p>Finished iteration 4, eps = 4.07, time = 1.0s / 0.0m, ESS = 1543 -&gt; 500, numSims = 133527<p>Finished iteration 5, eps = 3.73, time = 2.0s / 0.0m, ESS = 1328 -&gt; 500, numSims = 254258<p>Finished iteration 6, eps = 2.99, time = 2.0s / 0.0m, ESS = 1235 -&gt; 500, numSims = 456047<p>Finished iteration 7, eps = 2.49, time = 1.0s / 0.0m, ESS = 1338 -&gt; 500, numSims = 73337<p>Finished iteration 8, eps = 2.06, time = 1.0s / 0.0m, ESS = 1395 -&gt; 500, numSims = 85354<p>Finished iteration 9, eps = 1.85, time = 4.0s / 0.1m, ESS = 1450 -&gt; 500, numSims = 112846<p>Finished iteration 10, eps = 1.85, time = 3.0s / 0.0m, ESS = 1497 -&gt; 805, numSims = 142501 </p></p></p></p></p></p></p></p></p></p></p> <pre>Final population dists &lt;= 1.77, ESS = 805\nCPU times: user 14.2 s, sys: 923 ms, total: 15.1 s\nWall time: 34.9 s\n</pre>  Starting ABC-SMC with population size of 1000 and sample size of 50 (~&gt; 50) on 40 processes.<p>Finished iteration 0, eps = 241.76, time = 2.0s / 0.0m, ESS = 1000 -&gt; 500, numSims = 1000<p>Finished iteration 1, eps = 6.21, time = 9.0s / 0.2m, ESS = 1288 -&gt; 500, numSims = 3697354<p>Finished iteration 2, eps = 4.75, time = 5.0s / 0.1m, ESS = 1778 -&gt; 500, numSims = 2798020<p>Finished iteration 3, eps = 2.89, time = 1.0s / 0.0m, ESS = 1633 -&gt; 500, numSims = 91861<p>Finished iteration 4, eps = 1.79, time = 2.0s / 0.0m, ESS = 1494 -&gt; 500, numSims = 115674<p>Finished iteration 5, eps = 1.50, time = 5.0s / 0.1m, ESS = 1160 -&gt; 500, numSims = 153237<p>Finished iteration 6, eps = 1.07, time = 1.0s / 0.0m, ESS = 1169 -&gt; 500, numSims = 211308<p>Finished iteration 7, eps = 1.02, time = 1.0s / 0.0m, ESS = 1092 -&gt; 500, numSims = 327592<p>Finished iteration 8, eps = 0.92, time = 4.0s / 0.1m, ESS = 1070 -&gt; 500, numSims = 509367<p>Finished iteration 9, eps = 0.80, time = 4.0s / 0.1m, ESS = 1435 -&gt; 500, numSims = 834512<p>Finished iteration 10, eps = 0.80, time = 5.0s / 0.1m, ESS = 1402 -&gt; 732, numSims = 1184048 </p></p></p></p></p></p></p></p></p></p></p> <pre>Final population dists &lt;= 0.76, ESS = 732\nCPU times: user 15.1 s, sys: 1.05 s, total: 16.1 s\nWall time: 41.4 s\n</pre>  Starting ABC-SMC with population size of 1000 and sample size of 250 (~&gt; 250) on 40 processes.<p>Finished iteration 0, eps = 227.91, time = 1.0s / 0.0m, ESS = 1000 -&gt; 500, numSims = 1000<p>Finished iteration 1, eps = 6.65, time = 35.0s / 0.6m, ESS = 1295 -&gt; 500, numSims = 8540117<p>Finished iteration 2, eps = 6.11, time = 30.0s / 0.5m, ESS = 1815 -&gt; 500, numSims = 7054912<p>Finished iteration 3, eps = 5.55, time = 4.0s / 0.1m, ESS = 1697 -&gt; 500, numSims = 149951<p>Finished iteration 4, eps = 5.23, time = 2.0s / 0.0m, ESS = 1604 -&gt; 500, numSims = 187296<p>Finished iteration 5, eps = 5.09, time = 3.0s / 0.1m, ESS = 1569 -&gt; 500, numSims = 257713<p>Finished iteration 6, eps = 4.97, time = 2.0s / 0.0m, ESS = 1575 -&gt; 500, numSims = 543919<p>Finished iteration 7, eps = 4.87, time = 5.0s / 0.1m, ESS = 1372 -&gt; 500, numSims = 950479<p>Finished iteration 8, eps = 4.18, time = 7.0s / 0.1m, ESS = 1196 -&gt; 500, numSims = 1448716<p>Finished iteration 9, eps = 3.47, time = 2.0s / 0.0m, ESS = 1397 -&gt; 500, numSims = 113467<p>Finished iteration 10, eps = 3.47, time = 1.0s / 0.0m, ESS = 1680 -&gt; 844, numSims = 93185 </p></p></p></p></p></p></p></p></p></p></p> <pre>Final population dists &lt;= 3.25, ESS = 844\nCPU times: user 15.1 s, sys: 1.08 s, total: 16.1 s\nWall time: 1min 30s\n</pre>  Starting ABC-SMC with population size of 1000 and sample size of 250 (~&gt; 500) on 40 processes.<p>Finished iteration 0, eps = 227.91, time = 1.0s / 0.0m, ESS = 1000 -&gt; 500, numSims = 1000<p>Finished iteration 1, eps = 6.55, time = 36.0s / 0.6m, ESS = 1295 -&gt; 500, numSims = 8540117<p>Finished iteration 2, eps = 5.25, time = 28.0s / 0.5m, ESS = 1827 -&gt; 500, numSims = 6704856<p>Finished iteration 3, eps = 3.76, time = 2.0s / 0.0m, ESS = 1584 -&gt; 500, numSims = 148295<p>Finished iteration 4, eps = 2.63, time = 3.0s / 0.1m, ESS = 1257 -&gt; 500, numSims = 145498<p>Finished iteration 5, eps = 2.09, time = 6.0s / 0.1m, ESS = 1271 -&gt; 500, numSims = 202565<p>Finished iteration 6, eps = 1.89, time = 3.0s / 0.1m, ESS = 1483 -&gt; 500, numSims = 285492<p>Finished iteration 7, eps = 1.83, time = 5.0s / 0.1m, ESS = 1364 -&gt; 500, numSims = 481011<p>Finished iteration 8, eps = 1.79, time = 9.0s / 0.1m, ESS = 1255 -&gt; 500, numSims = 797275<p>Finished iteration 9, eps = 1.75, time = 16.0s / 0.3m, ESS = 1382 -&gt; 500, numSims = 1617794<p>Finished iteration 10, eps = 1.75, time = 32.0s / 0.5m, ESS = 1322 -&gt; 593, numSims = 3677519 </p></p></p></p></p></p></p></p></p></p></p> <pre>Final population dists &lt;= 1.73, ESS = 593\nCPU times: user 24.7 s, sys: 1.41 s, total: 26.1 s\nWall time: 2min 23s\n</pre>  Starting ABC-SMC with population size of 1000 and sample size of 250 (~&gt; 250) on 40 processes.<p>Finished iteration 0, eps = 227.91, time = 1.0s / 0.0m, ESS = 1000 -&gt; 500, numSims = 1000<p>Finished iteration 1, eps = 6.53, time = 35.0s / 0.6m, ESS = 1295 -&gt; 500, numSims = 8540117<p>Finished iteration 2, eps = 4.99, time = 26.0s / 0.4m, ESS = 1816 -&gt; 500, numSims = 6576644<p>Finished iteration 3, eps = 2.90, time = 2.0s / 0.0m, ESS = 1735 -&gt; 500, numSims = 180033<p>Finished iteration 4, eps = 1.51, time = 2.0s / 0.0m, ESS = 1487 -&gt; 500, numSims = 153563<p>Finished iteration 5, eps = 0.86, time = 1.0s / 0.0m, ESS = 1438 -&gt; 500, numSims = 208907<p>Finished iteration 6, eps = 0.64, time = 3.0s / 0.0m, ESS = 1479 -&gt; 500, numSims = 313184<p>Finished iteration 7, eps = 0.50, time = 4.0s / 0.1m, ESS = 1402 -&gt; 500, numSims = 546874<p>Finished iteration 8, eps = 0.44, time = 5.0s / 0.1m, ESS = 1306 -&gt; 500, numSims = 682801<p>Finished iteration 9, eps = 0.37, time = 6.0s / 0.1m, ESS = 1363 -&gt; 500, numSims = 1246630<p>Finished iteration 10, eps = 0.37, time = 8.0s / 0.1m, ESS = 1148 -&gt; 569, numSims = 1398196 </p></p></p></p></p></p></p></p></p></p></p> <pre>Final population dists &lt;= 0.35, ESS = 569\nCPU times: user 16.8 s, sys: 1.2 s, total: 18 s\nWall time: 1min 33s\nCPU times: user 1min 39s, sys: 7.67 s, total: 1min 47s\nWall time: 7min 26s\n</pre> In\u00a0[9]: Copied! <pre>for ss in sample_sizes:\n\n    fig, axs = plt.subplots(1, len(params), tight_layout=True)\n\n    for l in range(len(params)):\n        pLims = [prior.marginals[l].isf(1), prior.marginals[l].isf(0)]\n        #         axs[l].set_xlim(pLims)\n\n        for k, distance in enumerate(distances):\n            sampleData = dfABC.query(\"distance == @distance\").query(\"ss == @ss\")\n            sample = sampleData[params[l]]\n            weights = sampleData[\"weights\"]\n\n            dataResampled, xs, ys = abc.resample_and_kde(sample, weights, clip=pLims)\n            axs[l].plot(xs, ys, label=distance)\n\n        axs[l].axvline(\u03b8_True[l], **trueStyle)\n        #     axs[l].set_title(\"$\" + params[l] + \"$\")\n        axs[l].set_yticks([])\n\n    draw_prior(prior, axs)\n    sns.despine(left=True)\n    save_cropped(\"../Figures/hist-cyclical-poisson-lnorm-T\" + str(ss) + \".pdf\")\n    plt.show()\n</pre> for ss in sample_sizes:      fig, axs = plt.subplots(1, len(params), tight_layout=True)      for l in range(len(params)):         pLims = [prior.marginals[l].isf(1), prior.marginals[l].isf(0)]         #         axs[l].set_xlim(pLims)          for k, distance in enumerate(distances):             sampleData = dfABC.query(\"distance == @distance\").query(\"ss == @ss\")             sample = sampleData[params[l]]             weights = sampleData[\"weights\"]              dataResampled, xs, ys = abc.resample_and_kde(sample, weights, clip=pLims)             axs[l].plot(xs, ys, label=distance)          axs[l].axvline(\u03b8_True[l], **trueStyle)         #     axs[l].set_title(\"$\" + params[l] + \"$\")         axs[l].set_yticks([])      draw_prior(prior, axs)     sns.despine(left=True)     save_cropped(\"../Figures/hist-cyclical-poisson-lnorm-T\" + str(ss) + \".pdf\")     plt.show()     <pre>&lt;Figure size 1750x700 with 0 Axes&gt;</pre> <pre>&lt;Figure size 1750x700 with 0 Axes&gt;</pre> In\u00a0[10]: Copied! <pre>fig, axs = plt.subplots(1, len(params), tight_layout=True)\ndfABC_new = dfABC.iloc[\n    np.array(dfABC.distance == \"sorted_data\") | np.array(dfABC.distance == \"L1\")\n]\n\nfor l in range(len(params)):\n    pLims = [prior.marginals[l].isf(1), prior.marginals[l].isf(0)]\n    # axs[l].set_xlim(pLims)\n\n    for k, ss in enumerate(sample_sizes):\n        sampleData = dfABC_new.query(\"ss == @ss\")\n\n        sample = sampleData[params[l]]\n\n        weights = sampleData[\"weights\"]\n\n        dataResampled, xs, ys = abc.resample_and_kde(sample, weights, clip=pLims)\n        axs[l].plot(xs, ys, label=str(ss))\n\n    axs[l].axvline(\u03b8_True[l], **trueStyle)\n    #     axs[l].set_title(\"$\" + params[l] + \"$\")\n    axs[l].set_yticks([])\n\n\n# draw_prior(prior, axs)\nsns.despine(left=True)\n# save_cropped(\"../Figures/hist-cyclical-poisson-lnorm-post_combination.pdf\")\n</pre> fig, axs = plt.subplots(1, len(params), tight_layout=True) dfABC_new = dfABC.iloc[     np.array(dfABC.distance == \"sorted_data\") | np.array(dfABC.distance == \"L1\") ]  for l in range(len(params)):     pLims = [prior.marginals[l].isf(1), prior.marginals[l].isf(0)]     # axs[l].set_xlim(pLims)      for k, ss in enumerate(sample_sizes):         sampleData = dfABC_new.query(\"ss == @ss\")          sample = sampleData[params[l]]          weights = sampleData[\"weights\"]          dataResampled, xs, ys = abc.resample_and_kde(sample, weights, clip=pLims)         axs[l].plot(xs, ys, label=str(ss))      axs[l].axvline(\u03b8_True[l], **trueStyle)     #     axs[l].set_title(\"$\" + params[l] + \"$\")     axs[l].set_yticks([])   # draw_prior(prior, axs) sns.despine(left=True) # save_cropped(\"../Figures/hist-cyclical-poisson-lnorm-post_combination.pdf\") In\u00a0[11]: Copied! <pre>for distance in distances:\n    # distance = \"L1\"\n    fig, axs = plt.subplots(1, len(params), tight_layout=True)\n\n    for l in range(len(params)):\n        pLims = [prior.marginals[l].isf(1), prior.marginals[l].isf(0)]\n        # axs[l].set_xlim(pLims)\n\n        for k, ss in enumerate(sample_sizes):\n            sampleData = dfABC.query(\"ss == @ss\").query(\"distance == @distance\")\n            sample = sampleData[params[l]]\n\n            weights = sampleData[\"weights\"]\n\n            dataResampled, xs, ys = abc.resample_and_kde(sample, weights, clip=pLims)\n            axs[l].plot(xs, ys, label=str(ss))\n\n        axs[l].axvline(\u03b8_True[l], **trueStyle)\n        #     axs[l].set_title(\"$\" + params[l] + \"$\")\n        axs[l].set_yticks([])\n\n    #     handles, labels = axs[0].get_legend_handles_labels()\n    #     fig.legend(\n    #         handles, labels, ncol=3, borderaxespad=0.0, loc=\"upper center\", frameon=False\n    #     )\n    draw_prior(prior, axs)\n    sns.despine(left=True)\n    #     plt.show()\n    save_cropped(\"../Figures/hist-cyclical-poisson-lnorm-\" + str(distance) + \".pdf\")\n</pre> for distance in distances:     # distance = \"L1\"     fig, axs = plt.subplots(1, len(params), tight_layout=True)      for l in range(len(params)):         pLims = [prior.marginals[l].isf(1), prior.marginals[l].isf(0)]         # axs[l].set_xlim(pLims)          for k, ss in enumerate(sample_sizes):             sampleData = dfABC.query(\"ss == @ss\").query(\"distance == @distance\")             sample = sampleData[params[l]]              weights = sampleData[\"weights\"]              dataResampled, xs, ys = abc.resample_and_kde(sample, weights, clip=pLims)             axs[l].plot(xs, ys, label=str(ss))          axs[l].axvline(\u03b8_True[l], **trueStyle)         #     axs[l].set_title(\"$\" + params[l] + \"$\")         axs[l].set_yticks([])      #     handles, labels = axs[0].get_legend_handles_labels()     #     fig.legend(     #         handles, labels, ncol=3, borderaxespad=0.0, loc=\"upper center\", frameon=False     #     )     draw_prior(prior, axs)     sns.despine(left=True)     #     plt.show()     save_cropped(\"../Figures/hist-cyclical-poisson-lnorm-\" + str(distance) + \".pdf\") In\u00a0[12]: Copied! <pre>distance = \"curve_matching\"\nss = sample_sizes[1]\nparms_name = [\"a\", \"b\", \"c\", \"\\mu\", \"\\sigma\"]\n\u03b8_map = (\n    dfABC.query(\"ss == @ss\").query(\"distance == @distance\")[parms_name].mean().values\n)\na, b, c, \u03bc, \u03c3 = \u03b8_map\nt = np.arange(ss)\nmus = (\n    a\n    + b\n    + b * (np.cos(2 * np.pi * t * c) - np.cos(2 * np.pi * (t + 1) * c)) / 2 / np.pi / c\n)\nexpXs = np.exp(\u03bc + \u03c3 ** 2 / 2) * mus\n# expXs = \u03bc * mus\nfig, ax = plt.subplots()\nax.plot(t, expXs, label=\"Expected value for X_s\", c=\"k\")\nax.scatter(t, xData[:ss])\nax.set(xlabel=\"time period\", ylabel=\"Aggregated claim amounts\", title=\"\")\n# ax.grid()\nsns.despine()\n</pre> distance = \"curve_matching\" ss = sample_sizes[1] parms_name = [\"a\", \"b\", \"c\", \"\\mu\", \"\\sigma\"] \u03b8_map = (     dfABC.query(\"ss == @ss\").query(\"distance == @distance\")[parms_name].mean().values ) a, b, c, \u03bc, \u03c3 = \u03b8_map t = np.arange(ss) mus = (     a     + b     + b * (np.cos(2 * np.pi * t * c) - np.cos(2 * np.pi * (t + 1) * c)) / 2 / np.pi / c ) expXs = np.exp(\u03bc + \u03c3 ** 2 / 2) * mus # expXs = \u03bc * mus fig, ax = plt.subplots() ax.plot(t, expXs, label=\"Expected value for X_s\", c=\"k\") ax.scatter(t, xData[:ss]) ax.set(xlabel=\"time period\", ylabel=\"Aggregated claim amounts\", title=\"\") # ax.grid() sns.despine()"},{"location":"seasonal-claim-arrivals/#seasonal-claim-arrivals-wip","title":"Seasonal Claim Arrivals (WIP)\u00b6","text":""},{"location":"seasonal-claim-arrivals/#abc-posterior-when-exactly-matching-the-time-indices","title":"ABC posterior when exactly matching the time indices\u00b6","text":""},{"location":"seasonal-claim-arrivals/#plots-to-compare-the-posterior-distribution-based-on-different-distance-for-a-given-sample-size","title":"Plots to compare the posterior distribution based on different distance for a given sample size\u00b6","text":""},{"location":"seasonal-claim-arrivals/#combining-the-l1-and-l1-on-sorted-data-posteriors","title":"Combining the L1 and L1 on sorted data posteriors\u00b6","text":""},{"location":"seasonal-claim-arrivals/#plots-to-compare-the-posterior-distribution-based-on-a-specifi-distance-with-different-sample-size","title":"Plots to compare the posterior distribution based on a specifi distance with different sample size\u00b6","text":""},{"location":"seasonal-claim-arrivals/#recovered-signal-based-on-map-estimators-of-the-model-parameters-for-a-given-sample-size-and-a-given-distance","title":"Recovered signal based on MAP estimators of the model parameters for a given sample size and a given distance\u00b6","text":""},{"location":"simulation-model-example/","title":"Simulation model example","text":"In\u00a0[\u00a0]: Copied! <pre>import approxbayescomp as abc\nimport numpy as np\n</pre> import approxbayescomp as abc import numpy as np In\u00a0[\u00a0]: Copied! <pre># Load data to fit (modify this line to load real observations!)\nobsData = [1.0, 2.0, 3.0]\n</pre> # Load data to fit (modify this line to load real observations!) obsData = [1.0, 2.0, 3.0] In\u00a0[\u00a0]: Copied! <pre># Specify our prior beliefs over (lambda, mu, sigma).\nprior = abc.IndependentUniformPrior([(0, 100), (-5, 5), (0, 3)])\n</pre> # Specify our prior beliefs over (lambda, mu, sigma). prior = abc.IndependentUniformPrior([(0, 100), (-5, 5), (0, 3)]) In\u00a0[\u00a0]: Copied! <pre># Write a function to simulate from the data-generating process.\ndef simulate_aggregate_claims(rg, theta, T):\n    \"\"\"\n    Generate T observations from the model specified by theta\n    using the random number generator rg.\n    \"\"\"\n    lam, mu, sigma = theta\n    freqs = rg.poisson(lam, size=T)\n    aggClaims = np.empty(T, np.float64)\n    for t in range(T):\n        aggClaims[t] = np.sum(rg.lognormal(mu, sigma, size=freqs[t]))\n    return aggClaims\n</pre> # Write a function to simulate from the data-generating process. def simulate_aggregate_claims(rg, theta, T):     \"\"\"     Generate T observations from the model specified by theta     using the random number generator rg.     \"\"\"     lam, mu, sigma = theta     freqs = rg.poisson(lam, size=T)     aggClaims = np.empty(T, np.float64)     for t in range(T):         aggClaims[t] = np.sum(rg.lognormal(mu, sigma, size=freqs[t]))     return aggClaims In\u00a0[\u00a0]: Copied! <pre># Fit the model to the data using ABC\nmodel = abc.SimulationModel(\n    lambda rg, theta: simulate_aggregate_claims(rg, theta, len(obsData)), prior\n)\nnumIters = 6  # The number of SMC iterations to perform\npopSize = 250  # The population size of the SMC method\n</pre> # Fit the model to the data using ABC model = abc.SimulationModel(     lambda rg, theta: simulate_aggregate_claims(rg, theta, len(obsData)), prior ) numIters = 6  # The number of SMC iterations to perform popSize = 250  # The population size of the SMC method In\u00a0[\u00a0]: Copied! <pre>fit = abc.smc(numIters, popSize, obsData, model)\n</pre> fit = abc.smc(numIters, popSize, obsData, model)"},{"location":"what-is-abc/","title":"What is ABC?","text":"<p>Let's consider a toy example. Imagine we are trying to determine the probability \\(p = \\mathbb{P}(\\text{Heads})\\) of getting a heads when flipping a biased coin. We observe just three coin flips, and get <code>Heads, Tails, Heads</code>. One way to fit this data is to just start flipping (virtual) coins. Specifically, we randomly guess a value \\(p' \\in (0, 1)\\), flip a biased coin with this probability of heads three times, then remember this value of \\(p'\\) if this coin also got <code>Heads, Tails, Heads</code>.</p> Illustration of exact posterior sampling with 3 observed coin tosses from a biased coin. <p></p> Once Loop Reflect <p>Tip</p> <p>Press the play button (triangular shaped) on these animations to start them.</p> <p>In this simulation we found 15 values of \\(p'\\) which happened to generate the same <code>Heads, Tails, Heads</code>. These 15 values are actually samples from the posterior distribution for \\(p\\), given that we have a uniform prior belief.</p> <p>Of course, this method of generating fake data which is identical to the real observed data is not an efficient way to sample from the posterior distribution. If we increase the sample size of this toy example to eight observed coin flips, the following animation shows that event after 100 attempts we don't find a single \\(p'\\) which generates the exact same eight observations.</p> Illustration of exact posterior sampling with 8 observed coin tosses from a biased coin. <p></p> Once Loop Reflect <p>This is where the approximate part of Approximate Bayesian Computation (ABC) comes in. Let's just accept a \\(p'\\) value if the fake data it generates is pretty close to the observed data. In the following, our observed data had five heads, so we accept any fake data that has 4, 5, or 6 heads.</p> Illustration of ABC sampling with 8 observed coin tosses from a biased coin. <p></p> Once Loop Reflect <p>This generated a much larger sample (29 points) from an approximate posterior distribution. We can compare the true posterior to the various approximate posterior distributions where our \"accept when pretty close\" rule is more or less stringent.</p> Comparing the true posterior distribution to various ABC approximate posteriors. <p></p> Once Loop Reflect"}]}