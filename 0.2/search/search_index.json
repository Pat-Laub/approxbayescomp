{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Approximate Bayesian Computation Python Package","text":""},{"location":"#package-description","title":"Package Description","text":"<p>Approximate Bayesian Computation (ABC) is a statistical method to fit a Bayesian model to data when the likelihood function is hard to compute. The <code>approxbayescomp</code> package implements an efficient form of ABC \u2014 the sequential Monte Carlo (SMC) algorithm. While it can handle any general statistical problem, we built in some models so that fitting insurance loss distributions is particularly easy. </p>"},{"location":"#installation","title":"Installation","text":"<p>To install simply run</p> <p><code>pip install -U approxbayescomp</code></p> <p>The source code for the package is available on Github.</p>"},{"location":"#example","title":"Example","text":""},{"location":"#using-a-built-in-data-generating-process-simulation-method","title":"Using a built-in data generating process simulation method","text":"<p>Consider a basic insurance example where each month our insurance company receives a random number of claims, each of which is of a random size. Specifically, say that in month \\(i\\) we have \\(N_i \\sim \\mathsf{Poisson}(\\lambda)\\) i.i.d. number of claims, and each claim is \\(U_{i,j} \\sim \\mathsf{Lognormal}(\\mu, \\sigma^2)\\) sized and i.i.d. At each month we can observe the aggregate claims, that is, \\(X_i = \\sum_{j=1}^{N_i} U_{i,j}\\) for \\(i=1,\\dots,T\\), that is, we observe \\(T\\) months of data. Lastly, we have the prior beliefs that \\(\\lambda \\sim \\mathsf{Unif}(0, 100),\\) \\(\\mu \\sim \\mathsf{Unif}(-5, 5),\\) and \\(\\sigma \\sim \\mathsf{Unif}(0, 3).\\)</p> <p>The <code>approxbayescomp</code> code to fit this data would be:</p> <pre><code>import approxbayescomp as abc\n\n# Load data to fit (modify this line to load real observations!)\nobsData = [1.0, 2.0, 3.0]\n\n# Specify our prior beliefs over (lambda, mu, sigma).\nprior = abc.IndependentUniformPrior([(0, 100), (-5, 5), (0, 3)])\n\n# Fit the model to the data using ABC\nmodel = abc.Model(\"poisson\", \"lognormal\", abc.Psi(\"sum\"))\nnumIters = 6  # The number of SMC iterations to perform\npopSize = 250  # The population size of the SMC method\n\nfit = abc.smc(numIters, popSize, obsData, model, prior)\n</code></pre> <p>Then <code>fit</code> will contain a collection of weighted samples from the approximate posterior distribution of \\((\\lambda, \\mu, \\sigma)\\). The posterior mean for these parameters would be easily calculated:</p> <pre><code>import numpy as np\nprint(\"Posterior mean of lambda: \", np.sum(fit.samples[:, 0] * fit.weights))\nprint(\"Posterior mean of mu: \", np.sum(fit.samples[:, 1] * fit.weights))\nprint(\"Posterior mean of sigma: \", np.sum(fit.samples[:, 2] * fit.weights))\n</code></pre>"},{"location":"#using-a-user-suppled-simulation-method","title":"Using a user-suppled simulation method","text":"<p>We have built many standard insurance loss models into the package, so in the previous example</p> <pre><code>model = abc.Model(\"poisson\", \"lognormal\", abc.Psi(\"sum\"))\n</code></pre> <p>is all that is required to specify this data-generating process. However, for non-insurance processes, we have to supply a function to simulate from the data-generating process. The equivalent version for this example would be:</p> <pre><code>import numpy.random as rnd\n\ndef simulate_aggregate_claims(theta, T):\n    \"\"\"\n    Generate T observations from the model specified by theta.\n    \"\"\"\n    lam, mu, sigma = theta\n    freqs = rnd.poisson(lam, size=T)\n    aggClaims = np.empty(T, np.float64)\n    for t in range(T):\n        aggClaims[t] = np.sum(rnd.lognormal(mu, sigma, size=freqs[t]))\n    return aggClaims\n\nsimulator = lambda theta: simulate_aggregate_claims(theta, len(obsData))\nfit = abc.smc(numIters, popSize, obsData, simulator, prior)\n</code></pre> <p>Modifying just these lines will generate the identical output as the example above.</p>"},{"location":"#other-examples-and-resources","title":"Other Examples and Resources","text":"<p>See the What is ABC page for an illustrative example of the core ABC concept. For examples of this package in use, start with the Geometric-Exponential example page and the following ones.</p> <p>This package is the result of our paper \"Approximate Bayesian Computation to fit and compare insurance loss models\". For a detailed description of the aims and methodology of ABC check out this paper. It was written with ABC newcomers in mind.</p> <p>If you prefer audio/video, see Patrick's 7 min lightning talk at the Insurance Data Science conference:</p> <p> </p> ABC Talk at Insurance Data Science conference"},{"location":"#details","title":"Details","text":"<p>The main design goal for this package was computational speed. ABC is notoriously computationally demanding, so we spent a long time optimising the code as much as possible. The key functions are JIT-compiled to C with <code>numba</code> (we experimented with JIT-compiling the entire SMC algorithm, but <code>numba</code>'s random variable generation is surprisingly slower than <code>numpy</code>'s implementation). Everything that can be <code>numpy</code>-vectorised has been. And we scale to use as many CPU cores available on a machine using <code>joblib</code>. We also aimed to have total reproducibility, so for any given seed value the resulting ABC posterior samples will always be identical.</p> <p>Our main dependencies are joblib, numba, numpy, and scipy. Also, the package sometimes calls functions from matplotlib, tqdm, and hilbertcurve.</p> <p>Note</p> <p>Patrick has a rough start at a C++ version of this package at the cppabc repository. It only handles the specific Geometric-Exponential random sums case, though if you are interested in collaborating to expand this, let him know!</p>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Patrick Laub (author, maintainer),</li> <li>Pierre-Olivier Goffard (author).</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>Pierre-Olivier Goffard, Patrick J. Laub (2021), Approximate Bayesian Computations to fit and compare insurance loss models, Insurance: Mathematics and Economics, 100, pp. 350-371</p> <pre><code>@article{approxbayescomp,\n  title={Approximate Bayesian Computations to fit and compare insurance loss models},\n  author={Goffard, Pierre-Olivier and Laub, Patrick J},\n  journal={Insurance: Mathematics and Economics},\n  volume={100},\n  pages={350--371},\n  year={2021}\n}\n</code></pre>"},{"location":"bivariate-observations/","title":"Bivariate Observations","text":"<p>Insurers are typically exposed to more than one type of risk, and it can be beneficial for them to consider the joint risk profile for related products.</p> <p>This example considers a joint model for the frequency of claims reported for two nonlife insurance portfolios. The claim counts are Poisson distributed  with respective intensity $\\Lambda w_1$ and $\\Lambda w_2$ where $\\Lambda$ is some non-negative random variable.</p> <p>The frequency data $(n_1,m_1),\\ldots, (n_t,m_t)$ is i.i.d. according to a bivariate counting distribution with joint p.m.f. given by</p> <p>$$ p_{N,M}(n,m) = \\int\\frac{\\mathrm{e}^{-\\lambda w_1}(\\lambda w_1)^n}{n!}\\frac{\\mathrm{e}^{-\\lambda w_2}(\\lambda w_2)^m}{m!} \\mathrm{d} \\mathbb{P}_\\Lambda(\\lambda), \\quad n,m = 0, 1, \\dots. $$</p> <p>The severities associated to a given time period $s=1,\\ldots, t$ form two mutually independent, i.i.d. sequences of exponentially distributed random variables,</p> <p>$$ U_{s,1},\\ldots, U_{s,n_s} \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathsf{Exp}(m_1 = 10) \\quad \\text{and} \\quad V_{s,1},\\ldots, V_{s,m_s} \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathsf{Exp}(m_2 = 70). $$</p> <p>Each observation then is a pair of numbers corresponding to the aggregate claims for each risk. That is,</p> <p>$$ X_s = \\Bigl( \\sum_{i=1}^{N_s} U_{s,i} , \\sum_{i=1}^{M_s} V_{s,i} \\Bigr) $$</p> <p>We start by importing some necessary packages.</p> In\u00a0[1]: Copied! <pre>%config InlineBackend.figure_format = 'retina'\nimport approxbayescomp as abc\nimport numpy as np\nimport numpy.random as rnd\n</pre> %config InlineBackend.figure_format = 'retina' import approxbayescomp as abc import numpy as np import numpy.random as rnd <pre>/Users/plaub/approxbayescomp/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>We will fit simulated data, so that we know the true value of the parameters for the data-generating process. We let $\\Lambda$ be a lognormal random variable $\\mathsf{Lognormal}(\\sigma=0.2)$ (the mean log parameter is set to $0$) as it is consistent with the use of a generalized linear model equipped with a log link function to estimate the Poisson intensity given a set of covariates. The marginal components of the claim frequency distribution are set to $w_1 = 15$ and $w_2 = 5$. Finally, we observe $T = 250$ i.i.d. bivariate observations.</p> In\u00a0[2]: Copied! <pre># Create a pseudorandom number generator\nrg = rnd.default_rng(1234)\n\n# Parameters of the true model\nfreq = \"bivariate poisson\"\nsev = \"exponential\"\n\u03c3 = 0.2\nw1 = 15\nw2 = 5\nm1 = 10\nm2 = 70\ntrueTheta = (\u03c3, w1, w2, m1, m2)\n\n# Setting the time horizon\nT = 250\n\n# Simulating the claim data\nclaimsData = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)\n\n# Simulating the observed data\npsi = abc.Psi(\"sum\")\n\nxData1 = abc.compute_psi(claimsData[0][0], claimsData[0][1], psi)\nxData2 = abc.compute_psi(claimsData[1][0], claimsData[1][1], psi)\n\nxData = np.vstack([xData1, xData2]).T\n</pre> # Create a pseudorandom number generator rg = rnd.default_rng(1234)  # Parameters of the true model freq = \"bivariate poisson\" sev = \"exponential\" \u03c3 = 0.2 w1 = 15 w2 = 5 m1 = 10 m2 = 70 trueTheta = (\u03c3, w1, w2, m1, m2)  # Setting the time horizon T = 250  # Simulating the claim data claimsData = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)  # Simulating the observed data psi = abc.Psi(\"sum\")  xData1 = abc.compute_psi(claimsData[0][0], claimsData[0][1], psi) xData2 = abc.compute_psi(claimsData[1][0], claimsData[1][1], psi)  xData = np.vstack([xData1, xData2]).T <p>We can see if any of this observed data contains pesky zeros:</p> In\u00a0[3]: Copied! <pre>np.sum(xData == 0)\n</pre> np.sum(xData == 0) Out[3]: <pre>4</pre> <p>Also, as we have bivariate data, we will use a special distance function which expects to receive a matrix with two columns:</p> In\u00a0[4]: Copied! <pre>xData.shape\n</pre> xData.shape Out[4]: <pre>(250, 2)</pre> <p>With this data, we create objects to represent the data-generating process (the model) and the prior distribution. The priors are set as</p> <p>$$\\sigma \\sim \\mathsf{Unif}(0, 2), \\quad w_1 \\sim \\mathsf{Unif}(0, 50), \\quad w_2 \\sim \\mathsf{Unif}(0, 50)$$ $$m_1 \\sim \\mathsf{Unif}(0, 100), \\quad \\text{ and } \\quad m_2 \\sim \\mathsf{Unif}(0, 100).$$</p> In\u00a0[5]: Copied! <pre>model = abc.Model(freq, sev, psi)\nparams = (\"$\\\\delta$\", \"$w_1$\", \"$w_2$\", \"$m_1$\", \"$m_2$\")\nprior = abc.IndependentUniformPrior([(0, 2), (0, 50), (0, 50), (0, 100), (0, 100)], params)\n</pre> model = abc.Model(freq, sev, psi) params = (\"$\\\\delta$\", \"$w_1$\", \"$w_2$\", \"$m_1$\", \"$m_2$\") prior = abc.IndependentUniformPrior([(0, 2), (0, 50), (0, 50), (0, 100), (0, 100)], params) <p>Finally, we call the main <code>smc</code> method to fit the observed <code>xData</code>.</p> In\u00a0[7]: Copied! <pre>numIters = 10\npopSize = 250\n%time fit = abc.smc(numIters, popSize, xData, model, prior, distance=abc.wasserstein2D, numProcs=8, seed=1, verbose=True)\n</pre> numIters = 10 popSize = 250 %time fit = abc.smc(numIters, popSize, xData, model, prior, distance=abc.wasserstein2D, numProcs=8, seed=1, verbose=True) <pre>Starting ABC-SMC with population size of 250 and sample size of 250 (~&gt; 500) on 8 processes.\nFinished sampling from prior, eps = 2308.21, time = 4.0s / 0.1m, popSize = 250 -&gt; 125, ESS = 250 -&gt; 125, # sims = 250, total # sims = 250\nFinished SMC iteration 1, eps = 680.92, time = 2.0s / 0.0m, popSize = 403 -&gt; 135, ESS = 369 -&gt; 125, # sims = 2248, total # sims = 2498\nFinished SMC iteration 2, eps = 311.83, time = 1.0s / 0.0m, popSize = 402 -&gt; 143, ESS = 340 -&gt; 125, # sims = 4104, total # sims = 6602\nFinished SMC iteration 3, eps = 198.09, time = 2.0s / 0.0m, popSize = 402 -&gt; 156, ESS = 312 -&gt; 125, # sims = 6920, total # sims = 13522\nFinished SMC iteration 4, eps = 137.68, time = 3.0s / 0.1m, popSize = 443 -&gt; 171, ESS = 339 -&gt; 125, # sims = 12808, total # sims = 26330\nFinished SMC iteration 5, eps = 102.23, time = 7.0s / 0.1m, popSize = 422 -&gt; 166, ESS = 315 -&gt; 125, # sims = 23816, total # sims = 50146\nFinished SMC iteration 6, eps = 79.80, time = 11.0s / 0.2m, popSize = 423 -&gt; 159, ESS = 303 -&gt; 125, # sims = 30872, total # sims = 81018\nFinished SMC iteration 7, eps = 64.67, time = 14.0s / 0.2m, popSize = 412 -&gt; 188, ESS = 221 -&gt; 125, # sims = 35272, total # sims = 116290\nFinished SMC iteration 8, eps = 51.62, time = 15.0s / 0.2m, popSize = 456 -&gt; 160, ESS = 294 -&gt; 125, # sims = 35272, total # sims = 151562\nFinished SMC iteration 9, eps = 40.03, time = 15.0s / 0.3m, popSize = 629 -&gt; 158, ESS = 456 -&gt; 125, # sims = 35272, total # sims = 186834\nFinished SMC iteration 10, eps = 35.54, time = 14.0s / 0.2m, popSize = 668 -&gt; 250, ESS = 518 -&gt; 211, # sims = 35272, total # sims = 222106\nCPU times: user 403 ms, sys: 126 ms, total: 529 ms\nWall time: 1min 28s\n</pre> <p>These particles all generated fake data within the following distance to the observed data:</p> In\u00a0[8]: Copied! <pre>np.max(fit.dists)\n</pre> np.max(fit.dists) Out[8]: <pre>35.54484952368373</pre> <p>Plotting the fitted ABC posterior:</p> In\u00a0[9]: Copied! <pre>abc.plot_posteriors(\n    fit,\n    prior,\n    refLines=trueTheta,\n)\n</pre> abc.plot_posteriors(     fit,     prior,     refLines=trueTheta, ) In\u00a0[10]: Copied! <pre>%time fitMatchZeros = abc.smc(numIters, popSize, xData, model, prior, matchZeros=True, distance=abc.wasserstein2D, numProcs=8, seed=1, verbose=True)\n</pre> %time fitMatchZeros = abc.smc(numIters, popSize, xData, model, prior, matchZeros=True, distance=abc.wasserstein2D, numProcs=8, seed=1, verbose=True) <pre>Starting ABC-SMC with population size of 250 and sample size of 250 (~&gt; 500) on 8 processes.\nFinished sampling from prior, eps = 2308.21, time = 1.0s / 0.0m, popSize = 250 -&gt; 125, ESS = 250 -&gt; 125, # sims = 250, total # sims = 250\nFinished SMC iteration 1, eps = 803.98, time = 2.0s / 0.0m, popSize = 377 -&gt; 136, ESS = 340 -&gt; 125, # sims = 242088, total # sims = 242338\nFinished SMC iteration 2, eps = 304.74, time = 2.0s / 0.0m, popSize = 410 -&gt; 136, ESS = 373 -&gt; 125, # sims = 242088, total # sims = 484426\nFinished SMC iteration 3, eps = 155.41, time = 3.0s / 0.0m, popSize = 455 -&gt; 142, ESS = 408 -&gt; 125, # sims = 242088, total # sims = 726514\nFinished SMC iteration 4, eps = 86.32, time = 4.0s / 0.1m, popSize = 559 -&gt; 136, ESS = 505 -&gt; 125, # sims = 242088, total # sims = 968602\nFinished SMC iteration 5, eps = 46.57, time = 8.0s / 0.1m, popSize = 1226 -&gt; 134, ESS = 994 -&gt; 125, # sims = 242088, total # sims = 1210690\nFinished SMC iteration 6, eps = 35.65, time = 10.0s / 0.2m, popSize = 1027 -&gt; 149, ESS = 612 -&gt; 125, # sims = 242088, total # sims = 1452778\nFinished SMC iteration 7, eps = 33.03, time = 10.0s / 0.2m, popSize = 406 -&gt; 157, ESS = 307 -&gt; 125, # sims = 242088, total # sims = 1694866\nFinished SMC iteration 8, eps = 31.46, time = 20.0s / 0.3m, popSize = 410 -&gt; 210, ESS = 242 -&gt; 125, # sims = 458216, total # sims = 2153082\nFinished SMC iteration 9, eps = 29.77, time = 37.0s / 0.6m, popSize = 469 -&gt; 157, ESS = 305 -&gt; 125, # sims = 901624, total # sims = 3054706\nFinished SMC iteration 10, eps = 29.12, time = 64.0s / 1.1m, popSize = 408 -&gt; 250, ESS = 295 -&gt; 175, # sims = 1392832, total # sims = 4447538\nCPU times: user 496 ms, sys: 127 ms, total: 623 ms\nWall time: 2min 40s\n</pre> <p>These particles all generated fake data within the following distance to the observed data:</p> In\u00a0[11]: Copied! <pre>np.max(fitMatchZeros.dists)\n</pre> np.max(fitMatchZeros.dists) Out[11]: <pre>29.1217535245939</pre> <p>Plotting the fitted ABC posterior:</p> In\u00a0[12]: Copied! <pre>abc.plot_posteriors(\n    fitMatchZeros,\n    prior,\n    refLines=trueTheta,\n)\n</pre> abc.plot_posteriors(     fitMatchZeros,     prior,     refLines=trueTheta, ) <p>In this case, the <code>matchZeros</code> is much more accurate than the previous fit.</p>"},{"location":"bivariate-observations/#bivariate-observations","title":"Bivariate Observations\u00b6","text":""},{"location":"bivariate-observations/#generate-synthetic-observations","title":"Generate synthetic observations\u00b6","text":""},{"location":"bivariate-observations/#use-abc-to-fit-the-data","title":"Use ABC to fit the data\u00b6","text":""},{"location":"bivariate-observations/#ignoring-the-zeros","title":"Ignoring the zeros\u00b6","text":""},{"location":"bivariate-observations/#matching-the-zeros","title":"Matching the zeros\u00b6","text":"<p>Trying a second time with the <code>matchZeros</code> flag on:</p>"},{"location":"frequency-dependent-claim-sizes/","title":"Frequency-Dependent Claim Sizes","text":"<p>In this example, we look at a compound sum where the claim frequency and the claim sizes are dependent.</p> <p>Specifically, let's say that the claim frequency variables are Negative Binomial distributed</p> <p>$$ N_i \\overset{\\mathrm{i.i.d.}}{\\sim} \\textsf{Poisson}(\\lambda), \\quad i = 1, \\dots, T $$</p> <p>and the individual claim sizes are freqency dependent exponential, which means that</p> <p>$$ U_{i,1} \\ldots, U_{i, N_i} \\,|\\, N_i \\overset{\\mathrm{i.i.d.}}{\\sim} \\textsf{Exp}(\\beta\\times \\mathrm{e}^{\\delta N_i}), \\quad i = 1, \\dots, T. $$</p> <p>The available data is the total claim sizes</p> <p>$$ X_i = \\sum_{j = 1}^{N_i} U_{i,j}, \\quad i = 1, \\ldots, T. $$</p> <p>We start by importing some necessary packages.</p> In\u00a0[1]: Copied! <pre>%config InlineBackend.figure_format = 'retina'\nimport approxbayescomp as abc\nimport numpy as np\nimport numpy.random as rnd\n</pre> %config InlineBackend.figure_format = 'retina' import approxbayescomp as abc import numpy as np import numpy.random as rnd <pre>/Users/plaub/approxbayescomp/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>We will fit simulated data, so that we know the true value of the parameters for the data-generating process. Here, we start with $\\lambda = 4$, $\\beta = 2$, and $\\delta = 0.2$, and say that we observe $T = 100$ i.i.d. observations of the compound sum.</p> In\u00a0[2]: Copied! <pre># Create a pseudorandom number generator\nrg = rnd.default_rng(1234)\n\n# Parameters of the true model\nfreq = \"poisson\"\nsev = \"frequency dependent exponential\"\n\u03bb = 4\n\u03b2 = 2\n\u03b4 = 0.2\ntrueTheta = (\u03bb, \u03b2, \u03b4)\n\n# Setting the time horizon\nT = 100\n\n# Simulating the claim data\nfreqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)\n\n# Simulating the observed data\npsi = abc.Psi(\"sum\")\nxData = abc.compute_psi(freqs, sevs, psi)\n</pre> # Create a pseudorandom number generator rg = rnd.default_rng(1234)  # Parameters of the true model freq = \"poisson\" sev = \"frequency dependent exponential\" \u03bb = 4 \u03b2 = 2 \u03b4 = 0.2 trueTheta = (\u03bb, \u03b2, \u03b4)  # Setting the time horizon T = 100  # Simulating the claim data freqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)  # Simulating the observed data psi = abc.Psi(\"sum\") xData = abc.compute_psi(freqs, sevs, psi) <p>We can see if any of this observed data contains pesky zeros:</p> In\u00a0[3]: Copied! <pre>np.sum(xData == 0)\n</pre> np.sum(xData == 0) Out[3]: <pre>3</pre> <p>With this data, we create objects to represent the data-generating process (the model) and the prior distribution. The priors are set as $\\lambda \\sim \\mathsf{Unif}(0, 10)$, $\\beta \\sim \\mathsf{Unif}(0, 20)$, and $\\delta \\sim \\mathsf{Unif}(-1, 1).$</p> In\u00a0[4]: Copied! <pre>model = abc.Model(freq, sev, psi)\nparams = (\"$\\\\lambda$\", \"$\\\\beta$\", \"$\\\\delta$\")\nprior = abc.IndependentUniformPrior([(0, 10), (0, 20), (-1, 1)], params)\n</pre> model = abc.Model(freq, sev, psi) params = (\"$\\\\lambda$\", \"$\\\\beta$\", \"$\\\\delta$\") prior = abc.IndependentUniformPrior([(0, 10), (0, 20), (-1, 1)], params) <p>After, we call the main <code>smc</code> method which is provided by <code>approxbayescomp</code> to fit the observed <code>xData</code>.</p> In\u00a0[5]: Copied! <pre>numIters = 8\npopSize = 100\n%time fit = abc.smc(numIters, popSize, xData, model, prior, seed=1)\n</pre> numIters = 8 popSize = 100 %time fit = abc.smc(numIters, popSize, xData, model, prior, seed=1) <pre>CPU times: user 2.95 s, sys: 20.9 ms, total: 2.97 s\nWall time: 2.99 s\n</pre> <p>These particles all generated fake data within the following distance to the observed data:</p> In\u00a0[6]: Copied! <pre>np.max(fit.dists)\n</pre> np.max(fit.dists) Out[6]: <pre>2.9358266608426806</pre> <p>Plotting the fitted ABC posterior:</p> In\u00a0[7]: Copied! <pre>abc.plot_posteriors(\n    fit, prior, refLines=trueTheta\n)\n</pre> abc.plot_posteriors(     fit, prior, refLines=trueTheta ) <p>This quick fit managed to capture $\\delta$ quite well, $\\beta$ with moderate success, and $\\lambda$ with slightly less success.</p> In\u00a0[8]: Copied! <pre>%time fitMatchZeros = abc.smc(numIters, popSize, xData, model, prior, matchZeros=True, seed=1)\n</pre> %time fitMatchZeros = abc.smc(numIters, popSize, xData, model, prior, matchZeros=True, seed=1) <pre>CPU times: user 9.44 s, sys: 9.65 ms, total: 9.45 s\nWall time: 9.52 s\n</pre> <p>These particles all generated fake data within the following distance to the observed data:</p> In\u00a0[9]: Copied! <pre>np.max(fitMatchZeros.dists)\n</pre> np.max(fitMatchZeros.dists) Out[9]: <pre>2.3066212101015164</pre> <p>Plotting the fitted ABC posterior:</p> In\u00a0[10]: Copied! <pre>abc.plot_posteriors(\n    fitMatchZeros,\n    prior,\n    refLines=trueTheta,\n)\n</pre> abc.plot_posteriors(     fitMatchZeros,     prior,     refLines=trueTheta, )"},{"location":"frequency-dependent-claim-sizes/#frequency-dependent-claim-sizes","title":"Frequency-Dependent Claim Sizes\u00b6","text":""},{"location":"frequency-dependent-claim-sizes/#generating-some-synthetic-data-to-fit","title":"Generating some synthetic data to fit\u00b6","text":""},{"location":"frequency-dependent-claim-sizes/#use-abc-to-fit-the-data","title":"Use ABC to fit the data\u00b6","text":""},{"location":"frequency-dependent-claim-sizes/#ignoring-the-zeros","title":"Ignoring the zeros\u00b6","text":""},{"location":"frequency-dependent-claim-sizes/#matching-the-zeros","title":"Matching the zeros\u00b6","text":"<p>The observed data does have zeros, so enabling the <code>matchZeros</code> flag should increase the accuracy (and computation time) of the fits.</p>"},{"location":"geometric-exponential/","title":"Geometric-Exponential Compound Sums","text":"<p>Consider a basic insurance example where each month our insurance company receives a random number of claims, each of which is of a random size. Specifically, say that in month $i$ we have $N_i \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathsf{Geometric}(p)$ number of claims, and each claim is $U_{i,j} \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathsf{Exponential}(\\lambda = 1/\\delta)$ sized.</p> <p>At each month we can observe the aggregate claims, that is, $ X_i = \\sum_{j=1}^{N_i} U_{i,j} $ for $i=1,\\dots,T$, that is, we observe $T$ months of data.</p> <p>In this scenario, the likelihood of the $X_i$'s is tractable, so it is a useful test case to compare ABC against more traditional fitting methods, like Markov Chain Monte Carlo (MCMC) and Maximum Likelihood Estimation (MLE).</p> <p>We start by importing some necessary packages.</p> In\u00a0[1]: Copied! <pre>%config InlineBackend.figure_format = 'retina'\nimport approxbayescomp as abc\nimport numpy as np\nimport numpy.random as rnd\n</pre> %config InlineBackend.figure_format = 'retina' import approxbayescomp as abc import numpy as np import numpy.random as rnd <pre>/Users/plaub/approxbayescomp/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>We will fit simulated data, so that we know the true value of the parameters for the data-generating process. Here, we start with $p = 0.8$, and $\\delta = 5$, and say that we observe $T = 100$ i.i.d. observations of the compound sum.</p> In\u00a0[2]: Copied! <pre># Create a pseudorandom number generator\nrg = rnd.default_rng(1234)\n\n# Parameters of the true model\nfreq = \"geometric\"\nsev = \"exponential\"\ntrueTheta = (0.8, 5)\n\n# Setting the time horizon\nT = 100\n</pre> # Create a pseudorandom number generator rg = rnd.default_rng(1234)  # Parameters of the true model freq = \"geometric\" sev = \"exponential\" trueTheta = (0.8, 5)  # Setting the time horizon T = 100 <p>Inside <code>approxbayescomp</code>, generating this compound sum is done in two steps. Firstly, we generate the entire claims process:</p> In\u00a0[3]: Copied! <pre># Simulating the claim data\nfreqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)\n</pre> # Simulating the claim data freqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, trueTheta) <p>So <code>freqs</code> contains the $\\{ N_i \\}_{i=1,\\dots,T}$, and <code>sevs</code> contains the $\\{ U_{i,j} \\}_{i=1,\\dots,T, j=1,\\dots,N_i }$. For example:</p> In\u00a0[4]: Copied! <pre>freqs\n</pre> freqs Out[4]: <pre>array([ 6,  3,  8,  3,  4,  1,  0,  1,  6,  4,  7, 23, 11,  1,  2,  3, 14,\n        0,  0, 12,  0, 16,  2,  4,  0,  3,  5,  0,  0,  3,  5,  4,  6,  4,\n        3,  7,  2,  9,  0, 10,  1,  2,  9,  6, 13,  6,  2,  6,  1,  0,  2,\n        3,  4,  2,  9,  2,  2,  2,  4, 13,  3,  9,  3,  5,  0,  3,  2,  1,\n        3,  1, 14,  0,  3,  5,  0,  0,  4,  1,  5,  0,  0,  0,  6,  1,  1,\n        0,  9,  2,  1,  1,  7,  0,  0,  0,  1, 10,  2, 20,  4,  0])</pre> In\u00a0[5]: Copied! <pre>sevs[0:6]\n</pre> sevs[0:6] Out[5]: <pre>array([ 0.53075068,  8.74170314,  1.93800992,  1.23094078,  3.07323683,\n       12.37846317])</pre> <p>Note, the <code>sevs</code> vector contains all the claims sizes (a.k.a. severities) for all time periods in one long vector. That is, the length of <code>sevs</code> will be <code>np.sum(freqs)</code> long. This is to save computational time in the ABC algorithm, as the <code>simulate_claim_data</code> function is called many times.</p> <p>Then the $\\Psi$ operation is applied to produce a single value for each time period. Here, this means that we observe the sum of all the claims in each period, however <code>psi</code> can be set to handle censored values like stop-loss observations.</p> In\u00a0[6]: Copied! <pre># Simulating the observed data\npsi = abc.Psi(\"sum\")\nxData = abc.compute_psi(freqs, sevs, psi)\n</pre> # Simulating the observed data psi = abc.Psi(\"sum\") xData = abc.compute_psi(freqs, sevs, psi) In\u00a0[7]: Copied! <pre>xData\n</pre> xData Out[7]: <pre>array([ 27.89310452,  17.68445011,  43.97815086,  10.40718159,\n         8.81323352,   2.91015669,   0.        ,   0.73331647,\n        54.66211006,  15.40214731,  97.62341685,  68.75165795,\n        41.72228953,  16.97354716,  15.02697784,   9.2411639 ,\n        71.63352158,   0.        ,   0.        ,  47.31622624,\n         0.        ,  43.97581615,   0.9500043 ,  28.01171543,\n         0.        ,   6.39736742,  34.87762318,   0.        ,\n         0.        ,  13.20533558,  19.67856556,  20.78462773,\n        31.03229256,  16.08706747,   7.63900497,  40.2456368 ,\n        17.32230075,  38.54744184,   0.        ,  51.08710316,\n         2.37910798,   4.61828744,  65.38123231,  35.85594666,\n        98.43324628,  19.38163374,   6.45563628,  12.0755808 ,\n         0.90372386,   0.        ,   8.29971217,  29.44775501,\n        23.33491296,  25.80718148,  12.5482116 ,  10.15175395,\n        26.38410576,  38.06253589,  52.00790943,  82.63320756,\n        10.04313675,  39.01542845,  11.83534301,  24.36216167,\n         0.        ,  36.35342184,   1.51834634,   1.61287464,\n        11.03044295,   3.44329813,  76.99523447,   0.        ,\n        16.25783756,  28.20808019,   0.        ,   0.        ,\n        19.46729005,   5.23993577,  16.14610925,   0.        ,\n         0.        ,   0.        ,  19.96795078,   1.97252796,\n        10.84627396,   0.        ,  43.36145952,   5.53146636,\n         1.31794268,   2.62059041,  40.49646268,   0.        ,\n         0.        ,   0.        ,   0.1206393 ,  59.5075802 ,\n         8.46419934, 100.21132471,  14.67992158,   0.        ])</pre> <p>The observed values can include many zero values, as the compound distribution has an atom as 0.</p> In\u00a0[8]: Copied! <pre>print(\"Number of zeros:\", np.sum(xData == 0))\n</pre> print(\"Number of zeros:\", np.sum(xData == 0)) <pre>Number of zeros: 21\n</pre> <p>With this data, we create objects to represent the data-generating process (the model) and the prior distribution. The priors are simply that $p \\sim \\mathsf{Unif}(0, 1)$ and $\\delta \\sim \\mathsf{Unif}(0, 100).$</p> In\u00a0[9]: Copied! <pre>model = abc.Model(freq, sev, psi)\nparams = (\"$p$\", \"$\\\\delta$\")\nprior = abc.IndependentUniformPrior([(0, 1), (0, 100)], params)\n</pre> model = abc.Model(freq, sev, psi) params = (\"$p$\", \"$\\\\delta$\") prior = abc.IndependentUniformPrior([(0, 1), (0, 100)], params) <p>After, we call the main <code>smc</code> method which is provided by <code>approxbayescomp</code> to fit the observed <code>xData</code>.</p> In\u00a0[10]: Copied! <pre>numIters = 9\npopSize = 100\n%time fit = abc.smc(numIters, popSize, xData, model, prior, seed=1)\n</pre> numIters = 9 popSize = 100 %time fit = abc.smc(numIters, popSize, xData, model, prior, seed=1) <pre>CPU times: user 1.11 s, sys: 18.3 ms, total: 1.13 s\nWall time: 1.13 s\n</pre> <p>These particles all generated fake data within the following distance to the observed data:</p> In\u00a0[11]: Copied! <pre>np.max(fit.dists)\n</pre> np.max(fit.dists) Out[11]: <pre>1.5382234993824915</pre> <p>Plotting the fitted ABC posterior:</p> In\u00a0[12]: Copied! <pre>abc.plot_posteriors(fit, prior, refLines=trueTheta)\n</pre> abc.plot_posteriors(fit, prior, refLines=trueTheta) In\u00a0[13]: Copied! <pre>%time fitMatchZeros = abc.smc(numIters, popSize, xData, model, prior, matchZeros=True, seed=1)\n</pre> %time fitMatchZeros = abc.smc(numIters, popSize, xData, model, prior, matchZeros=True, seed=1) <pre>CPU times: user 1min 20s, sys: 272 ms, total: 1min 21s\nWall time: 1min 21s\n</pre> <p>These particles all generated fake data within the following distance to the observed data:</p> In\u00a0[14]: Copied! <pre>np.max(fitMatchZeros.dists)\n</pre> np.max(fitMatchZeros.dists) Out[14]: <pre>1.1057819982112789</pre> <p>Plotting the fitted ABC posterior:</p> In\u00a0[15]: Copied! <pre>abc.plot_posteriors(\n    fitMatchZeros, prior, refLines=trueTheta\n)\n</pre> abc.plot_posteriors(     fitMatchZeros, prior, refLines=trueTheta ) <p>As expected, the particles generated with <code>matchZeros=True</code> are better quality particles which generate a more accurate ABC posterior, though they required more computational time to obtain.</p> <p>As the likelihood for this simple/special case is tractable, we can fit the data using the standard Markov Chain Monte Carlo method via <code>PyMC</code>:</p> In\u00a0[22]: Copied! <pre>import pymc as pm\nimport pytensor.tensor as pt\n\n# Log probability of the compound-sum variable\ndef logp(value, p, \u03b4):\n    t0 = pt.sum(pt.eq(value, 0))\n    sumData = pt.sum(value)\n    return T * pt.log(1 - p) + (T - t0) * pt.log(p / \u03b4) - (1 - p) / \u03b4 * sumData\n\nbasic_model = pm.Model()\n\nwith basic_model:\n    # Priors for unknown model parameters\n    p = pm.Uniform(\"p\", 0, 1)\n    \u03b4 = pm.Uniform(\"\u03b4\", 0, 100)\n\n    exp_surv = pm.CustomDist(\"X\", p, \u03b4, logp=logp, observed=xData)\n\n    trace = pm.sample(\n        1000, tune=500, chains=1, random_seed=1, return_inferencedata=True\n    )\n</pre> import pymc as pm import pytensor.tensor as pt  # Log probability of the compound-sum variable def logp(value, p, \u03b4):     t0 = pt.sum(pt.eq(value, 0))     sumData = pt.sum(value)     return T * pt.log(1 - p) + (T - t0) * pt.log(p / \u03b4) - (1 - p) / \u03b4 * sumData  basic_model = pm.Model()  with basic_model:     # Priors for unknown model parameters     p = pm.Uniform(\"p\", 0, 1)     \u03b4 = pm.Uniform(\"\u03b4\", 0, 100)      exp_surv = pm.CustomDist(\"X\", p, \u03b4, logp=logp, observed=xData)      trace = pm.sample(         1000, tune=500, chains=1, random_seed=1, return_inferencedata=True     ) <pre>Auto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (1 chains in 1 job)\nNUTS: [p, \u03b4]\n</pre>        100.00% [1500/1500 00:00&lt;00:00 Sampling chain 0, 0 divergences]      <pre>Sampling 1 chain for 500 tune and 1_000 draw iterations (500 + 1_000 draws total) took 0 seconds.\nOnly one chain was sampled, this makes it impossible to run some convergence checks\n</pre> <p>Comparing this 'true' MCMC posterior to the 'approximate' ABC posterior we can see there is quite a strong agreement between the two:</p> In\u00a0[23]: Copied! <pre>import matplotlib.pyplot as plt\n\nabc.plot_posteriors(fitMatchZeros, prior)\n\nfig = plt.gcf()\naxs = fig.axes\n\nfor l, param in enumerate([\"p\", \"\u03b4\"]):\n    nMCMC = len(trace.posterior[param][0])\n    abc.weighted_distplot(\n        trace.posterior[param][0], np.ones(nMCMC) / nMCMC, ax=axs[l], hist=False\n    )\n\nplt.legend([\"ABC\", \"Prior\", \"MCMC\"]);\n</pre> import matplotlib.pyplot as plt  abc.plot_posteriors(fitMatchZeros, prior)  fig = plt.gcf() axs = fig.axes  for l, param in enumerate([\"p\", \"\u03b4\"]):     nMCMC = len(trace.posterior[param][0])     abc.weighted_distplot(         trace.posterior[param][0], np.ones(nMCMC) / nMCMC, ax=axs[l], hist=False     )  plt.legend([\"ABC\", \"Prior\", \"MCMC\"]);"},{"location":"geometric-exponential/#geometric-exponential-compound-sums","title":"Geometric-Exponential Compound Sums\u00b6","text":""},{"location":"geometric-exponential/#generating-some-synthetic-data-to-fit","title":"Generating some synthetic data to fit\u00b6","text":""},{"location":"geometric-exponential/#use-abc-to-fit-the-data","title":"Use ABC to fit the data\u00b6","text":""},{"location":"geometric-exponential/#ignoring-the-zeros","title":"Ignoring the zeros\u00b6","text":""},{"location":"geometric-exponential/#matching-the-zeros","title":"Matching the zeros\u00b6","text":"<p>Technically, the theory behind ABC doesn't like the fact that the compound distribution we're fitting is a combination of a continuous distribution and a discrete atom (at 0). The theory says that we should reject fake data which contains a different number of zero observations as the observed data. This behaviour can be enabled by the <code>matchZeros</code> flag. When the observed data contains zeros, enabling <code>matchZeros</code> should increase the accuracy of the fits, though it will significantly increase the computational time.</p>"},{"location":"geometric-exponential/#comparing-against-mcmc-with-pymc3","title":"Comparing against MCMC with PyMC3\u00b6","text":""},{"location":"insurance-model-example/","title":"Insurance model example","text":"In\u00a0[\u00a0]: Copied! <pre>import approxbayescomp as abc\n</pre> import approxbayescomp as abc In\u00a0[\u00a0]: Copied! <pre># Load data to fit (modify this line to load real observations!)\nobsData = [1.0, 2.0, 3.0]\n</pre> # Load data to fit (modify this line to load real observations!) obsData = [1.0, 2.0, 3.0] In\u00a0[\u00a0]: Copied! <pre># Specify our prior beliefs over (lambda, mu, sigma).\nprior = abc.IndependentUniformPrior([(0, 100), (-5, 5), (0, 3)])\n</pre> # Specify our prior beliefs over (lambda, mu, sigma). prior = abc.IndependentUniformPrior([(0, 100), (-5, 5), (0, 3)]) In\u00a0[\u00a0]: Copied! <pre># Fit the model to the data using ABC\nmodel = abc.Model(\"poisson\", \"lognormal\", abc.Psi(\"sum\"))\nnumIters = 6  # The number of SMC iterations to perform\npopSize = 250  # The population size of the SMC method\n</pre> # Fit the model to the data using ABC model = abc.Model(\"poisson\", \"lognormal\", abc.Psi(\"sum\")) numIters = 6  # The number of SMC iterations to perform popSize = 250  # The population size of the SMC method In\u00a0[\u00a0]: Copied! <pre>fit = abc.smc(numIters, popSize, obsData, model, prior)\n</pre> fit = abc.smc(numIters, popSize, obsData, model, prior)"},{"location":"model-selection/","title":"Model Selection (WIP)","text":"<p>One side-benefit of ABC is it straight-forward to use it to perform model selection. In this example, we'll simulate synthetic data where the claim frequency</p> <p>$$ N_s \\overset{\\text{i.i.d.}}{\\sim}\\text{NegativeBinomial}(\\alpha = 4, p = 2/3),\\text{ } $$</p> <p>and the individual claim sizes are Weibull distributed</p> <p>$$ U_{i,1}, \\ldots, U_{N_i}\\underset{\\textbf{i.i.d.}}{\\sim}\\text{Weib}(k = 1/2, \\beta = 1),\\text{ }s = 1,\\ldots 30. $$</p> <p>The available data is aggregated claim sizes in excess of the priority $c=1$ asociated to aa global stop-loss treaty, we have</p> <p>$$ x_s = \\left(\\sum_{k = 1}^{n_s}u_k-c\\right)_{+},\\text{ }s = 1,\\ldots, t. $$</p> <p>Our aim is to look into the finite sample performance of our ABC implementation when deciding which model is the most suited.</p> In\u00a0[1]: Copied! <pre>from preamble import *\nfrom infer_loss_distribution import *\nfrom infer_count_distribution import *\n\n%config InlineBackend.figure_format = 'retina'\n%load_ext lab_black\n</pre> from preamble import * from infer_loss_distribution import * from infer_count_distribution import *  %config InlineBackend.figure_format = 'retina' %load_ext lab_black In\u00a0[3]: Copied! <pre>import sys\n\nprint(\"ABC version:\", abc.__version__)\nprint(\"Python version:\", sys.version)\nprint(\"Numpy version:\", np.__version__)\nprint(\"PyMC3 version:\", pm.__version__)\nprint(\"Arviz version:\", arviz.__version__)\n\ntic()\n</pre> import sys  print(\"ABC version:\", abc.__version__) print(\"Python version:\", sys.version) print(\"Numpy version:\", np.__version__) print(\"PyMC3 version:\", pm.__version__) print(\"Arviz version:\", arviz.__version__)  tic() <pre>ABC version: 0.1.1\nPython version: 3.8.10 (default, May 19 2021, 18:05:58) \n[GCC 7.3.0]\nNumpy version: 1.20.3\nPyMC3 version: 3.11.2\nArviz version: 0.11.2\n</pre> In\u00a0[4]: Copied! <pre>FAST = False\n\n# Processor information and SMC calibration parameters\nif not FAST:\n    numIters = 7\n    numItersData = 10\n    popSize = 1000\n    popSizeModels = 1000\n    epsMin = 0\n    timeout = 1000\nelse:\n    numIters = 4\n    numItersData = 8\n    popSize = 500\n    popSizeModels = 1000\n    epsMin = 1\n    timeout = 30\n\nsmcArgs = {\"timeout\": timeout, \"epsMin\": epsMin, \"verbose\": True}\nsmcArgs[\"numProcs\"] = 40\n</pre> FAST = False  # Processor information and SMC calibration parameters if not FAST:     numIters = 7     numItersData = 10     popSize = 1000     popSizeModels = 1000     epsMin = 0     timeout = 1000 else:     numIters = 4     numItersData = 8     popSize = 500     popSizeModels = 1000     epsMin = 1     timeout = 30  smcArgs = {\"timeout\": timeout, \"epsMin\": epsMin, \"verbose\": True} smcArgs[\"numProcs\"] = 40 In\u00a0[5]: Copied! <pre>rg = default_rng(123)\n\nsample_sizes = [50, 250]\nT = sample_sizes[-1]\nt = np.arange(1, T + 1, 1)\n\n# Frequency-Loss Model\n\u03b1, p, k, \u03b2 = 4, 2 / 3, 1 / 3, 1\n\u03b8_True = \u03b1, p, k, \u03b2\n\u03b8_sev = k, \u03b2\n\u03b8_freq = \u03b1, p\nfreq = \"negative binomial\"\nsev = \"weibull\"\n\n# Aggregation process\nc = 1\npsi = abc.Psi(\"GSL\", c)\n\nfreqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, \u03b8_True)\ndf_full = pd.DataFrame(\n    {\n        \"time_period\": np.concatenate([np.repeat(s, freqs[s - 1]) for s in t]),\n        \"claim_size\": sevs,\n    }\n)\n\nxData = abc.compute_psi(freqs, sevs, psi)\n\ndf_agg = pd.DataFrame({\"time_period\": t, \"N\": freqs, \"X\": xData})\n</pre> rg = default_rng(123)  sample_sizes = [50, 250] T = sample_sizes[-1] t = np.arange(1, T + 1, 1)  # Frequency-Loss Model \u03b1, p, k, \u03b2 = 4, 2 / 3, 1 / 3, 1 \u03b8_True = \u03b1, p, k, \u03b2 \u03b8_sev = k, \u03b2 \u03b8_freq = \u03b1, p freq = \"negative binomial\" sev = \"weibull\"  # Aggregation process c = 1 psi = abc.Psi(\"GSL\", c)  freqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, \u03b8_True) df_full = pd.DataFrame(     {         \"time_period\": np.concatenate([np.repeat(s, freqs[s - 1]) for s in t]),         \"claim_size\": sevs,     } )  xData = abc.compute_psi(freqs, sevs, psi)  df_agg = pd.DataFrame({\"time_period\": t, \"N\": freqs, \"X\": xData}) In\u00a0[6]: Copied! <pre>[np.sum(xData[:ss] &gt; 0) for ss in sample_sizes]\n</pre> [np.sum(xData[:ss] &gt; 0) for ss in sample_sizes] Out[6]: <pre>[23, 130]</pre> In\u00a0[7]: Copied! <pre>Bayesian_Summary = pd.DataFrame({\"model\": [], \"ss\": [], \"k\": [], \"\u03b2\": []})\nmodels = [\"True weibull\", \"True gamma\"]\nfor m in models:\n    for ss in sample_sizes:\n\n        uData = np.array(df_full.claim_size[df_full.time_period &lt;= ss])\n        print(\"The number of individual claim sizes is \", len(uData))\n        if m == \"True weibull\":\n            # We fit a Weibull model using SMC\n            with pm.Model() as model_sev:\n                k = pm.Uniform(\"k\", lower=1e-1, upper=10)\n                \u03b2 = pm.Uniform(\"\u03b2\", lower=0, upper=20)\n                U = pm.Weibull(\"U\", alpha=k, beta=\u03b2, observed=uData)\n                %time trace = pm.sample_smc(popSize, random_seed=1, chains=1)\n\n        elif m == \"True gamma\":\n            # We fit a gamma model using SMC\n            with pm.Model() as model_sev:\n                param1 = pm.Uniform(\"k\", lower=0, upper=10)\n                param2 = pm.Uniform(\"\u03b2\", lower=0, upper=50)\n                U = pm.Gamma(\"U\", alpha=param1, beta=1 / param2, observed=uData)\n                %time trace = pm.sample_smc(popSize, random_seed=1, chains=1)\n\n        arviz.plot_posterior(trace)\n\n        log_lik = trace.report.log_marginal_likelihood[0]\n\n        res = pd.DataFrame(\n            {\n                \"model\": [m],\n                \"ss\": [ss],\n                \"k\": [trace[\"k\"].mean()],\n                \"\u03b2\": [trace[\"\u03b2\"].mean()],\n                \"marginal_log_likelihood\": [log_lik],\n            }\n        )\n        Bayesian_Summary = pd.concat([Bayesian_Summary, res])\n\nmax_marginal_log_likelihood = (\n    Bayesian_Summary[[\"ss\", \"marginal_log_likelihood\"]]\n    .groupby(\"ss\")\n    .max()\n    .marginal_log_likelihood.values\n)\nmax_marginal_log_likelihood = np.concatenate(\n    [max_marginal_log_likelihood, max_marginal_log_likelihood]\n)\nBayesian_Summary[\"BF\"] = np.exp(\n    Bayesian_Summary.marginal_log_likelihood - max_marginal_log_likelihood\n)\nsum_BF = Bayesian_Summary[[\"ss\", \"BF\"]].groupby(\"ss\").sum().BF.values\nsum_BF = np.concatenate([sum_BF, sum_BF])\nBayesian_Summary[\"model_probability\"] = Bayesian_Summary.BF / sum_BF\nBayesian_Summary\n</pre> Bayesian_Summary = pd.DataFrame({\"model\": [], \"ss\": [], \"k\": [], \"\u03b2\": []}) models = [\"True weibull\", \"True gamma\"] for m in models:     for ss in sample_sizes:          uData = np.array(df_full.claim_size[df_full.time_period &lt;= ss])         print(\"The number of individual claim sizes is \", len(uData))         if m == \"True weibull\":             # We fit a Weibull model using SMC             with pm.Model() as model_sev:                 k = pm.Uniform(\"k\", lower=1e-1, upper=10)                 \u03b2 = pm.Uniform(\"\u03b2\", lower=0, upper=20)                 U = pm.Weibull(\"U\", alpha=k, beta=\u03b2, observed=uData)                 %time trace = pm.sample_smc(popSize, random_seed=1, chains=1)          elif m == \"True gamma\":             # We fit a gamma model using SMC             with pm.Model() as model_sev:                 param1 = pm.Uniform(\"k\", lower=0, upper=10)                 param2 = pm.Uniform(\"\u03b2\", lower=0, upper=50)                 U = pm.Gamma(\"U\", alpha=param1, beta=1 / param2, observed=uData)                 %time trace = pm.sample_smc(popSize, random_seed=1, chains=1)          arviz.plot_posterior(trace)          log_lik = trace.report.log_marginal_likelihood[0]          res = pd.DataFrame(             {                 \"model\": [m],                 \"ss\": [ss],                 \"k\": [trace[\"k\"].mean()],                 \"\u03b2\": [trace[\"\u03b2\"].mean()],                 \"marginal_log_likelihood\": [log_lik],             }         )         Bayesian_Summary = pd.concat([Bayesian_Summary, res])  max_marginal_log_likelihood = (     Bayesian_Summary[[\"ss\", \"marginal_log_likelihood\"]]     .groupby(\"ss\")     .max()     .marginal_log_likelihood.values ) max_marginal_log_likelihood = np.concatenate(     [max_marginal_log_likelihood, max_marginal_log_likelihood] ) Bayesian_Summary[\"BF\"] = np.exp(     Bayesian_Summary.marginal_log_likelihood - max_marginal_log_likelihood ) sum_BF = Bayesian_Summary[[\"ss\", \"BF\"]].groupby(\"ss\").sum().BF.values sum_BF = np.concatenate([sum_BF, sum_BF]) Bayesian_Summary[\"model_probability\"] = Bayesian_Summary.BF / sum_BF Bayesian_Summary <pre>The number of individual claim sizes is  98\n</pre> <pre>Initializing SMC sampler...\nSampling 1 chain in 1 job\nStage:   0 Beta: 0.000\nStage:   1 Beta: 0.001\nStage:   2 Beta: 0.015\nStage:   3 Beta: 0.069\nStage:   4 Beta: 0.163\nStage:   5 Beta: 0.427\nStage:   6 Beta: 1.000\n/home/plaub/miniconda3/lib/python3.8/site-packages/arviz/data/io_pymc3.py:96: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  warnings.warn(\n</pre> <pre>CPU times: user 1.37 s, sys: 88.3 ms, total: 1.46 s\nWall time: 2.4 s\nThe number of individual claim sizes is  525\n</pre> <pre>Initializing SMC sampler...\nSampling 1 chain in 1 job\nStage:   0 Beta: 0.000\nStage:   1 Beta: 0.000\nStage:   2 Beta: 0.003\nStage:   3 Beta: 0.014\nStage:   4 Beta: 0.034\nStage:   5 Beta: 0.090\nStage:   6 Beta: 0.276\nStage:   7 Beta: 0.946\nStage:   8 Beta: 1.000\n/home/plaub/miniconda3/lib/python3.8/site-packages/arviz/data/io_pymc3.py:96: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  warnings.warn(\n</pre> <pre>CPU times: user 1.96 s, sys: 4.12 ms, total: 1.97 s\nWall time: 1.96 s\nThe number of individual claim sizes is  98\n</pre> <pre>Initializing SMC sampler...\nSampling 1 chain in 1 job\nStage:   0 Beta: 0.001\nStage:   1 Beta: 0.003\nStage:   2 Beta: 0.016\nStage:   3 Beta: 0.076\nStage:   4 Beta: 0.339\nStage:   5 Beta: 1.000\n/home/plaub/miniconda3/lib/python3.8/site-packages/arviz/data/io_pymc3.py:96: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  warnings.warn(\n</pre> <pre>CPU times: user 1.27 s, sys: 12.2 ms, total: 1.28 s\nWall time: 1.28 s\nThe number of individual claim sizes is  525\n</pre> <pre>Initializing SMC sampler...\nSampling 1 chain in 1 job\nStage:   0 Beta: 0.000\nStage:   1 Beta: 0.001\nStage:   2 Beta: 0.003\nStage:   3 Beta: 0.015\nStage:   4 Beta: 0.070\nStage:   5 Beta: 0.353\nStage:   6 Beta: 1.000\n/home/plaub/miniconda3/lib/python3.8/site-packages/arviz/data/io_pymc3.py:96: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  warnings.warn(\n</pre> <pre>CPU times: user 1.15 s, sys: 7.64 ms, total: 1.16 s\nWall time: 1.15 s\n</pre> Out[7]: model ss k \u03b2 marginal_log_likelihood BF model_probability 0 True weibull 50.0 0.336828 1.012060 -90.848702 1.000000e+00 9.999998e-01 0 True weibull 250.0 0.315289 0.886874 -410.439780 1.000000e+00 1.000000e+00 0 True gamma 50.0 0.194597 40.320096 -106.467064 1.648283e-07 1.648283e-07 0 True gamma 250.0 0.188851 38.701698 -463.865195 6.275335e-24 6.275335e-24 In\u00a0[8]: Copied! <pre># Frequency-Loss Model\n\u03b1, p, k, \u03b2 = 4, 2 / 3, 1 / 3, 1\nrg = default_rng(123)\nuData_10000 = abc.simulate_claim_sizes(rg, 10000, sev, \u03b8_sev)\nr_mle, m_mle, BIC = infer_gamma(uData_10000, [1, 1])\n\n\u03b8_plot = [[\u03b1, p, k, \u03b2], [\u03b1, p, np.NaN, np.NaN]]\n\u03b8_mle = [[np.NaN, np.NaN, np.NaN, np.NaN], [np.NaN, np.NaN, r_mle, m_mle]]\n</pre> # Frequency-Loss Model \u03b1, p, k, \u03b2 = 4, 2 / 3, 1 / 3, 1 rg = default_rng(123) uData_10000 = abc.simulate_claim_sizes(rg, 10000, sev, \u03b8_sev) r_mle, m_mle, BIC = infer_gamma(uData_10000, [1, 1])  \u03b8_plot = [[\u03b1, p, k, \u03b2], [\u03b1, p, np.NaN, np.NaN]] \u03b8_mle = [[np.NaN, np.NaN, np.NaN, np.NaN], [np.NaN, np.NaN, r_mle, m_mle]] In\u00a0[9]: Copied! <pre>params = ((\"\u03b1\", \"p\", \"k\", \"\u03b2\"), (\"\u03b1\", \"p\", \"r\", \"m\"))\n\nprior1 = abc.IndependentUniformPrior(\n    [(0, 20), (1e-3, 1), (1e-1, 10), (0, 20)], params[0]\n)\nmodel1 = abc.Model(\"negative binomial\", \"weibull\", psi, prior1)\n\nprior2 = abc.IndependentUniformPrior(\n    [(0, 20), (1e-3, 1), (1e-1, 10), (0, 50)], params[1]\n)\nmodel2 = abc.Model(\"negative binomial\", \"gamma\", psi, prior2)\n\nmodels = [model1, model2]\nmodel_names = [\"ABC negative binomial - weibull\", \"ABC negative binomial - gamma\"]\n</pre> params = ((\"\u03b1\", \"p\", \"k\", \"\u03b2\"), (\"\u03b1\", \"p\", \"r\", \"m\"))  prior1 = abc.IndependentUniformPrior(     [(0, 20), (1e-3, 1), (1e-1, 10), (0, 20)], params[0] ) model1 = abc.Model(\"negative binomial\", \"weibull\", psi, prior1)  prior2 = abc.IndependentUniformPrior(     [(0, 20), (1e-3, 1), (1e-1, 10), (0, 50)], params[1] ) model2 = abc.Model(\"negative binomial\", \"gamma\", psi, prior2)  models = [model1, model2] model_names = [\"ABC negative binomial - weibull\", \"ABC negative binomial - gamma\"] In\u00a0[10]: Copied! <pre>model_proba_abc = pd.DataFrame({\"model\": [], \"ss\": [], \"model_probability\": []})\ndfabc = pd.DataFrame(\n    {\"model\": [], \"ss\": [], \"weights\": [], \"\u03b1\": [], \"p\": [], \"param1\": [], \"param2\": []}\n)\n\nfor ss in sample_sizes:\n    xDataSS = df_agg.X[df_agg.time_period &lt;= ss].values\n\n    %time fit = abc.smc(numIters, popSizeModels, xDataSS, models, **smcArgs)\n\n    for k in range(len(models)):\n        weights = fit.weights[fit.models == k]\n        res_mp = pd.DataFrame(\n            {\n                \"model\": pd.Series([model_names[k]]),\n                \"ss\": np.array([ss]),\n                \"model_probability\": pd.Series(np.sum(fit.weights[fit.models == k])),\n            }\n        )\n\n        model_proba_abc = pd.concat([model_proba_abc, res_mp])\n\n        res_post_samples = pd.DataFrame(\n            {\n                \"model\": np.repeat(model_names[k], len(weights)),\n                \"ss\": np.repeat(ss, len(weights)),\n                \"weights\": weights / np.sum(weights),\n                \"\u03b1\": np.array(fit.samples)[fit.models == k, 0],\n                \"p\": np.array(fit.samples)[fit.models == k, 1],\n                \"param1\": np.array(fit.samples)[fit.models == k, 2],\n                \"param2\": np.array(fit.samples)[fit.models == k, 3],\n            }\n        )\n        dfabc = pd.concat([dfabc, res_post_samples])\n</pre> model_proba_abc = pd.DataFrame({\"model\": [], \"ss\": [], \"model_probability\": []}) dfabc = pd.DataFrame(     {\"model\": [], \"ss\": [], \"weights\": [], \"\u03b1\": [], \"p\": [], \"param1\": [], \"param2\": []} )  for ss in sample_sizes:     xDataSS = df_agg.X[df_agg.time_period &lt;= ss].values      %time fit = abc.smc(numIters, popSizeModels, xDataSS, models, **smcArgs)      for k in range(len(models)):         weights = fit.weights[fit.models == k]         res_mp = pd.DataFrame(             {                 \"model\": pd.Series([model_names[k]]),                 \"ss\": np.array([ss]),                 \"model_probability\": pd.Series(np.sum(fit.weights[fit.models == k])),             }         )          model_proba_abc = pd.concat([model_proba_abc, res_mp])          res_post_samples = pd.DataFrame(             {                 \"model\": np.repeat(model_names[k], len(weights)),                 \"ss\": np.repeat(ss, len(weights)),                 \"weights\": weights / np.sum(weights),                 \"\u03b1\": np.array(fit.samples)[fit.models == k, 0],                 \"p\": np.array(fit.samples)[fit.models == k, 1],                 \"param1\": np.array(fit.samples)[fit.models == k, 2],                 \"param2\": np.array(fit.samples)[fit.models == k, 3],             }         )         dfabc = pd.concat([dfabc, res_post_samples])  Starting ABC-SMC with population size of 1000 and sample size of 50 (~&gt; 50) on 40 processes.<p>Finished iteration 0, eps = 161.91, time = 6.0s / 0.1m, ESS = [519 481] -&gt; [337 163], numSims = 1000 \tmodel populations = [337, 163], model weights = [0.67 0.33]<p>Finished iteration 1, eps = 15.15, time = 3.0s / 0.0m, ESS = [743 550] -&gt; [419  81], numSims = 612440 \tmodel populations = [462, 92], model weights = [0.83 0.17]<p>Finished iteration 2, eps = 13.46, time = 1.0s / 0.0m, ESS = [1065  600] -&gt; [370 130], numSims = 639590 \tmodel populations = [462, 145], model weights = [0.75 0.25]<p>Finished iteration 3, eps = 12.83, time = 1.0s / 0.0m, ESS = [1103  319] -&gt; [261 239], numSims = 877396 \tmodel populations = [362, 331], model weights = [0.62 0.38]<p>Finished iteration 4, eps = 12.03, time = 1.0s / 0.0m, ESS = [855 570] -&gt; [174 326], numSims = 1499735 \tmodel populations = [223, 444], model weights = [0.52 0.48]<p>Finished iteration 5, eps = 11.46, time = 2.0s / 0.0m, ESS = [449 916] -&gt; [145 355], numSims = 1724297 \tmodel populations = [183, 551], model weights = [0.5 0.5]<p>Finished iteration 6, eps = 10.90, time = 2.0s / 0.0m, ESS = [256 918] -&gt; [157 343], numSims = 2470486 \tmodel populations = [212, 473], model weights = [0.55 0.45]<p>Finished iteration 7, eps = 10.90, time = 3.0s / 0.1m, ESS = [284 962] -&gt; [261 533], numSims = 3294649 \tmodel populations = [326, 674], model weights = [0.55 0.45] </p></p></p></p></p></p></p></p> <pre>Final population dists &lt;= 10.62, ESS = [261 533]\n\tmodel populations = [326, 674], model weights = [0.55 0.45]\nCPU times: user 10.6 s, sys: 1.39 s, total: 11.9 s\nWall time: 21.6 s\n</pre>  Starting ABC-SMC with population size of 1000 and sample size of 250 (~&gt; 250) on 40 processes.<p>Finished iteration 0, eps = 158.81, time = 1.0s / 0.0m, ESS = [519 481] -&gt; [339 161], numSims = 1000 \tmodel populations = [339, 161], model weights = [0.68 0.32]<p>Finished iteration 1, eps = 12.51, time = 7.0s / 0.1m, ESS = [749 546] -&gt; [399 101], numSims = 3066721 \tmodel populations = [443, 115], model weights = [0.78 0.22]<p>Finished iteration 2, eps = 10.92, time = 3.0s / 0.1m, ESS = [1035  580] -&gt; [339 161], numSims = 2925521 \tmodel populations = [476, 184], model weights = [0.71 0.29]<p>Finished iteration 3, eps = 10.15, time = 6.0s / 0.1m, ESS = [1145  368] -&gt; [248 252], numSims = 4223080 \tmodel populations = [343, 327], model weights = [0.56 0.44]<p>Finished iteration 4, eps = 9.01, time = 8.0s / 0.1m, ESS = [920 648] -&gt; [167 333], numSims = 5836570 \tmodel populations = [203, 399], model weights = [0.44 0.56]<p>Finished iteration 5, eps = 8.05, time = 5.0s / 0.1m, ESS = [ 303 1035] -&gt; [ 59 441], numSims = 3371523 \tmodel populations = [102, 569], model weights = [0.46 0.54]<p>Finished iteration 6, eps = 7.30, time = 7.0s / 0.1m, ESS = [ 124 1367] -&gt; [ 45 455], numSims = 4398979 \tmodel populations = [59, 519], model weights = [0.55 0.45]<p>Finished iteration 7, eps = 7.30, time = 7.0s / 0.1m, ESS = [ 130 1481] -&gt; [104 726], numSims = 3788676 \tmodel populations = [138, 862], model weights = [0.59 0.41] </p></p></p></p></p></p></p></p> <pre>Final population dists &lt;= 7.07, ESS = [104 726]\n\tmodel populations = [138, 862], model weights = [0.59 0.41]\nCPU times: user 9.46 s, sys: 655 ms, total: 10.1 s\nWall time: 44.5 s\n</pre> In\u00a0[11]: Copied! <pre>for l in range(len(models)):\n    fig, axs = plt.subplots(1, len(params[l]), tight_layout=True)\n    prior = models[l].prior\n\n    for k in range(len(params[l])):\n        pLims = [prior.marginals[k].isf(1), prior.marginals[k].isf(0)]\n        axs[k].set_xlim(pLims)\n\n        for i, ss in enumerate(sample_sizes):\n            selector = (dfabc.ss == ss) &amp; (dfabc.model == model_names[l])\n            sample = np.array(dfabc[[\"\u03b1\", \"p\", \"param1\", \"param2\"]])[selector, k]\n            weights = dfabc.weights[selector].values\n            dataResampled, xs, ys = abc.resample_and_kde(\n                sample, weights / sum(weights), clip=pLims\n            )\n            axs[k].plot(xs, ys)\n            axs[k].axvline(\u03b8_plot[l][k], **trueStyle)\n            axs[k].axvline(\u03b8_mle[l][k], **mleStyle)\n\n            axs[k].set_title(\"$\" + params[l][k] + \"$\")\n            axs[k].set_yticks([])\n\n    sns.despine(left=True)\n    # plt.save_cropped(f\"../Figures/hist-negbin-weibull-model-selection-{l}.pdf\")\n</pre> for l in range(len(models)):     fig, axs = plt.subplots(1, len(params[l]), tight_layout=True)     prior = models[l].prior      for k in range(len(params[l])):         pLims = [prior.marginals[k].isf(1), prior.marginals[k].isf(0)]         axs[k].set_xlim(pLims)          for i, ss in enumerate(sample_sizes):             selector = (dfabc.ss == ss) &amp; (dfabc.model == model_names[l])             sample = np.array(dfabc[[\"\u03b1\", \"p\", \"param1\", \"param2\"]])[selector, k]             weights = dfabc.weights[selector].values             dataResampled, xs, ys = abc.resample_and_kde(                 sample, weights / sum(weights), clip=pLims             )             axs[k].plot(xs, ys)             axs[k].axvline(\u03b8_plot[l][k], **trueStyle)             axs[k].axvline(\u03b8_mle[l][k], **mleStyle)              axs[k].set_title(\"$\" + params[l][k] + \"$\")             axs[k].set_yticks([])      sns.despine(left=True)     # plt.save_cropped(f\"../Figures/hist-negbin-weibull-model-selection-{l}.pdf\") In\u00a0[12]: Copied! <pre>params = ((\"k\", \"\u03b2\"), (\"r\", \"m\"))\n\nprior1 = abc.IndependentUniformPrior([(1e-1, 10), (0, 20)], params[0])\nprior2 = abc.IndependentUniformPrior([(0, 10), (0, 50)], params[1])\n\nmodel_names = (\"ABC with freqs - weibull\", \"ABC with freqs - gamma\")\n</pre> params = ((\"k\", \"\u03b2\"), (\"r\", \"m\"))  prior1 = abc.IndependentUniformPrior([(1e-1, 10), (0, 20)], params[0]) prior2 = abc.IndependentUniformPrior([(0, 10), (0, 50)], params[1])  model_names = (\"ABC with freqs - weibull\", \"ABC with freqs - gamma\") In\u00a0[13]: Copied! <pre>model_proba_abc_freq = pd.DataFrame({\"model\": [], \"ss\": [], \"model_probability\": []})\ndfabc_freq = pd.DataFrame(\n    {\"model\": [], \"ss\": [], \"weights\": [], \"param1\": [], \"param2\": []}\n)\n\nfor ss in sample_sizes:\n    xDataSS = df_agg.X[df_agg.time_period &lt;= ss].values\n    nData = df_agg.N[df_agg.time_period &lt;= ss].values\n\n    model1 = abc.Model(nData, \"weibull\", psi, prior1)\n    model2 = abc.Model(nData, \"gamma\", psi, prior2)\n    models = [model1, model2]\n\n    %time fit = abc.smc(numItersData, popSizeModels, xDataSS, models, **smcArgs)\n\n    for k in range(len(models)):\n        weights = fit.weights[fit.models == k]\n        res_mp = pd.DataFrame(\n            {\n                \"model\": pd.Series([model_names[k]]),\n                \"ss\": np.array([ss]),\n                \"model_probability\": pd.Series(np.sum(fit.weights[fit.models == k])),\n            }\n        )\n\n        model_proba_abc_freq = pd.concat([model_proba_abc_freq, res_mp])\n\n        res_post_samples = pd.DataFrame(\n            {\n                \"model\": np.repeat(model_names[k], len(weights)),\n                \"ss\": np.repeat(ss, len(weights)),\n                \"weights\": weights / np.sum(weights),\n                \"param1\": np.array(fit.samples)[fit.models == k, 0],\n                \"param2\": np.array(fit.samples)[fit.models == k, 1],\n            }\n        )\n        dfabc_freq = pd.concat([dfabc_freq, res_post_samples])\n</pre> model_proba_abc_freq = pd.DataFrame({\"model\": [], \"ss\": [], \"model_probability\": []}) dfabc_freq = pd.DataFrame(     {\"model\": [], \"ss\": [], \"weights\": [], \"param1\": [], \"param2\": []} )  for ss in sample_sizes:     xDataSS = df_agg.X[df_agg.time_period &lt;= ss].values     nData = df_agg.N[df_agg.time_period &lt;= ss].values      model1 = abc.Model(nData, \"weibull\", psi, prior1)     model2 = abc.Model(nData, \"gamma\", psi, prior2)     models = [model1, model2]      %time fit = abc.smc(numItersData, popSizeModels, xDataSS, models, **smcArgs)      for k in range(len(models)):         weights = fit.weights[fit.models == k]         res_mp = pd.DataFrame(             {                 \"model\": pd.Series([model_names[k]]),                 \"ss\": np.array([ss]),                 \"model_probability\": pd.Series(np.sum(fit.weights[fit.models == k])),             }         )          model_proba_abc_freq = pd.concat([model_proba_abc_freq, res_mp])          res_post_samples = pd.DataFrame(             {                 \"model\": np.repeat(model_names[k], len(weights)),                 \"ss\": np.repeat(ss, len(weights)),                 \"weights\": weights / np.sum(weights),                 \"param1\": np.array(fit.samples)[fit.models == k, 0],                 \"param2\": np.array(fit.samples)[fit.models == k, 1],             }         )         dfabc_freq = pd.concat([dfabc_freq, res_post_samples])  Starting ABC-SMC with population size of 1000 and sample size of 50 (~&gt; 50) on 40 processes.<p>Finished iteration 0, eps = 31.52, time = 1.0s / 0.0m, ESS = [519 481] -&gt; [431  69], numSims = 1000 \tmodel populations = [431, 69], model weights = [0.86 0.14]<p>Finished iteration 1, eps = 14.61, time = 1.0s / 0.0m, ESS = [712 585] -&gt; [ 52 448], numSims = 1210651 \tmodel populations = [52, 573], model weights = [0.16 0.84]<p>Finished iteration 2, eps = 11.63, time = 1.0s / 0.0m, ESS = [ 490 1247] -&gt; [ 79 421], numSims = 177485 \tmodel populations = [106, 470], model weights = [0.14 0.86]<p>Finished iteration 3, eps = 10.77, time = 1.0s / 0.0m, ESS = [647 928] -&gt; [278 222], numSims = 39556 \tmodel populations = [321, 298], model weights = [0.2 0.8]<p>Finished iteration 4, eps = 9.68, time = 1.0s / 0.0m, ESS = [966 697] -&gt; [397 103], numSims = 72111 \tmodel populations = [455, 113], model weights = [0.39 0.61]<p>Finished iteration 5, eps = 8.11, time = 1.0s / 0.0m, ESS = [1240  510] -&gt; [471  29], numSims = 166508 \tmodel populations = [521, 30], model weights = [0.75 0.25]<p>Finished iteration 6, eps = 6.41, time = 1.0s / 0.0m, ESS = [1362  223] -&gt; [495   5], numSims = 376363 \tmodel populations = [626, 5], model weights = [0.97 0.03]<p>Finished iteration 7, eps = 4.57, time = 1.0s / 0.0m, ESS = [1626   68] -&gt; [498   0], numSims = 925220 \tmodel populations = [548, 0], model weights = [1. 0.]<p>Finished iteration 8, eps = 3.36, time = 2.0s / 0.0m, ESS = [1690   19] -&gt; [499   0], numSims = 2025936 \tmodel populations = [594, 0], model weights = [1. 0.]<p>Finished iteration 9, eps = 2.48, time = 3.0s / 0.1m, ESS = [1676    0] -&gt; [500   0], numSims = 4807213 \tmodel populations = [573, 0], model weights = [1. 0.]<p>Finished iteration 10, eps = 2.48, time = 7.0s / 0.1m, ESS = [1630    0] -&gt; [874   0], numSims = 10158051 \tmodel populations = [1000, 0], model weights = [1. 0.] </p></p></p></p></p></p></p></p></p></p></p> <pre>Final population dists &lt;= 2.24, ESS = [874   0]\n\tmodel populations = [1000, 0], model weights = [1. 0.]\nCPU times: user 12.3 s, sys: 983 ms, total: 13.3 s\nWall time: 20.9 s\n</pre>  Starting ABC-SMC with population size of 1000 and sample size of 250 (~&gt; 250) on 40 processes.<p>Finished iteration 0, eps = 26.17, time = 1.0s / 0.0m, ESS = [519 481] -&gt; [430  70], numSims = 1000 \tmodel populations = [430, 70], model weights = [0.86 0.14]<p>Finished iteration 1, eps = 12.65, time = 5.0s / 0.1m, ESS = [779 511] -&gt; [125 375], numSims = 4312120 \tmodel populations = [125, 504], model weights = [0.37 0.63]<p>Finished iteration 2, eps = 7.95, time = 4.0s / 0.1m, ESS = [ 501 1343] -&gt; [ 66 434], numSims = 2598910 \tmodel populations = [68, 486], model weights = [0.16 0.84]<p>Finished iteration 3, eps = 6.94, time = 1.0s / 0.0m, ESS = [ 383 1219] -&gt; [268 232], numSims = 55989 \tmodel populations = [426, 262], model weights = [0.31 0.69]<p>Finished iteration 4, eps = 5.34, time = 1.0s / 0.0m, ESS = [1029  698] -&gt; [488  12], numSims = 89275 \tmodel populations = [558, 12], model weights = [0.89 0.11]<p>Finished iteration 5, eps = 4.45, time = 1.0s / 0.0m, ESS = [1343  264] -&gt; [599   0], numSims = 165564 \tmodel populations = [701, 0], model weights = [1. 0.]<p>Finished iteration 6, eps = 4.45, time = 1.0s / 0.0m, ESS = [1721   21] -&gt; [1336    0], numSims = 267283 \tmodel populations = [1505, 0], model weights = [1. 0.]<p>Finished iteration 7, eps = 3.27, time = 1.0s / 0.0m, ESS = [1777    0] -&gt; [500   0], numSims = 272925 \tmodel populations = [525, 0], model weights = [1. 0.]<p>Finished iteration 8, eps = 2.84, time = 1.0s / 0.0m, ESS = [1682    0] -&gt; [500   0], numSims = 783279 \tmodel populations = [603, 0], model weights = [1. 0.]<p>Finished iteration 9, eps = 2.46, time = 2.0s / 0.0m, ESS = [1691    0] -&gt; [500   0], numSims = 1960341 \tmodel populations = [583, 0], model weights = [1. 0.]<p>Finished iteration 10, eps = 2.46, time = 4.0s / 0.1m, ESS = [1753    0] -&gt; [872   0], numSims = 3912166 \tmodel populations = [1000, 0], model weights = [1. 0.] </p></p></p></p></p></p></p></p></p></p></p> <pre>/home/plaub/miniconda3/lib/python3.8/site-packages/approxbayescomp/smc.py:371: RuntimeWarning: invalid value encountered in double_scalars\n  np.sum(weights[ms == m]) ** 2\n</pre> <pre>Final population dists &lt;= 2.35, ESS = [872   0]\n\tmodel populations = [1000, 0], model weights = [1. 0.]\nCPU times: user 11 s, sys: 904 ms, total: 11.9 s\nWall time: 22.4 s\n</pre> In\u00a0[14]: Copied! <pre>for l in range(len(models)):\n    modelName = model_names[l]\n    prior = models[l].prior\n    fig, axs = plt.subplots(1, len(params[l]), tight_layout=True)\n\n    for k in range(len(params[l])):\n        pLims = [prior.marginals[k].isf(1), prior.marginals[k].isf(0)]\n        # axs[k].set_xlim(pLims)\n\n        for ss in sample_sizes:\n            sampleData = dfabc_freq.query(\"ss == @ss &amp; model == @modelName\")\n            selector = (dfabc_freq.ss == ss) &amp; (dfabc_freq.model == model_names[l])\n            sample = np.array(dfabc_freq[[\"param1\", \"param2\"]])[selector, k]\n            weights = dfabc_freq.weights[selector].values\n            if sampleData.shape[0] &gt; 1:\n                dataResampled, xs, ys = abc.resample_and_kde(\n                    sample, weights / sum(weights), clip=pLims\n                )\n                axs[k].plot(xs, ys)\n\n            axs[k].axvline(\u03b8_plot[l][k + 2], **trueStyle)\n            axs[k].axvline(\u03b8_mle[l][k + 2], **mleStyle)\n\n            axs[k].set_title(\n                \"$\" + params[l][k] + f\"\\\\in ({pLims[0]:.0f}, {pLims[1]:.0f})$\"\n            )\n            axs[k].set_yticks([])\n\n    sns.despine(left=True)\n    # plt.save_cropped(f\"../Figures/hist-freqs-weibull-model-selection-{l}.pdf\")\nBayesian_Summary\n</pre> for l in range(len(models)):     modelName = model_names[l]     prior = models[l].prior     fig, axs = plt.subplots(1, len(params[l]), tight_layout=True)      for k in range(len(params[l])):         pLims = [prior.marginals[k].isf(1), prior.marginals[k].isf(0)]         # axs[k].set_xlim(pLims)          for ss in sample_sizes:             sampleData = dfabc_freq.query(\"ss == @ss &amp; model == @modelName\")             selector = (dfabc_freq.ss == ss) &amp; (dfabc_freq.model == model_names[l])             sample = np.array(dfabc_freq[[\"param1\", \"param2\"]])[selector, k]             weights = dfabc_freq.weights[selector].values             if sampleData.shape[0] &gt; 1:                 dataResampled, xs, ys = abc.resample_and_kde(                     sample, weights / sum(weights), clip=pLims                 )                 axs[k].plot(xs, ys)              axs[k].axvline(\u03b8_plot[l][k + 2], **trueStyle)             axs[k].axvline(\u03b8_mle[l][k + 2], **mleStyle)              axs[k].set_title(                 \"$\" + params[l][k] + f\"\\\\in ({pLims[0]:.0f}, {pLims[1]:.0f})$\"             )             axs[k].set_yticks([])      sns.despine(left=True)     # plt.save_cropped(f\"../Figures/hist-freqs-weibull-model-selection-{l}.pdf\") Bayesian_Summary In\u00a0[16]: Copied! <pre>model_proba_df = pd.concat(\n    [\n        Bayesian_Summary[[\"model\", \"ss\", \"model_probability\"]],\n        model_proba_abc,\n        model_proba_abc_freq,\n    ]\n)\nmodel_proba_df = model_proba_df[\n    np.char.find(model_proba_df.model.tolist(), \"weibull\") &gt; -1\n]\n\nmodel_proba_df.model = model_proba_df.model.replace(\n    {\n        \"True weibull\": \"True\\n(w/ $N$'s, $U$'s)\",\n        \"ABC negative binomial - weibull\": \"ABC\\n(w/ $X$'s)\",\n        \"ABC with freqs - weibull\": \"ABC\\n(w/ $X$'s, $N$'s)\",\n    }\n)\n\nmodel_proba_df = model_proba_df.sort_values(\"model\")\n\nfig, ax = plt.subplots(1, 1, tight_layout=True)\n\ng = sns.barplot(\n    x=\"model\",\n    y=\"model_probability\",\n    hue=\"ss\",\n    data=model_proba_df,\n    #     legend=False,\n    ax=ax,\n)\nplt.legend([], frameon=False)\nplt.ylabel(\"\")\nplt.xlabel(\"\")\nplt.title(\"\")\n\nsns.despine()\nsave_cropped(\"../Figures-Slides/barplot-negbin-weibull-model-selection.pdf\")\n</pre> model_proba_df = pd.concat(     [         Bayesian_Summary[[\"model\", \"ss\", \"model_probability\"]],         model_proba_abc,         model_proba_abc_freq,     ] ) model_proba_df = model_proba_df[     np.char.find(model_proba_df.model.tolist(), \"weibull\") &gt; -1 ]  model_proba_df.model = model_proba_df.model.replace(     {         \"True weibull\": \"True\\n(w/ $N$'s, $U$'s)\",         \"ABC negative binomial - weibull\": \"ABC\\n(w/ $X$'s)\",         \"ABC with freqs - weibull\": \"ABC\\n(w/ $X$'s, $N$'s)\",     } )  model_proba_df = model_proba_df.sort_values(\"model\")  fig, ax = plt.subplots(1, 1, tight_layout=True)  g = sns.barplot(     x=\"model\",     y=\"model_probability\",     hue=\"ss\",     data=model_proba_df,     #     legend=False,     ax=ax, ) plt.legend([], frameon=False) plt.ylabel(\"\") plt.xlabel(\"\") plt.title(\"\")  sns.despine() save_cropped(\"../Figures-Slides/barplot-negbin-weibull-model-selection.pdf\") In\u00a0[17]: Copied! <pre>model_proba_df = pd.concat(\n    [\n        Bayesian_Summary[[\"model\", \"ss\", \"model_probability\"]],\n        model_proba_abc,\n        model_proba_abc_freq,\n    ]\n)\nprint(\n    pd.pivot_table(\n        model_proba_df,\n        values=\"model_probability\",\n        index=[\"ss\"],\n        columns=[\"model\"],\n        aggfunc=np.sum,\n    ).to_latex()\n)\n</pre> model_proba_df = pd.concat(     [         Bayesian_Summary[[\"model\", \"ss\", \"model_probability\"]],         model_proba_abc,         model_proba_abc_freq,     ] ) print(     pd.pivot_table(         model_proba_df,         values=\"model_probability\",         index=[\"ss\"],         columns=[\"model\"],         aggfunc=np.sum,     ).to_latex() ) <pre>\\begin{tabular}{lrrrrrr}\n\\toprule\nmodel &amp;  ABC negative binomial - gamma &amp;  ABC negative binomial - weibull &amp;  ABC with freqs - gamma &amp;  ABC with freqs - weibull &amp;    True gamma &amp;  True weibull \\\\\nss    &amp;                                &amp;                                  &amp;                         &amp;                           &amp;               &amp;               \\\\\n\\midrule\n50.0  &amp;                       0.453952 &amp;                         0.546048 &amp;                     0.0 &amp;                       1.0 &amp;  1.648283e-07 &amp;           1.0 \\\\\n250.0 &amp;                       0.405963 &amp;                         0.594037 &amp;                     0.0 &amp;                       1.0 &amp;  6.275335e-24 &amp;           1.0 \\\\\n\\bottomrule\n\\end{tabular}\n\n</pre>"},{"location":"model-selection/#model-selection-wip","title":"Model Selection (WIP)\u00b6","text":""},{"location":"model-selection/#true-posterior-samples","title":"True posterior samples\u00b6","text":"<p>We run a Bayesian analysis on the individual claim data and compuet the model probabilities when a Weibull or a gamma distribution is assumed. The prior distribution on the parameters are taken as independent uniform distribution (as in the ABC approach).</p>"},{"location":"model-selection/#fitting-a-weibull-and-a-gamma-model-to-the-individual-loss-data","title":"Fitting a Weibull and a gamma model to the individual loss data\u00b6","text":""},{"location":"model-selection/#abc-posterior-for-choosing-between-weibull-and-gamma-to-model-the-claim-sizes","title":"ABC posterior for choosing between Weibull and gamma to model the claim sizes\u00b6","text":""},{"location":"preamble/","title":"Preamble","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>plt.rcParams[\"figure.figsize\"] = (5.0, 2.0)\nplt.rcParams[\"figure.dpi\"] = 350\nplt.rcParams[\"savefig.bbox\"] = \"tight\"\nplt.rcParams[\"font.family\"] = \"serif\"\nplt.rcParams[\"pgf.texsystem\"] = \"pdflatex\"\n</pre> plt.rcParams[\"figure.figsize\"] = (5.0, 2.0) plt.rcParams[\"figure.dpi\"] = 350 plt.rcParams[\"savefig.bbox\"] = \"tight\" plt.rcParams[\"font.family\"] = \"serif\" plt.rcParams[\"pgf.texsystem\"] = \"pdflatex\""},{"location":"seasonal-claim-arrivals/","title":"Seasonal Claim Arrivals","text":"<p>Here, we set the claim arrival process to be governed by an inhomogenous Poisson process $N_t$ with intensity function</p> <p>$$ \\lambda(t) = a + b \\bigl( 1 + \\sin(2\\pi c t) \\bigr) $$</p> <p>The claim frequency data $n_s,\\,s = 1,\\ldots, t$ correspond to the increments of this non homogeneous Poisson process. These are independent Poisson variables $\\text{Pois}\\big(\\mu(s)\\big)$ with</p> <p>$$ \\mu(s) = \\int_{s}^{s+1} \\lambda(u) \\,\\mathrm{d}u = a + b + \\frac{b}{2\\pi c}\\left[\\cos(2\\pi c s) - \\cos(2\\pi c (s+1))\\right],\\quad s = 1,\\ldots, t \\,. $$</p> <p>The claim sizes are $U_{i,j} \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathsf{Pareto}(\\mu, \\alpha)$ where the $\\mu$ is the scale parameter and $\\alpha$ is the shape parameter.</p> <p>The available data is the total claim sizes $$ X_i = \\sum_{j = 1}^{N_i} U_{i,j}, \\quad i = 1, \\ldots, T. $$</p> <p>We start by importing some necessary packages.</p> In\u00a0[2]: Copied! <pre>%config InlineBackend.figure_format = 'retina'\n%run preamble.py\nimport approxbayescomp as abc\nimport numpy as np\nimport numpy.random as rnd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> %config InlineBackend.figure_format = 'retina' %run preamble.py import approxbayescomp as abc import numpy as np import numpy.random as rnd import matplotlib.pyplot as plt import seaborn as sns <p>We will fit simulated data, so that we know the true value of the parameters for the data-generating process. Here, we start with $a = 1$, $b = 5$, and $c = \\frac{1}{50}$ for the cyclical Poisson process, $\\mu = 10$ and $\\alpha = 2$ for the Pareto claim sizes, and say that we observe $T = 250$ i.i.d. observations of the compound sum.</p> In\u00a0[3]: Copied! <pre># Distributional families and parameters for the true model\nfreq = \"cyclical_poisson\"\nsev = \"pareto\"\n\n# Poisson process parameters\na = 1\nb = 5\nc = 1 / 50\n\n# Pareto claim size parameters\n\u03bc = 10\n\u03b1 = 2\n\ntrueTheta = (a, b, c, \u03bc, \u03b1)\n\n# Setting the time horizon\nT = 250\n\n# Simulating the claim data\nrg = rnd.default_rng(123)\nfreqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)\n\n# Simulating the observed data\npsi = abc.Psi(\"sum\")\nxData = abc.compute_psi(freqs, sevs, psi)\n</pre> # Distributional families and parameters for the true model freq = \"cyclical_poisson\" sev = \"pareto\"  # Poisson process parameters a = 1 b = 5 c = 1 / 50  # Pareto claim size parameters \u03bc = 10 \u03b1 = 2  trueTheta = (a, b, c, \u03bc, \u03b1)  # Setting the time horizon T = 250  # Simulating the claim data rg = rnd.default_rng(123) freqs, sevs = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)  # Simulating the observed data psi = abc.Psi(\"sum\") xData = abc.compute_psi(freqs, sevs, psi) In\u00a0[4]: Copied! <pre>t = np.arange(T)\ncosTerms = np.cos(2 * np.pi * c * t) - np.cos(2 * np.pi * (t + 1) * c)\nfreqMean = a + b + (b / (2 * np.pi * c)) * cosTerms\nseverityMean = \u03bc * \u03b1 / (\u03b1 - 1)\nxMean = freqMean * severityMean\n\nplt.plot(t, xMean, label=\"$\\mathbb{E}[X_s]$\", c=\"k\")\nplt.plot(t, xData, label=\"Observed $x_s$\", alpha=0.8)\n\nplt.xlabel(\"$t$\")\nplt.legend(ncol=2)\nsns.despine()\n</pre> t = np.arange(T) cosTerms = np.cos(2 * np.pi * c * t) - np.cos(2 * np.pi * (t + 1) * c) freqMean = a + b + (b / (2 * np.pi * c)) * cosTerms severityMean = \u03bc * \u03b1 / (\u03b1 - 1) xMean = freqMean * severityMean  plt.plot(t, xMean, label=\"$\\mathbb{E}[X_s]$\", c=\"k\") plt.plot(t, xData, label=\"Observed $x_s$\", alpha=0.8)  plt.xlabel(\"$t$\") plt.legend(ncol=2) sns.despine() <p>As we normally assume that we observe independent and identically distributed data, the default distance function in <code>smc</code> does not take into account the order of the observations. For this example, we need to swap out the distance measure with one which is designed for time series comparison.</p> <p>Here, we use the dynamic time warping distance, with the implementation from the dtaidistance package. To illustrate this distance, we simulate some more data from the true data-generating process:</p> In\u00a0[5]: Copied! <pre>rg = rnd.default_rng(456)\nfreqs2, sevs2 = abc.simulate_claim_data(rg, T, freq, sev, trueTheta)\nxData2 = abc.compute_psi(freqs2, sevs2, psi)\n</pre> rg = rnd.default_rng(456) freqs2, sevs2 = abc.simulate_claim_data(rg, T, freq, sev, trueTheta) xData2 = abc.compute_psi(freqs2, sevs2, psi) <p>Plotting the original $x_s$ data alongside this new $x_s'$ data:</p> In\u00a0[6]: Copied! <pre>plt.plot(t, xData, label=\"Observed $x_s$\")\nplt.plot(t, xData2, label=\"Observed $x_s'$\", alpha=0.8)\nplt.xlabel(\"$t$\")\nplt.legend(ncol=3)\nsns.despine()\n</pre> plt.plot(t, xData, label=\"Observed $x_s$\") plt.plot(t, xData2, label=\"Observed $x_s'$\", alpha=0.8) plt.xlabel(\"$t$\") plt.legend(ncol=3) sns.despine() <p>The dynamic time warping method speeds up and slows down the time in one time series to better match the other time series. The <code>dtaidistance</code> package has a built-in visualisation to show this matching between the series:</p> In\u00a0[7]: Copied! <pre>from dtaidistance import dtw\nfrom dtaidistance import dtw_visualisation as dtwvis\n\npath = dtw.warping_path(xData, xData2)\nfig = dtwvis.plot_warping(xData, xData2, path)\nsns.despine()\n</pre> from dtaidistance import dtw from dtaidistance import dtw_visualisation as dtwvis  path = dtw.warping_path(xData, xData2) fig = dtwvis.plot_warping(xData, xData2, path) sns.despine() <p>With this data, we create objects to represent the data-generating process (the model) and the prior distribution. The priors are set as</p> <p>$$ a \\sim \\mathsf{Unif}(0, 50), \\quad b \\sim \\mathsf{Unif}(0, 50), \\quad c \\sim \\mathsf{Unif}(\\frac{1}{1000}, \\frac{1}{10}) $$ $$ \\mu \\sim \\mathsf{Unif}(0, 50), \\quad \\text{ and } \\quad \\alpha \\sim \\mathsf{Unif}(1, 10) \\,. $$</p> In\u00a0[8]: Copied! <pre>model = abc.Model(\"cyclical_poisson\", \"pareto\", psi)\nparams = (\"$a$\", \"$b$\", \"$c$\", \"$\\\\mu$\", \"$\\\\alpha$\")\nprior = abc.IndependentUniformPrior(\n    [(0, 50), (0, 50), (1 / 1000, 1 / 10), (0, 50), (1, 10)], params\n)\n</pre> model = abc.Model(\"cyclical_poisson\", \"pareto\", psi) params = (\"$a$\", \"$b$\", \"$c$\", \"$\\\\mu$\", \"$\\\\alpha$\") prior = abc.IndependentUniformPrior(     [(0, 50), (0, 50), (1 / 1000, 1 / 10), (0, 50), (1, 10)], params ) In\u00a0[11]: Copied! <pre>numIters = 10\npopSize = 200\n\n%time fit = abc.smc(numIters, popSize, xData, model, prior, distance=dtw.distance, numProcs=8, seed=1, verbose=True)\n</pre> numIters = 10 popSize = 200  %time fit = abc.smc(numIters, popSize, xData, model, prior, distance=dtw.distance, numProcs=8, seed=1, verbose=True) <pre>Starting ABC-SMC with population size of 200 and sample size of 250 (~&gt; 250) on 8 processes.\nFinished sampling from prior, eps = 13962.21, time = 5.0s / 0.1m, popSize = 200 -&gt; 100, ESS = 200 -&gt; 100, # sims = 200, total # sims = 200\nFinished SMC iteration 1, eps = 3268.49, time = 2.0s / 0.0m, popSize = 302 -&gt; 111, ESS = 269 -&gt; 100, # sims = 1704, total # sims = 1904\nFinished SMC iteration 2, eps = 1467.07, time = 2.0s / 0.0m, popSize = 311 -&gt; 114, ESS = 260 -&gt; 100, # sims = 2176, total # sims = 4080\nFinished SMC iteration 3, eps = 1124.92, time = 3.0s / 0.0m, popSize = 327 -&gt; 124, ESS = 230 -&gt; 100, # sims = 4776, total # sims = 8856\nFinished SMC iteration 4, eps = 1038.30, time = 3.0s / 0.1m, popSize = 337 -&gt; 136, ESS = 250 -&gt; 100, # sims = 8504, total # sims = 17360\nFinished SMC iteration 5, eps = 991.24, time = 5.0s / 0.1m, popSize = 349 -&gt; 116, ESS = 271 -&gt; 100, # sims = 13432, total # sims = 30792\nFinished SMC iteration 6, eps = 960.14, time = 9.0s / 0.1m, popSize = 330 -&gt; 132, ESS = 263 -&gt; 100, # sims = 13432, total # sims = 44224\nFinished SMC iteration 7, eps = 938.08, time = 14.0s / 0.2m, popSize = 347 -&gt; 168, ESS = 231 -&gt; 100, # sims = 20392, total # sims = 64616\nFinished SMC iteration 8, eps = 926.74, time = 18.0s / 0.3m, popSize = 369 -&gt; 244, ESS = 161 -&gt; 100, # sims = 28984, total # sims = 93600\nFinished SMC iteration 9, eps = 903.74, time = 21.0s / 0.4m, popSize = 447 -&gt; 182, ESS = 246 -&gt; 100, # sims = 28984, total # sims = 122584\nFinished SMC iteration 10, eps = 886.16, time = 26.0s / 0.4m, popSize = 451 -&gt; 200, ESS = 188 -&gt; 61, # sims = 28984, total # sims = 151568\nCPU times: user 371 ms, sys: 112 ms, total: 483 ms\nWall time: 1min 48s\n</pre> <p>These particles all generated fake data within the following distance to the observed data:</p> In\u00a0[12]: Copied! <pre>np.max(fit.dists)\n</pre> np.max(fit.dists) Out[12]: <pre>886.1586423863746</pre> <p>Plotting the fitted ABC posterior:</p> In\u00a0[13]: Copied! <pre>abc.plot_posteriors(\n    fit,\n    prior,\n    refLines=trueTheta,\n)\n</pre> abc.plot_posteriors(     fit,     prior,     refLines=trueTheta, ) <p>This ABC posterior is quite accurate.</p>"},{"location":"seasonal-claim-arrivals/#seasonal-claim-arrivals","title":"Seasonal Claim Arrivals\u00b6","text":""},{"location":"seasonal-claim-arrivals/#generate-synthetic-observations","title":"Generate synthetic observations\u00b6","text":""},{"location":"seasonal-claim-arrivals/#plot-the-cyclical-observations-against-their-expected-values","title":"Plot the cyclical observations against their expected values\u00b6","text":""},{"location":"seasonal-claim-arrivals/#try-a-time-series-distance-function","title":"Try a time series distance function\u00b6","text":""},{"location":"seasonal-claim-arrivals/#use-abc-to-fit-the-data","title":"Use ABC to fit the data\u00b6","text":""},{"location":"simulation-model-example/","title":"Simulation model example","text":"In\u00a0[\u00a0]: Copied! <pre>import approxbayescomp as abc\nimport numpy as np\nimport numpy.random as rnd\n</pre> import approxbayescomp as abc import numpy as np import numpy.random as rnd In\u00a0[\u00a0]: Copied! <pre># Load data to fit (modify this line to load real observations!)\nobsData = [1.0, 2.0, 3.0]\n</pre> # Load data to fit (modify this line to load real observations!) obsData = [1.0, 2.0, 3.0] In\u00a0[\u00a0]: Copied! <pre># Specify our prior beliefs over (lambda, mu, sigma).\nprior = abc.IndependentUniformPrior([(0, 100), (-5, 5), (0, 3)])\n</pre> # Specify our prior beliefs over (lambda, mu, sigma). prior = abc.IndependentUniformPrior([(0, 100), (-5, 5), (0, 3)]) In\u00a0[\u00a0]: Copied! <pre># Write a function to simulate from the data-generating process.\ndef simulate_aggregate_claims(theta, T):\n    \"\"\"\n    Generate T observations from the model specified by theta.\n    \"\"\"\n    lam, mu, sigma = theta\n    freqs = rnd.poisson(lam, size=T)\n    aggClaims = np.empty(T, np.float64)\n    for t in range(T):\n        aggClaims[t] = np.sum(rnd.lognormal(mu, sigma, size=freqs[t]))\n    return aggClaims\n</pre> # Write a function to simulate from the data-generating process. def simulate_aggregate_claims(theta, T):     \"\"\"     Generate T observations from the model specified by theta.     \"\"\"     lam, mu, sigma = theta     freqs = rnd.poisson(lam, size=T)     aggClaims = np.empty(T, np.float64)     for t in range(T):         aggClaims[t] = np.sum(rnd.lognormal(mu, sigma, size=freqs[t]))     return aggClaims In\u00a0[\u00a0]: Copied! <pre># Fit the model to the data using ABC\ndef model(theta):\n    return simulate_aggregate_claims(theta, len(obsData))\n</pre> # Fit the model to the data using ABC def model(theta):     return simulate_aggregate_claims(theta, len(obsData)) In\u00a0[\u00a0]: Copied! <pre>numIters = 6  # The number of SMC iterations to perform\npopSize = 250  # The population size of the SMC method\n</pre> numIters = 6  # The number of SMC iterations to perform popSize = 250  # The population size of the SMC method In\u00a0[\u00a0]: Copied! <pre>fit = abc.smc(numIters, popSize, obsData, model, prior)\n</pre> fit = abc.smc(numIters, popSize, obsData, model, prior)"},{"location":"what-is-abc/","title":"What is ABC?","text":"<p>Let's consider a toy example. Imagine we are trying to determine the probability \\(p = \\mathbb{P}(\\text{Heads})\\) of getting a heads when flipping a biased coin. We observe just three coin flips, and get <code>Heads, Tails, Heads</code>. One way to fit this data is to just start flipping (virtual) coins. Specifically, we randomly guess a value \\(p' \\in (0, 1)\\), flip a biased coin with this probability of heads three times, then remember this value of \\(p'\\) if this coin also got <code>Heads, Tails, Heads</code>.</p> Illustration of exact posterior sampling with 3 observed coin tosses from a biased coin. <p></p> Once Loop Reflect <p>Tip</p> <p>Press the play button (triangular shaped) on these animations to start them.</p> <p>In this simulation we found 15 values of \\(p'\\) which happened to generate the same <code>Heads, Tails, Heads</code>. These 15 values are actually samples from the posterior distribution for \\(p\\), given that we have a uniform prior belief.</p> <p>Of course, this method of generating fake data which is identical to the real observed data is not an efficient way to sample from the posterior distribution. If we increase the sample size of this toy example to eight observed coin flips, the following animation shows that event after 100 attempts we don't find a single \\(p'\\) which generates the exact same eight observations.</p> Illustration of exact posterior sampling with 8 observed coin tosses from a biased coin. <p></p> Once Loop Reflect <p>This is where the approximate part of Approximate Bayesian Computation (ABC) comes in. Let's just accept a \\(p'\\) value if the fake data it generates is pretty close to the observed data. In the following, our observed data had five heads, so we accept any fake data that has 4, 5, or 6 heads.</p> Illustration of ABC sampling with 8 observed coin tosses from a biased coin. <p></p> Once Loop Reflect <p>This generated a much larger sample (29 points) from an approximate posterior distribution. We can compare the true posterior to the various approximate posterior distributions where our \"accept when pretty close\" rule is more or less stringent.</p> Comparing the true posterior distribution to various ABC approximate posteriors. <p></p> Once Loop Reflect"}]}